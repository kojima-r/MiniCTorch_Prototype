{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20210824_minictorch_cross_entropy_loss.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPOnWyMPzoSIXAQvfNmgyo5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Sml8FMQqbgEj"},"source":["cross entropy loss　分類問題"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ULQtL8IeXTg2","executionInfo":{"status":"ok","timestamp":1630757351792,"user_tz":-540,"elapsed":21356,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"6fa1f255-b061-470e-b21f-cae601d4b6ec"},"source":["#　colaboraory用: Google drive をマウントする\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NujIexoFXUmL","executionInfo":{"status":"ok","timestamp":1630757402827,"user_tz":-540,"elapsed":394,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"a15c5ad6-6cc8-4712-b377-380c531f6a5d"},"source":["# colaboratory用: フォルダを移る\n","%cd \"drive/My Drive/Colab Notebooks/\""],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks\n"]}]},{"cell_type":"markdown","metadata":{"id":"0DNl9nIDfY0l"},"source":["フォルダは自分の指定のものに変更してね。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gR9cSFPNfVA4","executionInfo":{"status":"ok","timestamp":1630757405852,"user_tz":-540,"elapsed":526,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"eadbc441-6c14-493d-d07d-50c67ffc4977"},"source":["%cd \"ctorch210824/MiniCTorch_Prototype\""],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks/ctorch210824/MiniCTorch_Prototype\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPPGcVEQ7fwE","executionInfo":{"status":"ok","timestamp":1630757411027,"user_tz":-540,"elapsed":4359,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"8beb332f-2b40-4c56-fef7-4a730f4b3a0e"},"source":["! pip install lark-parser"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lark-parser\n","  Downloading lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n","\u001b[?25l\r\u001b[K     |███▏                            | 10 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 20 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 30 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 40 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 71 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 92 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 102 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 103 kB 5.8 MB/s \n","\u001b[?25hInstalling collected packages: lark-parser\n","Successfully installed lark-parser-0.12.0\n"]}]},{"cell_type":"code","metadata":{"id":"vuIJaurj7brd","executionInfo":{"status":"ok","timestamp":1630757419750,"user_tz":-540,"elapsed":6631,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}}},"source":["import json\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import minictorch\n","import minictorch.generator as GN\n","import minictorch.converter as CV"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"UWUPb5h0Blqx","executionInfo":{"status":"ok","timestamp":1630757422461,"user_tz":-540,"elapsed":1313,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}}},"source":["from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing   import StandardScaler\n","\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fuAfJop8BpYI"},"source":["データ読み込み"]},{"cell_type":"code","metadata":{"id":"5FYRis-rBr_Y","executionInfo":{"status":"ok","timestamp":1630757425101,"user_tz":-540,"elapsed":378,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}}},"source":["# データ読み込み\n","iris = datasets.load_iris()\n","data   = iris['data']\n","target = iris['target']\n","\n","# 学習データと検証データに分割\n","x_train, x_valid, y_train, y_valid = train_test_split( data, target, shuffle=True )\n","\n","# 特徴量の標準化\n","scaler = StandardScaler()\n","scaler.fit( x_train )\n","\n","x_train = scaler.transform(x_train)\n","x_valid = scaler.transform(x_valid)\n","\n","# Tensor型に変換\n","# 学習に入れるときはfloat型 or long型になっている必要があるのここで変換してしまう\n","x_train = torch.from_numpy(x_train).float()\n","y_train = torch.from_numpy(y_train).long()\n","x_valid = torch.from_numpy(x_valid).float()\n","y_valid = torch.from_numpy(y_valid).long()\n","\n","#print('x_train : ', x_train.shape)\n","#print('y_train : ', y_train.shape)\n","#print('x_valid : ', x_valid.shape)\n","#print('y_valid : ', y_valid.shape)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cDivgOEAB-wC"},"source":["DataSetとDataLoaderの生成"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-AznF7ZCEEJ","executionInfo":{"status":"ok","timestamp":1630757428224,"user_tz":-540,"elapsed":527,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"635287a4-2ab2-4a3f-8ac3-ec7871caf7a2"},"source":["train_dataset = TensorDataset(x_train, y_train)\n","valid_dataset = TensorDataset(x_valid, y_valid)\n","\n","# indexを指定すればデータを取り出すことができます。\n","index = 0\n","print( train_dataset.__getitem__(index)[0].size() )\n","print( train_dataset.__getitem__(index)[1] )\n","\n","\n","batch_size = 112\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","\n","# 動作確認\n","# こんな感じでバッチ単位で取り出す子ができます。\n","# イテレータに変換\n","batch_iterator = iter(train_dataloader)\n","\n","# 1番目の要素を取り出す\n","inputs, labels = next(batch_iterator)\n","print(inputs.size())\n","print(labels.size())\n","print(inputs)\n","print(labels)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4])\n","tensor(1)\n","torch.Size([112, 4])\n","torch.Size([112])\n","tensor([[-1.0889, -1.4458, -0.2520, -0.2455],\n","        [-0.0264, -0.7616,  0.1996, -0.2455],\n","        [-1.4431,  0.3788, -1.3244, -1.3032],\n","        [ 0.4459, -1.9020,  0.4253,  0.4155],\n","        [-0.3805, -1.4458,  0.0302, -0.1133],\n","        [ 0.0917, -0.0774,  0.2560,  0.4155],\n","        [-0.9708, -0.0774, -1.2115, -1.3032],\n","        [ 1.1543,  0.3788,  1.2155,  1.4732],\n","        [ 1.6265, -0.0774,  1.1591,  0.5477],\n","        [ 1.0362, -1.2178,  1.1591,  0.8121],\n","        [-1.6792,  0.3788, -1.3808, -1.3032],\n","        [ 0.8001,  0.3788,  0.7640,  1.0766],\n","        [-0.8528,  1.7472, -1.2115, -1.3032],\n","        [-0.8528,  1.5191, -1.2679, -1.0388],\n","        [-1.4431,  0.1507, -1.2679, -1.3032],\n","        [-0.8528,  0.8349, -1.2679, -1.3032],\n","        [ 1.0362,  0.1507,  1.0462,  1.6054],\n","        [-1.4431,  0.8349, -1.3244, -1.1710],\n","        [-1.2070,  0.8349, -1.0422, -1.3032],\n","        [-0.1444, -0.5335,  0.1996,  0.1511],\n","        [ 0.2098, -0.7616,  0.7640,  0.5477],\n","        [ 0.6820,  0.1507,  0.9897,  0.8121],\n","        [ 0.8001, -0.0774,  0.8204,  1.0766],\n","        [ 0.0917,  0.3788,  0.5947,  0.8121],\n","        [ 0.0917, -0.0774,  0.7640,  0.8121],\n","        [-0.4986,  1.9753, -1.3808, -1.0388],\n","        [-0.7347, -0.7616,  0.0867,  0.2833],\n","        [ 2.2168, -0.0774,  1.3284,  1.4732],\n","        [ 0.5639, -0.5335,  0.7640,  0.4155],\n","        [ 1.8626, -0.5335,  1.3284,  0.9443],\n","        [ 0.9181, -0.3055,  0.4818,  0.1511],\n","        [-0.0264, -0.7616,  0.0867,  0.0189],\n","        [-0.7347,  2.4314, -1.2679, -1.4354],\n","        [-1.5611, -1.6739, -1.3808, -1.1710],\n","        [ 1.0362,  0.1507,  0.3689,  0.2833],\n","        [-0.9708,  1.2911, -1.3244, -1.3032],\n","        [ 0.6820, -0.5335,  1.0462,  1.3410],\n","        [-0.1444, -0.9897, -0.1391, -0.2455],\n","        [ 0.2098, -0.0774,  0.5947,  0.8121],\n","        [-1.0889,  0.1507, -1.2679, -1.3032],\n","        [-0.9708,  1.0630, -1.2115, -0.7744],\n","        [ 0.6820, -0.7616,  0.8769,  0.9443],\n","        [ 1.0362,  0.6068,  1.1026,  1.2088],\n","        [-0.0264, -0.7616,  0.7640,  0.9443],\n","        [-0.8528,  1.7472, -1.0422, -1.0388],\n","        [-1.2070,  0.8349, -1.2115, -1.3032],\n","        [-0.4986,  0.8349, -1.1550, -1.3032],\n","        [ 0.8001, -0.0774,  0.9897,  0.8121],\n","        [ 1.6265,  1.2911,  1.3284,  1.7376],\n","        [-0.9708, -1.6739, -0.2520, -0.2455],\n","        [ 0.8001, -0.5335,  0.4818,  0.4155],\n","        [ 1.2723,  0.1507,  0.9333,  1.2088],\n","        [ 0.6820,  0.3788,  0.8769,  1.4732],\n","        [-1.6792, -0.3055, -1.3244, -1.3032],\n","        [ 0.4459, -0.5335,  0.5947,  0.8121],\n","        [ 0.5639, -1.6739,  0.3689,  0.1511],\n","        [-0.0264,  2.2034, -1.4372, -1.3032],\n","        [ 1.0362, -0.0774,  0.8204,  1.4732],\n","        [-0.9708,  0.8349, -1.2679, -1.3032],\n","        [-0.8528,  0.6068, -1.1550, -0.9066],\n","        [-0.3805, -1.6739,  0.1431,  0.1511],\n","        [-0.8528, -1.2178, -0.4213, -0.1133],\n","        [-1.2070,  0.1507, -1.2115, -1.3032],\n","        [ 2.0987, -0.0774,  1.6106,  1.2088],\n","        [ 1.2723,  0.3788,  1.1026,  1.4732],\n","        [ 0.6820, -0.5335,  1.0462,  1.2088],\n","        [-0.3805, -0.9897,  0.3689,  0.0189],\n","        [-0.2625, -0.3055, -0.0826,  0.1511],\n","        [-0.4986,  0.8349, -1.2679, -1.0388],\n","        [ 2.2168, -0.9897,  1.7799,  1.4732],\n","        [ 0.5639, -0.7616,  0.6511,  0.8121],\n","        [ 1.0362, -0.0774,  0.7075,  0.6799],\n","        [ 0.6820,  0.3788,  0.4253,  0.4155],\n","        [-0.8528,  1.7472, -1.2679, -1.1710],\n","        [-0.9708,  0.6068, -1.3244, -1.3032],\n","        [ 1.1543, -0.5335,  0.5947,  0.2833],\n","        [-1.0889,  1.2911, -1.3244, -1.4354],\n","        [-0.9708,  0.8349, -1.2115, -1.0388],\n","        [-0.2625, -0.5335,  0.6511,  1.0766],\n","        [-1.2070, -0.0774, -1.3244, -1.1710],\n","        [-0.3805, -1.4458, -0.0262, -0.2455],\n","        [ 0.5639, -1.2178,  0.7075,  0.9443],\n","        [-1.0889, -1.2178,  0.4253,  0.6799],\n","        [-0.1444, -0.3055,  0.2560,  0.1511],\n","        [-1.0889,  0.1507, -1.2679, -1.4354],\n","        [-0.6167,  1.5191, -1.2679, -1.3032],\n","        [-0.4986,  1.9753, -1.1550, -1.0388],\n","        [ 0.5639,  0.6068,  1.2719,  1.7376],\n","        [ 2.4529,  1.7472,  1.4977,  1.0766],\n","        [ 2.2168,  1.7472,  1.6670,  1.3410],\n","        [-0.4986,  1.5191, -1.2679, -1.3032],\n","        [-0.9708, -2.3581, -0.1391, -0.2455],\n","        [ 0.9181, -0.0774,  0.3689,  0.2833],\n","        [-0.9708,  1.0630, -1.3808, -1.1710],\n","        [ 0.3278, -0.5335,  0.5382,  0.0189],\n","        [ 0.2098, -1.9020,  0.1431, -0.2455],\n","        [ 0.5639, -0.3055,  1.0462,  0.8121],\n","        [ 1.7446, -0.3055,  1.4413,  0.8121],\n","        [ 0.2098, -1.9020,  0.7075,  0.4155],\n","        [-0.1444, -0.0774,  0.2560,  0.0189],\n","        [-1.3250,  0.3788, -1.2115, -1.3032],\n","        [ 0.3278, -0.9897,  1.0462,  0.2833],\n","        [ 0.4459, -0.3055,  0.3124,  0.1511],\n","        [-0.0264, -0.9897,  0.1431,  0.0189],\n","        [-1.4431,  1.2911, -1.5501, -1.3032],\n","        [-1.3250,  0.3788, -1.3808, -1.3032],\n","        [ 0.5639,  0.8349,  1.0462,  1.6054],\n","        [ 1.5084, -0.0774,  1.2155,  1.2088],\n","        [-1.0889, -0.0774, -1.3244, -1.3032],\n","        [-0.1444, -0.5335,  0.4253,  0.1511],\n","        [ 0.3278, -0.5335,  0.1431,  0.1511],\n","        [-0.2625, -0.0774,  0.4253,  0.4155]])\n","tensor([1, 1, 0, 1, 1, 1, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 1, 2, 2, 1,\n","        2, 0, 1, 2, 2, 2, 1, 1, 0, 0, 1, 0, 2, 1, 2, 0, 0, 2, 2, 2, 0, 0, 0, 2,\n","        2, 1, 1, 2, 2, 0, 2, 1, 0, 2, 0, 0, 1, 1, 0, 2, 2, 2, 1, 1, 0, 2, 2, 1,\n","        1, 0, 0, 1, 0, 0, 2, 0, 1, 2, 2, 1, 0, 0, 0, 2, 2, 2, 0, 1, 1, 0, 1, 1,\n","        2, 2, 2, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0, 1, 1, 1])\n"]}]},{"cell_type":"markdown","metadata":{"id":"8Io-p4ogJysT"},"source":["ニューラルネットワークの定義"]},{"cell_type":"code","metadata":{"id":"FMLT7mbxauCN","executionInfo":{"status":"ok","timestamp":1630757434491,"user_tz":-540,"elapsed":397,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}}},"source":["class Net(nn.Module):    \n","    def __init__(self,t):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(4, 64)\n","        self.fc2 = nn.Linear(64, 3)\n","        self.target = t\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        #x = F.log_softmax(x, dim=1)\n","        #return x\n","        \n","        #print(x);\n","        #print(self.target);\n","        \n","        self.out = x\n","\n","        loss = nn.CrossEntropyLoss()\n","        #loss = nn.NLLLoss()\n","        output = loss(x,self.target)\n","        return output\n","        \n","\n","class Net2(nn.Module):    \n","    def __init__(self):\n","        super(Net2, self).__init__()\n","        self.fc1 = nn.Linear(4, 64)\n","        self.fc2 = nn.Linear(64, 3)\n","        #self.fc1 = nn.Linear(4, 128)\n","        #self.fc2 = nn.Linear(128, 3)\n","        #self.fc3 = nn.Linear(64, 3)\n","        #self.fc4 = nn.Linear(128, 3)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        #x = F.relu(self.fc2(x))\n","        #x = F.relu(self.fc3(x))\n","        x = self.fc2(x)\n","        #x = F.softmax(x, dim=1)\n","        return x"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"_yu6J3uzDVsp","executionInfo":{"status":"ok","timestamp":1630757437935,"user_tz":-540,"elapsed":469,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}}},"source":["def generate_json( json_path, input, target ):\n","\n","    model = Net( target )\n","    model.eval()\n","    with torch.no_grad():\n","        print(\"[SAVE]\", json_path )\n","        GN.generate_minictorch_file( model, input, json_path )\n","\n","    return model"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gavYhJ2Z6tft","executionInfo":{"status":"ok","timestamp":1630757442535,"user_tz":-540,"elapsed":1391,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"aac07b20-faa7-47a5-804b-c803eeee6834"},"source":["torch.manual_seed( 1 )\n","\n","print(\"inputs\",inputs)\n","print(\"target\",labels)\n","inputs.requires_grad = True\n","\n","project = 'cse1'\n","json_path = 'network/' + project +'.json'\n","\n","model = generate_json( json_path, inputs, labels )\n","\n","with torch.set_grad_enabled(True):\n","\n","  output = model( inputs )\n","  print(\"output\",output)\n","\n","  model.zero_grad()\n","  #torch.zeros_like(inputs.grad)\n","\n","  output.backward()\n","  print(\"output grad\",output.grad)\n","  #print(\"fc1 grad\",model.fc1.weight.grad)\n","  #print(\"fc2 grad\",model.fc2.weight.grad)\n","  print(\"input grad\",inputs.grad)\n","\n","  # ラベルを予測\n","  #print(\"output\", model.out, inputs.size(0))\n","  _, preds = torch.max( model.out, 1 )\n","\n","  # イテレーション結果の計算\n","  epoch_loss = output * inputs.size(0)\n","\n","  # 正解数の合計を更新\n","  epoch_corrects = torch.sum( preds == labels.data )\n","\n","  epoch_loss = epoch_loss / float(inputs.size(0))\n","  epoch_acc  = epoch_corrects.double() / float(inputs.size(0))\n","\n","  epoch=1\n","  print('Train Loss {}: {:.4f} Acc: {:.4f}'.format( epoch, epoch_loss, epoch_acc ))"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs tensor([[-1.0889, -1.4458, -0.2520, -0.2455],\n","        [-0.0264, -0.7616,  0.1996, -0.2455],\n","        [-1.4431,  0.3788, -1.3244, -1.3032],\n","        [ 0.4459, -1.9020,  0.4253,  0.4155],\n","        [-0.3805, -1.4458,  0.0302, -0.1133],\n","        [ 0.0917, -0.0774,  0.2560,  0.4155],\n","        [-0.9708, -0.0774, -1.2115, -1.3032],\n","        [ 1.1543,  0.3788,  1.2155,  1.4732],\n","        [ 1.6265, -0.0774,  1.1591,  0.5477],\n","        [ 1.0362, -1.2178,  1.1591,  0.8121],\n","        [-1.6792,  0.3788, -1.3808, -1.3032],\n","        [ 0.8001,  0.3788,  0.7640,  1.0766],\n","        [-0.8528,  1.7472, -1.2115, -1.3032],\n","        [-0.8528,  1.5191, -1.2679, -1.0388],\n","        [-1.4431,  0.1507, -1.2679, -1.3032],\n","        [-0.8528,  0.8349, -1.2679, -1.3032],\n","        [ 1.0362,  0.1507,  1.0462,  1.6054],\n","        [-1.4431,  0.8349, -1.3244, -1.1710],\n","        [-1.2070,  0.8349, -1.0422, -1.3032],\n","        [-0.1444, -0.5335,  0.1996,  0.1511],\n","        [ 0.2098, -0.7616,  0.7640,  0.5477],\n","        [ 0.6820,  0.1507,  0.9897,  0.8121],\n","        [ 0.8001, -0.0774,  0.8204,  1.0766],\n","        [ 0.0917,  0.3788,  0.5947,  0.8121],\n","        [ 0.0917, -0.0774,  0.7640,  0.8121],\n","        [-0.4986,  1.9753, -1.3808, -1.0388],\n","        [-0.7347, -0.7616,  0.0867,  0.2833],\n","        [ 2.2168, -0.0774,  1.3284,  1.4732],\n","        [ 0.5639, -0.5335,  0.7640,  0.4155],\n","        [ 1.8626, -0.5335,  1.3284,  0.9443],\n","        [ 0.9181, -0.3055,  0.4818,  0.1511],\n","        [-0.0264, -0.7616,  0.0867,  0.0189],\n","        [-0.7347,  2.4314, -1.2679, -1.4354],\n","        [-1.5611, -1.6739, -1.3808, -1.1710],\n","        [ 1.0362,  0.1507,  0.3689,  0.2833],\n","        [-0.9708,  1.2911, -1.3244, -1.3032],\n","        [ 0.6820, -0.5335,  1.0462,  1.3410],\n","        [-0.1444, -0.9897, -0.1391, -0.2455],\n","        [ 0.2098, -0.0774,  0.5947,  0.8121],\n","        [-1.0889,  0.1507, -1.2679, -1.3032],\n","        [-0.9708,  1.0630, -1.2115, -0.7744],\n","        [ 0.6820, -0.7616,  0.8769,  0.9443],\n","        [ 1.0362,  0.6068,  1.1026,  1.2088],\n","        [-0.0264, -0.7616,  0.7640,  0.9443],\n","        [-0.8528,  1.7472, -1.0422, -1.0388],\n","        [-1.2070,  0.8349, -1.2115, -1.3032],\n","        [-0.4986,  0.8349, -1.1550, -1.3032],\n","        [ 0.8001, -0.0774,  0.9897,  0.8121],\n","        [ 1.6265,  1.2911,  1.3284,  1.7376],\n","        [-0.9708, -1.6739, -0.2520, -0.2455],\n","        [ 0.8001, -0.5335,  0.4818,  0.4155],\n","        [ 1.2723,  0.1507,  0.9333,  1.2088],\n","        [ 0.6820,  0.3788,  0.8769,  1.4732],\n","        [-1.6792, -0.3055, -1.3244, -1.3032],\n","        [ 0.4459, -0.5335,  0.5947,  0.8121],\n","        [ 0.5639, -1.6739,  0.3689,  0.1511],\n","        [-0.0264,  2.2034, -1.4372, -1.3032],\n","        [ 1.0362, -0.0774,  0.8204,  1.4732],\n","        [-0.9708,  0.8349, -1.2679, -1.3032],\n","        [-0.8528,  0.6068, -1.1550, -0.9066],\n","        [-0.3805, -1.6739,  0.1431,  0.1511],\n","        [-0.8528, -1.2178, -0.4213, -0.1133],\n","        [-1.2070,  0.1507, -1.2115, -1.3032],\n","        [ 2.0987, -0.0774,  1.6106,  1.2088],\n","        [ 1.2723,  0.3788,  1.1026,  1.4732],\n","        [ 0.6820, -0.5335,  1.0462,  1.2088],\n","        [-0.3805, -0.9897,  0.3689,  0.0189],\n","        [-0.2625, -0.3055, -0.0826,  0.1511],\n","        [-0.4986,  0.8349, -1.2679, -1.0388],\n","        [ 2.2168, -0.9897,  1.7799,  1.4732],\n","        [ 0.5639, -0.7616,  0.6511,  0.8121],\n","        [ 1.0362, -0.0774,  0.7075,  0.6799],\n","        [ 0.6820,  0.3788,  0.4253,  0.4155],\n","        [-0.8528,  1.7472, -1.2679, -1.1710],\n","        [-0.9708,  0.6068, -1.3244, -1.3032],\n","        [ 1.1543, -0.5335,  0.5947,  0.2833],\n","        [-1.0889,  1.2911, -1.3244, -1.4354],\n","        [-0.9708,  0.8349, -1.2115, -1.0388],\n","        [-0.2625, -0.5335,  0.6511,  1.0766],\n","        [-1.2070, -0.0774, -1.3244, -1.1710],\n","        [-0.3805, -1.4458, -0.0262, -0.2455],\n","        [ 0.5639, -1.2178,  0.7075,  0.9443],\n","        [-1.0889, -1.2178,  0.4253,  0.6799],\n","        [-0.1444, -0.3055,  0.2560,  0.1511],\n","        [-1.0889,  0.1507, -1.2679, -1.4354],\n","        [-0.6167,  1.5191, -1.2679, -1.3032],\n","        [-0.4986,  1.9753, -1.1550, -1.0388],\n","        [ 0.5639,  0.6068,  1.2719,  1.7376],\n","        [ 2.4529,  1.7472,  1.4977,  1.0766],\n","        [ 2.2168,  1.7472,  1.6670,  1.3410],\n","        [-0.4986,  1.5191, -1.2679, -1.3032],\n","        [-0.9708, -2.3581, -0.1391, -0.2455],\n","        [ 0.9181, -0.0774,  0.3689,  0.2833],\n","        [-0.9708,  1.0630, -1.3808, -1.1710],\n","        [ 0.3278, -0.5335,  0.5382,  0.0189],\n","        [ 0.2098, -1.9020,  0.1431, -0.2455],\n","        [ 0.5639, -0.3055,  1.0462,  0.8121],\n","        [ 1.7446, -0.3055,  1.4413,  0.8121],\n","        [ 0.2098, -1.9020,  0.7075,  0.4155],\n","        [-0.1444, -0.0774,  0.2560,  0.0189],\n","        [-1.3250,  0.3788, -1.2115, -1.3032],\n","        [ 0.3278, -0.9897,  1.0462,  0.2833],\n","        [ 0.4459, -0.3055,  0.3124,  0.1511],\n","        [-0.0264, -0.9897,  0.1431,  0.0189],\n","        [-1.4431,  1.2911, -1.5501, -1.3032],\n","        [-1.3250,  0.3788, -1.3808, -1.3032],\n","        [ 0.5639,  0.8349,  1.0462,  1.6054],\n","        [ 1.5084, -0.0774,  1.2155,  1.2088],\n","        [-1.0889, -0.0774, -1.3244, -1.3032],\n","        [-0.1444, -0.5335,  0.4253,  0.1511],\n","        [ 0.3278, -0.5335,  0.1431,  0.1511],\n","        [-0.2625, -0.0774,  0.4253,  0.4155]])\n","target tensor([1, 1, 0, 1, 1, 1, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 1, 2, 2, 1,\n","        2, 0, 1, 2, 2, 2, 1, 1, 0, 0, 1, 0, 2, 1, 2, 0, 0, 2, 2, 2, 0, 0, 0, 2,\n","        2, 1, 1, 2, 2, 0, 2, 1, 0, 2, 0, 0, 1, 1, 0, 2, 2, 2, 1, 1, 0, 2, 2, 1,\n","        1, 0, 0, 1, 0, 0, 2, 0, 1, 2, 2, 1, 0, 0, 0, 2, 2, 2, 0, 1, 1, 0, 1, 1,\n","        2, 2, 2, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0, 1, 1, 1])\n","[SAVE] network/cse1.json\n","skip: Net/Linear[fc1]/weight/26\n","skip: Net/Linear[fc1]/weight/26\n","skip: Net/Linear[fc2]/weight/29\n","skip: Net/Linear[fc2]/weight/29\n","output tensor(1.1594, grad_fn=<NllLossBackward>)\n","output grad None\n","input grad tensor([[ 8.2042e-04, -6.0342e-04,  1.3901e-03,  5.5852e-04],\n","        [ 1.1230e-03, -8.1382e-05,  1.6038e-03,  1.2079e-04],\n","        [-1.4634e-03, -6.0126e-04,  4.2243e-04, -3.2474e-04],\n","        [-2.6614e-04,  9.3596e-04,  6.4735e-05,  1.3009e-04],\n","        [ 3.8126e-04,  3.6224e-04,  9.7009e-04,  5.5561e-06],\n","        [ 1.9122e-04, -5.9756e-04,  2.0021e-04, -1.2942e-03],\n","        [-1.3470e-03, -5.5443e-04,  2.5391e-04, -2.7090e-04],\n","        [ 4.4192e-04,  1.0732e-03, -8.5067e-05,  6.4894e-04],\n","        [ 1.3583e-03,  1.0215e-03, -1.7722e-04,  2.0351e-05],\n","        [ 1.2042e-03,  6.4352e-04, -9.2763e-04,  7.1028e-04],\n","        [-1.3561e-03, -4.2397e-04,  3.7536e-04, -4.6901e-04],\n","        [ 6.4193e-04,  7.3208e-04, -6.3732e-04,  6.6529e-04],\n","        [-1.4707e-03,  3.2822e-04,  3.6720e-04,  3.5229e-04],\n","        [-1.3227e-03, -3.7962e-06,  9.8856e-06,  2.9166e-04],\n","        [-1.3075e-03, -4.5507e-04,  4.0728e-04, -4.6108e-04],\n","        [-1.5032e-03, -6.4191e-04,  1.1673e-04, -1.7338e-04],\n","        [ 3.0760e-04,  1.0879e-03, -1.4493e-04,  7.1358e-04],\n","        [-1.3571e-03, -4.6555e-04,  5.0196e-04, -4.8292e-04],\n","        [-1.6185e-03, -5.0602e-04, -1.0153e-04, -4.1815e-05],\n","        [ 5.4108e-04, -1.6146e-04,  1.2000e-03, -8.3154e-04],\n","        [-4.9033e-04, -1.7062e-04,  1.5076e-05, -1.5735e-03],\n","        [ 7.6070e-04,  1.1405e-03, -8.5285e-04,  9.6750e-04],\n","        [ 4.5418e-04,  1.2515e-03, -9.4106e-04,  9.7488e-04],\n","        [ 1.9623e-04, -3.9825e-04, -4.9833e-04, -1.5361e-03],\n","        [ 8.7534e-04,  9.0704e-04, -1.0984e-03,  9.2614e-04],\n","        [-1.9591e-03,  4.2316e-04,  5.9475e-04,  2.0013e-04],\n","        [ 8.5799e-04, -2.4360e-04,  1.1363e-03, -1.5814e-04],\n","        [ 1.0647e-03,  3.2582e-04, -2.4958e-04,  4.3950e-04],\n","        [ 1.1093e-03,  7.7825e-04, -6.3382e-04,  8.7334e-04],\n","        [ 1.4067e-03,  8.1009e-04, -8.0973e-04,  5.8261e-04],\n","        [-6.0224e-04, -2.2852e-05, -3.4571e-04, -1.1919e-03],\n","        [ 4.1960e-04, -4.5466e-05,  1.3255e-03, -6.6928e-04],\n","        [-1.5313e-03,  3.9040e-04,  3.6180e-04,  3.5454e-04],\n","        [-2.0728e-03, -1.0087e-03,  1.6397e-03, -5.6428e-04],\n","        [-5.0056e-04, -1.1255e-04, -6.2165e-04, -1.0510e-03],\n","        [-1.4944e-03, -5.6474e-04,  8.1433e-05, -1.0745e-04],\n","        [ 7.1067e-04,  9.9567e-04, -7.5732e-04,  7.5603e-04],\n","        [ 1.0643e-03, -3.5184e-05,  1.4865e-03,  5.2131e-04],\n","        [ 8.4505e-04,  9.7393e-04, -1.0510e-03,  8.5640e-04],\n","        [-1.2641e-03, -5.6082e-04,  4.0343e-04, -3.1571e-04],\n","        [-1.5196e-03, -3.3079e-04,  5.1039e-04,  4.5390e-05],\n","        [ 1.1874e-03,  7.0751e-04, -1.0250e-03,  7.4633e-04],\n","        [ 4.8514e-04,  8.9898e-04, -1.0834e-04,  7.1936e-04],\n","        [ 7.7710e-04,  1.3277e-03, -1.3088e-03,  8.7194e-04],\n","        [-1.4797e-03,  3.2974e-04,  3.6963e-04,  3.5455e-04],\n","        [-1.5655e-03, -6.1090e-04,  4.1007e-04, -3.3679e-04],\n","        [-1.8797e-03, -5.6259e-04,  3.1347e-04, -3.0604e-04],\n","        [ 7.5610e-04,  1.2320e-03, -6.9101e-04,  8.3675e-04],\n","        [ 9.5542e-04,  1.5256e-03, -6.4088e-05,  3.0220e-04],\n","        [ 6.5708e-04, -2.9350e-04,  1.2823e-03,  2.6108e-04],\n","        [-3.9367e-04, -4.7658e-04, -1.0999e-04, -1.1013e-03],\n","        [ 8.3623e-04,  6.4899e-04, -2.9448e-04,  3.8683e-04],\n","        [ 3.9983e-04,  9.6545e-04, -1.9684e-04,  8.3105e-04],\n","        [-1.3912e-03, -7.4222e-04,  7.2024e-04, -5.8070e-04],\n","        [ 5.9472e-04,  1.1222e-03, -1.0167e-03,  8.2861e-04],\n","        [-2.4166e-04,  9.7081e-04,  2.2239e-05,  1.3376e-04],\n","        [-1.4566e-03, -3.9183e-05,  1.4785e-04, -2.0453e-04],\n","        [ 1.9534e-04,  8.6116e-04, -7.0718e-04,  9.1051e-04],\n","        [-1.3659e-03, -5.9765e-04,  3.3665e-04, -3.1493e-04],\n","        [-1.3036e-03, -6.1388e-04,  3.4724e-04, -2.9238e-04],\n","        [ 5.2082e-04,  1.9034e-04,  6.5777e-04,  1.4074e-04],\n","        [ 9.8509e-04, -3.7335e-04,  1.1907e-03,  4.6085e-04],\n","        [-1.3836e-03, -6.8512e-04,  4.1627e-04, -3.4441e-04],\n","        [ 9.1562e-04,  4.9957e-04, -4.6776e-05,  3.6372e-04],\n","        [ 3.9583e-04,  1.0240e-03, -5.8930e-05,  6.1946e-04],\n","        [ 5.5627e-04,  1.0713e-03, -6.9892e-04,  8.7227e-04],\n","        [ 3.2512e-04,  4.3307e-04,  9.3852e-04, -4.0131e-04],\n","        [ 1.1449e-03, -6.3048e-04,  1.3760e-03, -2.5698e-04],\n","        [-1.8676e-03, -5.1141e-04,  3.2332e-04, -2.9124e-04],\n","        [ 1.4021e-03,  7.9516e-04, -7.1256e-04,  6.3654e-04],\n","        [ 1.1918e-03,  7.1674e-04, -1.0572e-03,  7.2733e-04],\n","        [-5.0382e-04,  4.8225e-05, -4.5273e-04, -9.7748e-04],\n","        [-1.5063e-04, -7.0409e-04, -5.2769e-04, -1.0818e-03],\n","        [-1.5249e-03,  2.6362e-04,  2.6441e-04,  4.2665e-04],\n","        [-1.3168e-03, -6.4075e-04,  2.9527e-04, -3.5544e-04],\n","        [-8.2657e-04, -1.3471e-04, -6.0093e-04, -5.3237e-04],\n","        [-1.5814e-03, -6.2310e-04,  9.7078e-05, -1.9614e-04],\n","        [-1.3455e-03, -6.0177e-04,  3.3941e-04, -3.0776e-04],\n","        [ 9.2928e-04,  9.9001e-04, -1.1153e-03,  7.7739e-04],\n","        [-1.2391e-03, -4.9514e-04,  3.9339e-04, -5.0634e-04],\n","        [-2.6175e-04,  8.1606e-04,  1.0981e-03,  4.4584e-04],\n","        [ 1.1646e-03,  6.9439e-04, -1.0073e-03,  7.3050e-04],\n","        [ 7.7054e-04,  5.5781e-04, -1.6034e-03,  4.8668e-04],\n","        [ 7.6421e-04, -1.0693e-03,  7.5890e-04, -5.7254e-04],\n","        [-1.2694e-03, -5.5456e-04,  3.9589e-04, -3.2102e-04],\n","        [-1.0758e-03, -2.2900e-04, -1.7084e-04,  6.3213e-06],\n","        [-1.4379e-03,  3.1108e-04,  3.6314e-04,  3.4690e-04],\n","        [ 4.6303e-04,  1.0078e-03, -1.2674e-04,  9.0326e-04],\n","        [ 7.0706e-04,  5.6815e-04,  8.1737e-04,  6.5673e-04],\n","        [ 8.0166e-04,  6.1101e-04,  7.6167e-04,  5.7518e-04],\n","        [-1.5250e-03, -6.6236e-05,  1.4048e-04, -1.8601e-04],\n","        [-8.3261e-05,  3.2434e-04,  1.1641e-03,  3.8460e-04],\n","        [-2.8459e-04,  1.5309e-04, -6.6465e-04, -1.2863e-03],\n","        [-1.3195e-03, -5.4314e-04,  3.0859e-04, -2.4692e-04],\n","        [-9.9751e-04,  7.4346e-05, -5.5224e-05, -1.6808e-03],\n","        [ 6.9638e-05,  8.4860e-04,  9.7325e-04,  6.9166e-04],\n","        [ 7.3629e-04,  1.0972e-03, -5.5595e-04,  7.9404e-04],\n","        [ 1.1777e-03,  1.0988e-03, -4.9922e-04,  5.6294e-04],\n","        [ 5.5635e-04,  1.8712e-04, -1.1587e-03,  1.6337e-04],\n","        [ 9.0188e-04, -1.3409e-03,  6.7857e-04, -7.5656e-04],\n","        [-1.3368e-03, -4.9767e-04,  4.2473e-04, -2.8987e-04],\n","        [ 1.3768e-03,  7.3422e-04, -1.0687e-03,  9.0357e-04],\n","        [-2.0965e-04, -4.0932e-04, -4.5584e-05, -1.6382e-03],\n","        [ 7.8473e-04,  1.6208e-04,  1.0875e-03, -3.2158e-04],\n","        [-1.6247e-03, -5.8780e-04,  3.9270e-04, -3.6119e-04],\n","        [-1.3422e-03, -4.8224e-04,  4.1259e-04, -2.9483e-04],\n","        [ 3.5070e-04,  9.4260e-04, -5.2913e-05,  8.2088e-04],\n","        [ 9.4538e-04,  5.3776e-04, -1.3102e-04,  3.4976e-04],\n","        [-1.2334e-03, -4.9551e-04,  3.9826e-04, -5.0484e-04],\n","        [-8.3319e-05, -6.1612e-04,  1.1034e-03, -1.5136e-03],\n","        [ 1.5727e-04, -6.4590e-04,  6.8629e-04, -9.4228e-04],\n","        [ 8.1879e-04, -1.2354e-03,  8.3247e-04, -6.3745e-04]])\n","Train Loss 1: 1.1594 Acc: 0.1607\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MN3gKf3D8iut","executionInfo":{"status":"ok","timestamp":1630757463472,"user_tz":-540,"elapsed":2194,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"35d0b61d-c92e-426e-999f-16ff7262152f"},"source":["\"\"\"\n","def convert_json( project, folder, model, input_x, json_path, rand_flag=0 ):\n","\n","    #folder = \"src\"\n","    cpp_fname   = project + \".cpp\"\n","    param_fname = project + \"_param.cpp\"\n","    cpp_path    = folder + \"/\" + cpp_fname\n","    param_path  = folder + \"/\" + param_fname\n","    make_path   = folder + \"/\" + \"Makefile\"\n","\n","    # load json file\n","    print( \"[JSON]\", json_path )\n","    fp = open( json_path )\n","    obj = json.load( fp )\n","\n","    # save parameter file\n","    code1 = CV.c_param_generator( obj, model, input_x )\n","    if len( code1 ) > 0:\n","       print( \"[PARAM]\", param_path )\n","       ofparam = open( param_path, \"w\" )\n","       ofparam.write( code1 )\n","\n","    # save cpp file\n","    print( \"[CPP]  \", cpp_path )\n","    code2 = CV.c_code_generator( obj, model, rand_flag )\n","\n","    #ofp=open(args.path+\"/\"+args.output,\"w\")\n","    ofp = open( cpp_path, \"w\" )\n","    ofp.write( code2 )\n","\n","    # save make file\n","    print( \"[MAKE] \", make_path )\n","    make_code = CV.makefile_generator( cpp_fname )\n","\n","    #makefp=open(args.path+\"/\"+\"Makefile\",\"w\")\n","    makefp = open( make_path, \"w\" )\n","    makefp.write( make_code )\n","\"\"\"\n","CV.convert_json( project, \"src\", model, inputs, json_path )"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[JSON] network/cse1.json\n","{'name': 'Net/Linear[fc1]/weight/35', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [3], 'sorted_id': 1}\n","{'name': 'Net/Linear[fc1]/bias/34', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [3], 'sorted_id': 2}\n","{'name': 'Net/Linear[fc2]/weight/38', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [7], 'sorted_id': 5}\n","{'name': 'Net/Linear[fc2]/bias/37', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [7], 'sorted_id': 6}\n","[PARAM] src/cse1_param.cpp\n","{'name': 'input/x', 'op': 'IO Node', 'in': [], 'shape': [112, 4], 'out': [3], 'sorted_id': 0}\n","{'name': 'Net/Linear[fc1]/weight/35', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [3], 'sorted_id': 1}\n","Net/Linear[fc1]/weight/35  ->  fc1_weight\n","{'name': 'Net/Linear[fc1]/bias/34', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [3], 'sorted_id': 2}\n","Net/Linear[fc1]/bias/34  ->  fc1_bias\n","{'name': 'Net/Linear[fc1]/input.1', 'op': 'aten::linear', 'in': [0, 1, 2], 'shape': [112, 64], 'out': [4], 'sorted_id': 3}\n","{'name': 'Net/input.3', 'op': 'aten::relu', 'in': [3], 'shape': [112, 64], 'out': [7], 'sorted_id': 4}\n","{'name': 'Net/Linear[fc2]/weight/38', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [7], 'sorted_id': 5}\n","Net/Linear[fc2]/weight/38  ->  fc2_weight\n","{'name': 'Net/Linear[fc2]/bias/37', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [7], 'sorted_id': 6}\n","Net/Linear[fc2]/bias/37  ->  fc2_bias\n","{'name': 'Net/Linear[fc2]/input', 'op': 'aten::linear', 'in': [4, 5, 6], 'shape': [112, 3], 'out': [12], 'sorted_id': 7}\n","{'name': 'Net/17', 'op': 'prim::Constant', 'in': [], 'shape': [112], 'constant_value': [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 0.0, 1.0, 1.0, 0.0, 2.0, 2.0, 2.0, 1.0, 1.0, 0.0, 2.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 2.0, 0.0, 1.0, 1.0, 1.0], 'out': [12], 'sorted_id': 8}\n","{'name': 'Net/18', 'op': 'prim::Constant', 'in': [], 'shape': [], 'out': [12], 'sorted_id': 9}\n","{'name': 'Net/19', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1.0, 'out': [12], 'sorted_id': 10}\n","{'name': 'Net/20', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': -100.0, 'out': [12], 'sorted_id': 11}\n","{'name': 'Net/21', 'op': 'aten::cross_entropy_loss', 'in': [7, 8, 9, 10, 11], 'shape': [], 'out': [13], 'sorted_id': 12}\n","{'name': 'output/output.1', 'op': 'IO Node', 'in': [12], 'shape': [], 'out': [], 'sorted_id': 13}\n","[CPP]  src/cse1.cpp\n","[MAKE] src/Makefile\n"]}]},{"cell_type":"code","metadata":{"id":"zf1OiQzc9u5t","executionInfo":{"status":"ok","timestamp":1630758212994,"user_tz":-540,"elapsed":748013,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}}},"source":["!g++ -std=c++14 ./src/cse1.cpp ./src/cse1_param.cpp -I ../../ctorch/lib -lblas -o ./bin/cse1"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4bIvl823mZVt"},"source":["(注意) ctorch/libにxtensor関連のincludeを置いています。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I31lNv_hh4s2","executionInfo":{"status":"ok","timestamp":1630758340078,"user_tz":-540,"elapsed":2769,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"f184887d-edfb-407f-b24a-e793313b0d89"},"source":["!./bin/cse1"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["### forward computation ...\n","ashape1123\n"," 1.159395\n","### backward computation ...\n","input_grad{{ 0.00082 , -0.000603,  0.00139 ,  0.000559},\n"," { 0.001123, -0.000081,  0.001604,  0.000121},\n"," {-0.001463, -0.000601,  0.000422, -0.000325},\n"," {-0.000266,  0.000936,  0.000065,  0.00013 },\n"," { 0.000381,  0.000362,  0.00097 ,  0.000006},\n"," { 0.000191, -0.000598,  0.0002  , -0.001294},\n"," {-0.001347, -0.000554,  0.000254, -0.000271},\n"," { 0.000442,  0.001073, -0.000085,  0.000649},\n"," { 0.001358,  0.001022, -0.000177,  0.00002 },\n"," { 0.001204,  0.000644, -0.000928,  0.00071 },\n"," {-0.001356, -0.000424,  0.000375, -0.000469},\n"," { 0.000642,  0.000732, -0.000637,  0.000665},\n"," {-0.001471,  0.000328,  0.000367,  0.000352},\n"," {-0.001323, -0.000004,  0.00001 ,  0.000292},\n"," {-0.001307, -0.000455,  0.000407, -0.000461},\n"," {-0.001503, -0.000642,  0.000117, -0.000173},\n"," { 0.000308,  0.001088, -0.000145,  0.000714},\n"," {-0.001357, -0.000466,  0.000502, -0.000483},\n"," {-0.001619, -0.000506, -0.000102, -0.000042},\n"," { 0.000541, -0.000161,  0.0012  , -0.000832},\n"," {-0.00049 , -0.000171,  0.000015, -0.001574},\n"," { 0.000761,  0.00114 , -0.000853,  0.000967},\n"," { 0.000454,  0.001251, -0.000941,  0.000975},\n"," { 0.000196, -0.000398, -0.000498, -0.001536},\n"," { 0.000875,  0.000907, -0.001098,  0.000926},\n"," {-0.001959,  0.000423,  0.000595,  0.0002  },\n"," { 0.000858, -0.000244,  0.001136, -0.000158},\n"," { 0.001065,  0.000326, -0.00025 ,  0.000439},\n"," { 0.001109,  0.000778, -0.000634,  0.000873},\n"," { 0.001407,  0.00081 , -0.00081 ,  0.000583},\n"," {-0.000602, -0.000023, -0.000346, -0.001192},\n"," { 0.00042 , -0.000045,  0.001325, -0.000669},\n"," {-0.001531,  0.00039 ,  0.000362,  0.000355},\n"," {-0.002073, -0.001009,  0.00164 , -0.000564},\n"," {-0.000501, -0.000113, -0.000622, -0.001051},\n"," {-0.001494, -0.000565,  0.000081, -0.000107},\n"," { 0.000711,  0.000996, -0.000757,  0.000756},\n"," { 0.001064, -0.000035,  0.001486,  0.000521},\n"," { 0.000845,  0.000974, -0.001051,  0.000856},\n"," {-0.001264, -0.000561,  0.000403, -0.000316},\n"," {-0.00152 , -0.000331,  0.00051 ,  0.000045},\n"," { 0.001187,  0.000708, -0.001025,  0.000746},\n"," { 0.000485,  0.000899, -0.000108,  0.000719},\n"," { 0.000777,  0.001328, -0.001309,  0.000872},\n"," {-0.00148 ,  0.00033 ,  0.00037 ,  0.000355},\n"," {-0.001566, -0.000611,  0.00041 , -0.000337},\n"," {-0.00188 , -0.000563,  0.000313, -0.000306},\n"," { 0.000756,  0.001232, -0.000691,  0.000837},\n"," { 0.000955,  0.001526, -0.000064,  0.000302},\n"," { 0.000657, -0.000293,  0.001282,  0.000261},\n"," {-0.000394, -0.000477, -0.00011 , -0.001101},\n"," { 0.000836,  0.000649, -0.000294,  0.000387},\n"," { 0.0004  ,  0.000965, -0.000197,  0.000831},\n"," {-0.001391, -0.000742,  0.00072 , -0.000581},\n"," { 0.000595,  0.001122, -0.001017,  0.000829},\n"," {-0.000242,  0.000971,  0.000022,  0.000134},\n"," {-0.001457, -0.000039,  0.000148, -0.000205},\n"," { 0.000195,  0.000861, -0.000707,  0.000911},\n"," {-0.001366, -0.000598,  0.000337, -0.000315},\n"," {-0.001304, -0.000614,  0.000347, -0.000292},\n"," { 0.000521,  0.00019 ,  0.000658,  0.000141},\n"," { 0.000985, -0.000373,  0.001191,  0.000461},\n"," {-0.001384, -0.000685,  0.000416, -0.000344},\n"," { 0.000916,  0.0005  , -0.000047,  0.000364},\n"," { 0.000396,  0.001024, -0.000059,  0.000619},\n"," { 0.000556,  0.001071, -0.000699,  0.000872},\n"," { 0.000325,  0.000433,  0.000939, -0.000401},\n"," { 0.001145, -0.00063 ,  0.001376, -0.000257},\n"," {-0.001868, -0.000511,  0.000323, -0.000291},\n"," { 0.001402,  0.000795, -0.000713,  0.000637},\n"," { 0.001192,  0.000717, -0.001057,  0.000727},\n"," {-0.000504,  0.000048, -0.000453, -0.000977},\n"," {-0.000151, -0.000704, -0.000528, -0.001082},\n"," {-0.001525,  0.000264,  0.000264,  0.000427},\n"," {-0.001317, -0.000641,  0.000295, -0.000355},\n"," {-0.000827, -0.000135, -0.000601, -0.000532},\n"," {-0.001581, -0.000623,  0.000097, -0.000196},\n"," {-0.001345, -0.000602,  0.000339, -0.000308},\n"," { 0.000929,  0.00099 , -0.001115,  0.000777},\n"," {-0.001239, -0.000495,  0.000393, -0.000506},\n"," {-0.000262,  0.000816,  0.001098,  0.000446},\n"," { 0.001165,  0.000694, -0.001007,  0.00073 },\n"," { 0.000771,  0.000558, -0.001603,  0.000487},\n"," { 0.000764, -0.001069,  0.000759, -0.000573},\n"," {-0.001269, -0.000555,  0.000396, -0.000321},\n"," {-0.001076, -0.000229, -0.000171,  0.000006},\n"," {-0.001438,  0.000311,  0.000363,  0.000347},\n"," { 0.000463,  0.001008, -0.000127,  0.000903},\n"," { 0.000707,  0.000568,  0.000817,  0.000657},\n"," { 0.000802,  0.000611,  0.000762,  0.000575},\n"," {-0.001525, -0.000066,  0.00014 , -0.000186},\n"," {-0.000083,  0.000324,  0.001164,  0.000385},\n"," {-0.000285,  0.000153, -0.000665, -0.001286},\n"," {-0.00132 , -0.000543,  0.000309, -0.000247},\n"," {-0.000998,  0.000074, -0.000055, -0.001681},\n"," { 0.00007 ,  0.000849,  0.000973,  0.000692},\n"," { 0.000736,  0.001097, -0.000556,  0.000794},\n"," { 0.001178,  0.001099, -0.000499,  0.000563},\n"," { 0.000556,  0.000187, -0.001159,  0.000163},\n"," { 0.000902, -0.001341,  0.000679, -0.000757},\n"," {-0.001337, -0.000498,  0.000425, -0.00029 },\n"," { 0.001377,  0.000734, -0.001069,  0.000904},\n"," {-0.00021 , -0.000409, -0.000046, -0.001638},\n"," { 0.000785,  0.000162,  0.001087, -0.000322},\n"," {-0.001625, -0.000588,  0.000393, -0.000361},\n"," {-0.001342, -0.000482,  0.000413, -0.000295},\n"," { 0.000351,  0.000943, -0.000053,  0.000821},\n"," { 0.000945,  0.000538, -0.000131,  0.00035 },\n"," {-0.001233, -0.000496,  0.000398, -0.000505},\n"," {-0.000083, -0.000616,  0.001103, -0.001514},\n"," { 0.000157, -0.000646,  0.000686, -0.000942},\n"," { 0.000819, -0.001235,  0.000832, -0.000637}}\n"]}]},{"cell_type":"code","metadata":{"id":"-AaUeeTX6UmM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629784821977,"user_tz":-540,"elapsed":1498,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"440dd362-cab6-4cc6-95e3-76638ca7c0b3"},"source":["torch.manual_seed( 1 )\n","\n","#print(\"target\",target)\n","inputs.requires_grad = True\n","\n","#model = Net( labels )\n","model = Net2()\n","\n","num = inputs.size(0)\n","\n","project = 'test5'\n","#json_path = 'network/' + project +'.json'\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD( model.parameters(), lr=0.01 )\n","\n","num_epochs = 300\n","\n","acc = []\n","\n","for epoch in range(num_epochs):\n","  with torch.set_grad_enabled(True):\n","\n","    model.train()   # モデルを訓練モードに設定\n","\n","    outputs = model( inputs )\n","    #print(outputs)\n","    #print(labels)\n","\n","    #print(\"input grad\",inputs.grad)\n","\n","    loss = criterion( outputs, labels )\n","    print(\"loss \",epoch, \" - \",loss)\n","\n","    # ラベルを予測\n","    #print(\"output\", outputs, num )\n","    _, preds = torch.max( outputs, 1 )\n","    #print(labels)\n","    #print(preds)\n","\n","    optimizer.zero_grad()\n","\n","    # 逆伝搬の計算\n","    loss.backward()\n","                    \n","    # パラメータの更新\n","    optimizer.step()\n","\n","    # イテレーション結果の計算\n","    epoch_loss = loss.item() * float(num)\n","\n","    # 正解数の合計を更新\n","    epoch_corrects = torch.sum( preds == labels )\n","\n","    epoch_loss = epoch_loss / float(num)\n","    epoch_acc  = epoch_corrects.double() / float(num)\n","    print('Train Loss {}: {:.4f} Acc: {:.4f} {}'.format( epoch, epoch_loss, epoch_acc, epoch_corrects ))\n","\n","    acc.append( epoch_acc )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loss  0  -  tensor(1.1457, grad_fn=<NllLossBackward>)\n","Train Loss 0: 1.1457 Acc: 0.2054 23\n","loss  1  -  tensor(1.1206, grad_fn=<NllLossBackward>)\n","Train Loss 1: 1.1206 Acc: 0.2321 26\n","loss  2  -  tensor(1.0965, grad_fn=<NllLossBackward>)\n","Train Loss 2: 1.0965 Acc: 0.2232 25\n","loss  3  -  tensor(1.0735, grad_fn=<NllLossBackward>)\n","Train Loss 3: 1.0735 Acc: 0.2321 26\n","loss  4  -  tensor(1.0514, grad_fn=<NllLossBackward>)\n","Train Loss 4: 1.0514 Acc: 0.2500 28\n","loss  5  -  tensor(1.0303, grad_fn=<NllLossBackward>)\n","Train Loss 5: 1.0303 Acc: 0.2500 28\n","loss  6  -  tensor(1.0101, grad_fn=<NllLossBackward>)\n","Train Loss 6: 1.0101 Acc: 0.2679 30\n","loss  7  -  tensor(0.9907, grad_fn=<NllLossBackward>)\n","Train Loss 7: 0.9907 Acc: 0.3036 34\n","loss  8  -  tensor(0.9722, grad_fn=<NllLossBackward>)\n","Train Loss 8: 0.9722 Acc: 0.4286 48\n","loss  9  -  tensor(0.9545, grad_fn=<NllLossBackward>)\n","Train Loss 9: 0.9545 Acc: 0.4732 53\n","loss  10  -  tensor(0.9376, grad_fn=<NllLossBackward>)\n","Train Loss 10: 0.9376 Acc: 0.5268 59\n","loss  11  -  tensor(0.9215, grad_fn=<NllLossBackward>)\n","Train Loss 11: 0.9215 Acc: 0.5625 63\n","loss  12  -  tensor(0.9060, grad_fn=<NllLossBackward>)\n","Train Loss 12: 0.9060 Acc: 0.5982 67\n","loss  13  -  tensor(0.8912, grad_fn=<NllLossBackward>)\n","Train Loss 13: 0.8912 Acc: 0.6339 71\n","loss  14  -  tensor(0.8771, grad_fn=<NllLossBackward>)\n","Train Loss 14: 0.8771 Acc: 0.6875 77\n","loss  15  -  tensor(0.8636, grad_fn=<NllLossBackward>)\n","Train Loss 15: 0.8636 Acc: 0.6964 78\n","loss  16  -  tensor(0.8507, grad_fn=<NllLossBackward>)\n","Train Loss 16: 0.8507 Acc: 0.7054 79\n","loss  17  -  tensor(0.8383, grad_fn=<NllLossBackward>)\n","Train Loss 17: 0.8383 Acc: 0.7054 79\n","loss  18  -  tensor(0.8265, grad_fn=<NllLossBackward>)\n","Train Loss 18: 0.8265 Acc: 0.7054 79\n","loss  19  -  tensor(0.8152, grad_fn=<NllLossBackward>)\n","Train Loss 19: 0.8152 Acc: 0.7054 79\n","loss  20  -  tensor(0.8043, grad_fn=<NllLossBackward>)\n","Train Loss 20: 0.8043 Acc: 0.7054 79\n","loss  21  -  tensor(0.7939, grad_fn=<NllLossBackward>)\n","Train Loss 21: 0.7939 Acc: 0.7054 79\n","loss  22  -  tensor(0.7839, grad_fn=<NllLossBackward>)\n","Train Loss 22: 0.7839 Acc: 0.7054 79\n","loss  23  -  tensor(0.7744, grad_fn=<NllLossBackward>)\n","Train Loss 23: 0.7744 Acc: 0.7054 79\n","loss  24  -  tensor(0.7652, grad_fn=<NllLossBackward>)\n","Train Loss 24: 0.7652 Acc: 0.7143 80\n","loss  25  -  tensor(0.7564, grad_fn=<NllLossBackward>)\n","Train Loss 25: 0.7564 Acc: 0.7143 80\n","loss  26  -  tensor(0.7479, grad_fn=<NllLossBackward>)\n","Train Loss 26: 0.7479 Acc: 0.7143 80\n","loss  27  -  tensor(0.7398, grad_fn=<NllLossBackward>)\n","Train Loss 27: 0.7398 Acc: 0.7143 80\n","loss  28  -  tensor(0.7320, grad_fn=<NllLossBackward>)\n","Train Loss 28: 0.7320 Acc: 0.7143 80\n","loss  29  -  tensor(0.7245, grad_fn=<NllLossBackward>)\n","Train Loss 29: 0.7245 Acc: 0.7143 80\n","loss  30  -  tensor(0.7172, grad_fn=<NllLossBackward>)\n","Train Loss 30: 0.7172 Acc: 0.7143 80\n","loss  31  -  tensor(0.7102, grad_fn=<NllLossBackward>)\n","Train Loss 31: 0.7102 Acc: 0.7143 80\n","loss  32  -  tensor(0.7035, grad_fn=<NllLossBackward>)\n","Train Loss 32: 0.7035 Acc: 0.7143 80\n","loss  33  -  tensor(0.6970, grad_fn=<NllLossBackward>)\n","Train Loss 33: 0.6970 Acc: 0.7143 80\n","loss  34  -  tensor(0.6907, grad_fn=<NllLossBackward>)\n","Train Loss 34: 0.6907 Acc: 0.7143 80\n","loss  35  -  tensor(0.6847, grad_fn=<NllLossBackward>)\n","Train Loss 35: 0.6847 Acc: 0.7232 81\n","loss  36  -  tensor(0.6788, grad_fn=<NllLossBackward>)\n","Train Loss 36: 0.6788 Acc: 0.7232 81\n","loss  37  -  tensor(0.6732, grad_fn=<NllLossBackward>)\n","Train Loss 37: 0.6732 Acc: 0.7232 81\n","loss  38  -  tensor(0.6677, grad_fn=<NllLossBackward>)\n","Train Loss 38: 0.6677 Acc: 0.7232 81\n","loss  39  -  tensor(0.6624, grad_fn=<NllLossBackward>)\n","Train Loss 39: 0.6624 Acc: 0.7232 81\n","loss  40  -  tensor(0.6573, grad_fn=<NllLossBackward>)\n","Train Loss 40: 0.6573 Acc: 0.7321 82\n","loss  41  -  tensor(0.6523, grad_fn=<NllLossBackward>)\n","Train Loss 41: 0.6523 Acc: 0.7321 82\n","loss  42  -  tensor(0.6475, grad_fn=<NllLossBackward>)\n","Train Loss 42: 0.6475 Acc: 0.7321 82\n","loss  43  -  tensor(0.6428, grad_fn=<NllLossBackward>)\n","Train Loss 43: 0.6428 Acc: 0.7321 82\n","loss  44  -  tensor(0.6383, grad_fn=<NllLossBackward>)\n","Train Loss 44: 0.6383 Acc: 0.7321 82\n","loss  45  -  tensor(0.6338, grad_fn=<NllLossBackward>)\n","Train Loss 45: 0.6338 Acc: 0.7321 82\n","loss  46  -  tensor(0.6296, grad_fn=<NllLossBackward>)\n","Train Loss 46: 0.6296 Acc: 0.7321 82\n","loss  47  -  tensor(0.6254, grad_fn=<NllLossBackward>)\n","Train Loss 47: 0.6254 Acc: 0.7321 82\n","loss  48  -  tensor(0.6213, grad_fn=<NllLossBackward>)\n","Train Loss 48: 0.6213 Acc: 0.7321 82\n","loss  49  -  tensor(0.6174, grad_fn=<NllLossBackward>)\n","Train Loss 49: 0.6174 Acc: 0.7321 82\n","loss  50  -  tensor(0.6135, grad_fn=<NllLossBackward>)\n","Train Loss 50: 0.6135 Acc: 0.7321 82\n","loss  51  -  tensor(0.6098, grad_fn=<NllLossBackward>)\n","Train Loss 51: 0.6098 Acc: 0.7321 82\n","loss  52  -  tensor(0.6061, grad_fn=<NllLossBackward>)\n","Train Loss 52: 0.6061 Acc: 0.7411 83\n","loss  53  -  tensor(0.6025, grad_fn=<NllLossBackward>)\n","Train Loss 53: 0.6025 Acc: 0.7411 83\n","loss  54  -  tensor(0.5990, grad_fn=<NllLossBackward>)\n","Train Loss 54: 0.5990 Acc: 0.7411 83\n","loss  55  -  tensor(0.5957, grad_fn=<NllLossBackward>)\n","Train Loss 55: 0.5957 Acc: 0.7411 83\n","loss  56  -  tensor(0.5923, grad_fn=<NllLossBackward>)\n","Train Loss 56: 0.5923 Acc: 0.7411 83\n","loss  57  -  tensor(0.5891, grad_fn=<NllLossBackward>)\n","Train Loss 57: 0.5891 Acc: 0.7500 84\n","loss  58  -  tensor(0.5859, grad_fn=<NllLossBackward>)\n","Train Loss 58: 0.5859 Acc: 0.7500 84\n","loss  59  -  tensor(0.5828, grad_fn=<NllLossBackward>)\n","Train Loss 59: 0.5828 Acc: 0.7500 84\n","loss  60  -  tensor(0.5798, grad_fn=<NllLossBackward>)\n","Train Loss 60: 0.5798 Acc: 0.7500 84\n","loss  61  -  tensor(0.5768, grad_fn=<NllLossBackward>)\n","Train Loss 61: 0.5768 Acc: 0.7500 84\n","loss  62  -  tensor(0.5739, grad_fn=<NllLossBackward>)\n","Train Loss 62: 0.5739 Acc: 0.7500 84\n","loss  63  -  tensor(0.5710, grad_fn=<NllLossBackward>)\n","Train Loss 63: 0.5710 Acc: 0.7500 84\n","loss  64  -  tensor(0.5683, grad_fn=<NllLossBackward>)\n","Train Loss 64: 0.5683 Acc: 0.7500 84\n","loss  65  -  tensor(0.5655, grad_fn=<NllLossBackward>)\n","Train Loss 65: 0.5655 Acc: 0.7589 85\n","loss  66  -  tensor(0.5628, grad_fn=<NllLossBackward>)\n","Train Loss 66: 0.5628 Acc: 0.7589 85\n","loss  67  -  tensor(0.5602, grad_fn=<NllLossBackward>)\n","Train Loss 67: 0.5602 Acc: 0.7589 85\n","loss  68  -  tensor(0.5576, grad_fn=<NllLossBackward>)\n","Train Loss 68: 0.5576 Acc: 0.7589 85\n","loss  69  -  tensor(0.5551, grad_fn=<NllLossBackward>)\n","Train Loss 69: 0.5551 Acc: 0.7589 85\n","loss  70  -  tensor(0.5526, grad_fn=<NllLossBackward>)\n","Train Loss 70: 0.5526 Acc: 0.7589 85\n","loss  71  -  tensor(0.5502, grad_fn=<NllLossBackward>)\n","Train Loss 71: 0.5502 Acc: 0.7589 85\n","loss  72  -  tensor(0.5478, grad_fn=<NllLossBackward>)\n","Train Loss 72: 0.5478 Acc: 0.7589 85\n","loss  73  -  tensor(0.5454, grad_fn=<NllLossBackward>)\n","Train Loss 73: 0.5454 Acc: 0.7589 85\n","loss  74  -  tensor(0.5431, grad_fn=<NllLossBackward>)\n","Train Loss 74: 0.5431 Acc: 0.7679 86\n","loss  75  -  tensor(0.5408, grad_fn=<NllLossBackward>)\n","Train Loss 75: 0.5408 Acc: 0.7679 86\n","loss  76  -  tensor(0.5386, grad_fn=<NllLossBackward>)\n","Train Loss 76: 0.5386 Acc: 0.7679 86\n","loss  77  -  tensor(0.5364, grad_fn=<NllLossBackward>)\n","Train Loss 77: 0.5364 Acc: 0.7768 87\n","loss  78  -  tensor(0.5342, grad_fn=<NllLossBackward>)\n","Train Loss 78: 0.5342 Acc: 0.7768 87\n","loss  79  -  tensor(0.5320, grad_fn=<NllLossBackward>)\n","Train Loss 79: 0.5320 Acc: 0.7768 87\n","loss  80  -  tensor(0.5299, grad_fn=<NllLossBackward>)\n","Train Loss 80: 0.5299 Acc: 0.7768 87\n","loss  81  -  tensor(0.5279, grad_fn=<NllLossBackward>)\n","Train Loss 81: 0.5279 Acc: 0.7768 87\n","loss  82  -  tensor(0.5258, grad_fn=<NllLossBackward>)\n","Train Loss 82: 0.5258 Acc: 0.7768 87\n","loss  83  -  tensor(0.5238, grad_fn=<NllLossBackward>)\n","Train Loss 83: 0.5238 Acc: 0.7768 87\n","loss  84  -  tensor(0.5219, grad_fn=<NllLossBackward>)\n","Train Loss 84: 0.5219 Acc: 0.7857 88\n","loss  85  -  tensor(0.5199, grad_fn=<NllLossBackward>)\n","Train Loss 85: 0.5199 Acc: 0.7946 89\n","loss  86  -  tensor(0.5180, grad_fn=<NllLossBackward>)\n","Train Loss 86: 0.5180 Acc: 0.7946 89\n","loss  87  -  tensor(0.5161, grad_fn=<NllLossBackward>)\n","Train Loss 87: 0.5161 Acc: 0.7946 89\n","loss  88  -  tensor(0.5142, grad_fn=<NllLossBackward>)\n","Train Loss 88: 0.5142 Acc: 0.7946 89\n","loss  89  -  tensor(0.5124, grad_fn=<NllLossBackward>)\n","Train Loss 89: 0.5124 Acc: 0.7946 89\n","loss  90  -  tensor(0.5106, grad_fn=<NllLossBackward>)\n","Train Loss 90: 0.5106 Acc: 0.7946 89\n","loss  91  -  tensor(0.5088, grad_fn=<NllLossBackward>)\n","Train Loss 91: 0.5088 Acc: 0.7946 89\n","loss  92  -  tensor(0.5070, grad_fn=<NllLossBackward>)\n","Train Loss 92: 0.5070 Acc: 0.7946 89\n","loss  93  -  tensor(0.5053, grad_fn=<NllLossBackward>)\n","Train Loss 93: 0.5053 Acc: 0.7946 89\n","loss  94  -  tensor(0.5036, grad_fn=<NllLossBackward>)\n","Train Loss 94: 0.5036 Acc: 0.7946 89\n","loss  95  -  tensor(0.5019, grad_fn=<NllLossBackward>)\n","Train Loss 95: 0.5019 Acc: 0.7946 89\n","loss  96  -  tensor(0.5002, grad_fn=<NllLossBackward>)\n","Train Loss 96: 0.5002 Acc: 0.7946 89\n","loss  97  -  tensor(0.4985, grad_fn=<NllLossBackward>)\n","Train Loss 97: 0.4985 Acc: 0.7946 89\n","loss  98  -  tensor(0.4969, grad_fn=<NllLossBackward>)\n","Train Loss 98: 0.4969 Acc: 0.7946 89\n","loss  99  -  tensor(0.4953, grad_fn=<NllLossBackward>)\n","Train Loss 99: 0.4953 Acc: 0.7946 89\n","loss  100  -  tensor(0.4937, grad_fn=<NllLossBackward>)\n","Train Loss 100: 0.4937 Acc: 0.7946 89\n","loss  101  -  tensor(0.4921, grad_fn=<NllLossBackward>)\n","Train Loss 101: 0.4921 Acc: 0.7946 89\n","loss  102  -  tensor(0.4905, grad_fn=<NllLossBackward>)\n","Train Loss 102: 0.4905 Acc: 0.8036 90\n","loss  103  -  tensor(0.4890, grad_fn=<NllLossBackward>)\n","Train Loss 103: 0.4890 Acc: 0.8036 90\n","loss  104  -  tensor(0.4875, grad_fn=<NllLossBackward>)\n","Train Loss 104: 0.4875 Acc: 0.8036 90\n","loss  105  -  tensor(0.4860, grad_fn=<NllLossBackward>)\n","Train Loss 105: 0.4860 Acc: 0.8036 90\n","loss  106  -  tensor(0.4845, grad_fn=<NllLossBackward>)\n","Train Loss 106: 0.4845 Acc: 0.8036 90\n","loss  107  -  tensor(0.4830, grad_fn=<NllLossBackward>)\n","Train Loss 107: 0.4830 Acc: 0.8036 90\n","loss  108  -  tensor(0.4816, grad_fn=<NllLossBackward>)\n","Train Loss 108: 0.4816 Acc: 0.8036 90\n","loss  109  -  tensor(0.4801, grad_fn=<NllLossBackward>)\n","Train Loss 109: 0.4801 Acc: 0.8036 90\n","loss  110  -  tensor(0.4787, grad_fn=<NllLossBackward>)\n","Train Loss 110: 0.4787 Acc: 0.8036 90\n","loss  111  -  tensor(0.4773, grad_fn=<NllLossBackward>)\n","Train Loss 111: 0.4773 Acc: 0.8125 91\n","loss  112  -  tensor(0.4759, grad_fn=<NllLossBackward>)\n","Train Loss 112: 0.4759 Acc: 0.8125 91\n","loss  113  -  tensor(0.4745, grad_fn=<NllLossBackward>)\n","Train Loss 113: 0.4745 Acc: 0.8125 91\n","loss  114  -  tensor(0.4732, grad_fn=<NllLossBackward>)\n","Train Loss 114: 0.4732 Acc: 0.8125 91\n","loss  115  -  tensor(0.4718, grad_fn=<NllLossBackward>)\n","Train Loss 115: 0.4718 Acc: 0.8125 91\n","loss  116  -  tensor(0.4705, grad_fn=<NllLossBackward>)\n","Train Loss 116: 0.4705 Acc: 0.8125 91\n","loss  117  -  tensor(0.4692, grad_fn=<NllLossBackward>)\n","Train Loss 117: 0.4692 Acc: 0.8125 91\n","loss  118  -  tensor(0.4679, grad_fn=<NllLossBackward>)\n","Train Loss 118: 0.4679 Acc: 0.8214 92\n","loss  119  -  tensor(0.4666, grad_fn=<NllLossBackward>)\n","Train Loss 119: 0.4666 Acc: 0.8214 92\n","loss  120  -  tensor(0.4653, grad_fn=<NllLossBackward>)\n","Train Loss 120: 0.4653 Acc: 0.8214 92\n","loss  121  -  tensor(0.4640, grad_fn=<NllLossBackward>)\n","Train Loss 121: 0.4640 Acc: 0.8125 91\n","loss  122  -  tensor(0.4628, grad_fn=<NllLossBackward>)\n","Train Loss 122: 0.4628 Acc: 0.8125 91\n","loss  123  -  tensor(0.4615, grad_fn=<NllLossBackward>)\n","Train Loss 123: 0.4615 Acc: 0.8125 91\n","loss  124  -  tensor(0.4603, grad_fn=<NllLossBackward>)\n","Train Loss 124: 0.4603 Acc: 0.8125 91\n","loss  125  -  tensor(0.4591, grad_fn=<NllLossBackward>)\n","Train Loss 125: 0.4591 Acc: 0.8125 91\n","loss  126  -  tensor(0.4579, grad_fn=<NllLossBackward>)\n","Train Loss 126: 0.4579 Acc: 0.8125 91\n","loss  127  -  tensor(0.4567, grad_fn=<NllLossBackward>)\n","Train Loss 127: 0.4567 Acc: 0.8125 91\n","loss  128  -  tensor(0.4555, grad_fn=<NllLossBackward>)\n","Train Loss 128: 0.4555 Acc: 0.8125 91\n","loss  129  -  tensor(0.4543, grad_fn=<NllLossBackward>)\n","Train Loss 129: 0.4543 Acc: 0.8125 91\n","loss  130  -  tensor(0.4532, grad_fn=<NllLossBackward>)\n","Train Loss 130: 0.4532 Acc: 0.8214 92\n","loss  131  -  tensor(0.4520, grad_fn=<NllLossBackward>)\n","Train Loss 131: 0.4520 Acc: 0.8214 92\n","loss  132  -  tensor(0.4509, grad_fn=<NllLossBackward>)\n","Train Loss 132: 0.4509 Acc: 0.8214 92\n","loss  133  -  tensor(0.4497, grad_fn=<NllLossBackward>)\n","Train Loss 133: 0.4497 Acc: 0.8214 92\n","loss  134  -  tensor(0.4486, grad_fn=<NllLossBackward>)\n","Train Loss 134: 0.4486 Acc: 0.8304 93\n","loss  135  -  tensor(0.4475, grad_fn=<NllLossBackward>)\n","Train Loss 135: 0.4475 Acc: 0.8304 93\n","loss  136  -  tensor(0.4464, grad_fn=<NllLossBackward>)\n","Train Loss 136: 0.4464 Acc: 0.8304 93\n","loss  137  -  tensor(0.4453, grad_fn=<NllLossBackward>)\n","Train Loss 137: 0.4453 Acc: 0.8304 93\n","loss  138  -  tensor(0.4442, grad_fn=<NllLossBackward>)\n","Train Loss 138: 0.4442 Acc: 0.8304 93\n","loss  139  -  tensor(0.4432, grad_fn=<NllLossBackward>)\n","Train Loss 139: 0.4432 Acc: 0.8304 93\n","loss  140  -  tensor(0.4421, grad_fn=<NllLossBackward>)\n","Train Loss 140: 0.4421 Acc: 0.8304 93\n","loss  141  -  tensor(0.4410, grad_fn=<NllLossBackward>)\n","Train Loss 141: 0.4410 Acc: 0.8304 93\n","loss  142  -  tensor(0.4400, grad_fn=<NllLossBackward>)\n","Train Loss 142: 0.4400 Acc: 0.8304 93\n","loss  143  -  tensor(0.4390, grad_fn=<NllLossBackward>)\n","Train Loss 143: 0.4390 Acc: 0.8393 94\n","loss  144  -  tensor(0.4379, grad_fn=<NllLossBackward>)\n","Train Loss 144: 0.4379 Acc: 0.8393 94\n","loss  145  -  tensor(0.4369, grad_fn=<NllLossBackward>)\n","Train Loss 145: 0.4369 Acc: 0.8393 94\n","loss  146  -  tensor(0.4359, grad_fn=<NllLossBackward>)\n","Train Loss 146: 0.4359 Acc: 0.8393 94\n","loss  147  -  tensor(0.4349, grad_fn=<NllLossBackward>)\n","Train Loss 147: 0.4349 Acc: 0.8393 94\n","loss  148  -  tensor(0.4339, grad_fn=<NllLossBackward>)\n","Train Loss 148: 0.4339 Acc: 0.8393 94\n","loss  149  -  tensor(0.4329, grad_fn=<NllLossBackward>)\n","Train Loss 149: 0.4329 Acc: 0.8393 94\n","loss  150  -  tensor(0.4320, grad_fn=<NllLossBackward>)\n","Train Loss 150: 0.4320 Acc: 0.8393 94\n","loss  151  -  tensor(0.4310, grad_fn=<NllLossBackward>)\n","Train Loss 151: 0.4310 Acc: 0.8482 95\n","loss  152  -  tensor(0.4300, grad_fn=<NllLossBackward>)\n","Train Loss 152: 0.4300 Acc: 0.8482 95\n","loss  153  -  tensor(0.4291, grad_fn=<NllLossBackward>)\n","Train Loss 153: 0.4291 Acc: 0.8482 95\n","loss  154  -  tensor(0.4281, grad_fn=<NllLossBackward>)\n","Train Loss 154: 0.4281 Acc: 0.8482 95\n","loss  155  -  tensor(0.4272, grad_fn=<NllLossBackward>)\n","Train Loss 155: 0.4272 Acc: 0.8482 95\n","loss  156  -  tensor(0.4263, grad_fn=<NllLossBackward>)\n","Train Loss 156: 0.4263 Acc: 0.8482 95\n","loss  157  -  tensor(0.4253, grad_fn=<NllLossBackward>)\n","Train Loss 157: 0.4253 Acc: 0.8482 95\n","loss  158  -  tensor(0.4244, grad_fn=<NllLossBackward>)\n","Train Loss 158: 0.4244 Acc: 0.8482 95\n","loss  159  -  tensor(0.4235, grad_fn=<NllLossBackward>)\n","Train Loss 159: 0.4235 Acc: 0.8482 95\n","loss  160  -  tensor(0.4226, grad_fn=<NllLossBackward>)\n","Train Loss 160: 0.4226 Acc: 0.8482 95\n","loss  161  -  tensor(0.4217, grad_fn=<NllLossBackward>)\n","Train Loss 161: 0.4217 Acc: 0.8482 95\n","loss  162  -  tensor(0.4208, grad_fn=<NllLossBackward>)\n","Train Loss 162: 0.4208 Acc: 0.8482 95\n","loss  163  -  tensor(0.4199, grad_fn=<NllLossBackward>)\n","Train Loss 163: 0.4199 Acc: 0.8482 95\n","loss  164  -  tensor(0.4191, grad_fn=<NllLossBackward>)\n","Train Loss 164: 0.4191 Acc: 0.8393 94\n","loss  165  -  tensor(0.4182, grad_fn=<NllLossBackward>)\n","Train Loss 165: 0.4182 Acc: 0.8393 94\n","loss  166  -  tensor(0.4173, grad_fn=<NllLossBackward>)\n","Train Loss 166: 0.4173 Acc: 0.8393 94\n","loss  167  -  tensor(0.4165, grad_fn=<NllLossBackward>)\n","Train Loss 167: 0.4165 Acc: 0.8393 94\n","loss  168  -  tensor(0.4156, grad_fn=<NllLossBackward>)\n","Train Loss 168: 0.4156 Acc: 0.8393 94\n","loss  169  -  tensor(0.4148, grad_fn=<NllLossBackward>)\n","Train Loss 169: 0.4148 Acc: 0.8393 94\n","loss  170  -  tensor(0.4139, grad_fn=<NllLossBackward>)\n","Train Loss 170: 0.4139 Acc: 0.8393 94\n","loss  171  -  tensor(0.4131, grad_fn=<NllLossBackward>)\n","Train Loss 171: 0.4131 Acc: 0.8393 94\n","loss  172  -  tensor(0.4123, grad_fn=<NllLossBackward>)\n","Train Loss 172: 0.4123 Acc: 0.8393 94\n","loss  173  -  tensor(0.4115, grad_fn=<NllLossBackward>)\n","Train Loss 173: 0.4115 Acc: 0.8393 94\n","loss  174  -  tensor(0.4107, grad_fn=<NllLossBackward>)\n","Train Loss 174: 0.4107 Acc: 0.8393 94\n","loss  175  -  tensor(0.4098, grad_fn=<NllLossBackward>)\n","Train Loss 175: 0.4098 Acc: 0.8393 94\n","loss  176  -  tensor(0.4090, grad_fn=<NllLossBackward>)\n","Train Loss 176: 0.4090 Acc: 0.8393 94\n","loss  177  -  tensor(0.4082, grad_fn=<NllLossBackward>)\n","Train Loss 177: 0.4082 Acc: 0.8393 94\n","loss  178  -  tensor(0.4075, grad_fn=<NllLossBackward>)\n","Train Loss 178: 0.4075 Acc: 0.8393 94\n","loss  179  -  tensor(0.4067, grad_fn=<NllLossBackward>)\n","Train Loss 179: 0.4067 Acc: 0.8393 94\n","loss  180  -  tensor(0.4059, grad_fn=<NllLossBackward>)\n","Train Loss 180: 0.4059 Acc: 0.8393 94\n","loss  181  -  tensor(0.4051, grad_fn=<NllLossBackward>)\n","Train Loss 181: 0.4051 Acc: 0.8393 94\n","loss  182  -  tensor(0.4043, grad_fn=<NllLossBackward>)\n","Train Loss 182: 0.4043 Acc: 0.8393 94\n","loss  183  -  tensor(0.4036, grad_fn=<NllLossBackward>)\n","Train Loss 183: 0.4036 Acc: 0.8393 94\n","loss  184  -  tensor(0.4028, grad_fn=<NllLossBackward>)\n","Train Loss 184: 0.4028 Acc: 0.8393 94\n","loss  185  -  tensor(0.4021, grad_fn=<NllLossBackward>)\n","Train Loss 185: 0.4021 Acc: 0.8393 94\n","loss  186  -  tensor(0.4013, grad_fn=<NllLossBackward>)\n","Train Loss 186: 0.4013 Acc: 0.8393 94\n","loss  187  -  tensor(0.4006, grad_fn=<NllLossBackward>)\n","Train Loss 187: 0.4006 Acc: 0.8393 94\n","loss  188  -  tensor(0.3998, grad_fn=<NllLossBackward>)\n","Train Loss 188: 0.3998 Acc: 0.8393 94\n","loss  189  -  tensor(0.3991, grad_fn=<NllLossBackward>)\n","Train Loss 189: 0.3991 Acc: 0.8393 94\n","loss  190  -  tensor(0.3984, grad_fn=<NllLossBackward>)\n","Train Loss 190: 0.3984 Acc: 0.8393 94\n","loss  191  -  tensor(0.3977, grad_fn=<NllLossBackward>)\n","Train Loss 191: 0.3977 Acc: 0.8393 94\n","loss  192  -  tensor(0.3969, grad_fn=<NllLossBackward>)\n","Train Loss 192: 0.3969 Acc: 0.8393 94\n","loss  193  -  tensor(0.3962, grad_fn=<NllLossBackward>)\n","Train Loss 193: 0.3962 Acc: 0.8393 94\n","loss  194  -  tensor(0.3955, grad_fn=<NllLossBackward>)\n","Train Loss 194: 0.3955 Acc: 0.8393 94\n","loss  195  -  tensor(0.3948, grad_fn=<NllLossBackward>)\n","Train Loss 195: 0.3948 Acc: 0.8393 94\n","loss  196  -  tensor(0.3941, grad_fn=<NllLossBackward>)\n","Train Loss 196: 0.3941 Acc: 0.8393 94\n","loss  197  -  tensor(0.3934, grad_fn=<NllLossBackward>)\n","Train Loss 197: 0.3934 Acc: 0.8393 94\n","loss  198  -  tensor(0.3927, grad_fn=<NllLossBackward>)\n","Train Loss 198: 0.3927 Acc: 0.8482 95\n","loss  199  -  tensor(0.3920, grad_fn=<NllLossBackward>)\n","Train Loss 199: 0.3920 Acc: 0.8482 95\n","loss  200  -  tensor(0.3913, grad_fn=<NllLossBackward>)\n","Train Loss 200: 0.3913 Acc: 0.8482 95\n","loss  201  -  tensor(0.3907, grad_fn=<NllLossBackward>)\n","Train Loss 201: 0.3907 Acc: 0.8482 95\n","loss  202  -  tensor(0.3900, grad_fn=<NllLossBackward>)\n","Train Loss 202: 0.3900 Acc: 0.8482 95\n","loss  203  -  tensor(0.3893, grad_fn=<NllLossBackward>)\n","Train Loss 203: 0.3893 Acc: 0.8482 95\n","loss  204  -  tensor(0.3887, grad_fn=<NllLossBackward>)\n","Train Loss 204: 0.3887 Acc: 0.8571 96\n","loss  205  -  tensor(0.3880, grad_fn=<NllLossBackward>)\n","Train Loss 205: 0.3880 Acc: 0.8571 96\n","loss  206  -  tensor(0.3873, grad_fn=<NllLossBackward>)\n","Train Loss 206: 0.3873 Acc: 0.8571 96\n","loss  207  -  tensor(0.3867, grad_fn=<NllLossBackward>)\n","Train Loss 207: 0.3867 Acc: 0.8571 96\n","loss  208  -  tensor(0.3860, grad_fn=<NllLossBackward>)\n","Train Loss 208: 0.3860 Acc: 0.8571 96\n","loss  209  -  tensor(0.3854, grad_fn=<NllLossBackward>)\n","Train Loss 209: 0.3854 Acc: 0.8571 96\n","loss  210  -  tensor(0.3847, grad_fn=<NllLossBackward>)\n","Train Loss 210: 0.3847 Acc: 0.8571 96\n","loss  211  -  tensor(0.3841, grad_fn=<NllLossBackward>)\n","Train Loss 211: 0.3841 Acc: 0.8571 96\n","loss  212  -  tensor(0.3835, grad_fn=<NllLossBackward>)\n","Train Loss 212: 0.3835 Acc: 0.8571 96\n","loss  213  -  tensor(0.3828, grad_fn=<NllLossBackward>)\n","Train Loss 213: 0.3828 Acc: 0.8571 96\n","loss  214  -  tensor(0.3822, grad_fn=<NllLossBackward>)\n","Train Loss 214: 0.3822 Acc: 0.8571 96\n","loss  215  -  tensor(0.3816, grad_fn=<NllLossBackward>)\n","Train Loss 215: 0.3816 Acc: 0.8571 96\n","loss  216  -  tensor(0.3810, grad_fn=<NllLossBackward>)\n","Train Loss 216: 0.3810 Acc: 0.8571 96\n","loss  217  -  tensor(0.3803, grad_fn=<NllLossBackward>)\n","Train Loss 217: 0.3803 Acc: 0.8571 96\n","loss  218  -  tensor(0.3797, grad_fn=<NllLossBackward>)\n","Train Loss 218: 0.3797 Acc: 0.8571 96\n","loss  219  -  tensor(0.3791, grad_fn=<NllLossBackward>)\n","Train Loss 219: 0.3791 Acc: 0.8571 96\n","loss  220  -  tensor(0.3785, grad_fn=<NllLossBackward>)\n","Train Loss 220: 0.3785 Acc: 0.8571 96\n","loss  221  -  tensor(0.3779, grad_fn=<NllLossBackward>)\n","Train Loss 221: 0.3779 Acc: 0.8571 96\n","loss  222  -  tensor(0.3773, grad_fn=<NllLossBackward>)\n","Train Loss 222: 0.3773 Acc: 0.8571 96\n","loss  223  -  tensor(0.3767, grad_fn=<NllLossBackward>)\n","Train Loss 223: 0.3767 Acc: 0.8571 96\n","loss  224  -  tensor(0.3761, grad_fn=<NllLossBackward>)\n","Train Loss 224: 0.3761 Acc: 0.8571 96\n","loss  225  -  tensor(0.3755, grad_fn=<NllLossBackward>)\n","Train Loss 225: 0.3755 Acc: 0.8571 96\n","loss  226  -  tensor(0.3750, grad_fn=<NllLossBackward>)\n","Train Loss 226: 0.3750 Acc: 0.8571 96\n","loss  227  -  tensor(0.3744, grad_fn=<NllLossBackward>)\n","Train Loss 227: 0.3744 Acc: 0.8571 96\n","loss  228  -  tensor(0.3738, grad_fn=<NllLossBackward>)\n","Train Loss 228: 0.3738 Acc: 0.8571 96\n","loss  229  -  tensor(0.3732, grad_fn=<NllLossBackward>)\n","Train Loss 229: 0.3732 Acc: 0.8571 96\n","loss  230  -  tensor(0.3726, grad_fn=<NllLossBackward>)\n","Train Loss 230: 0.3726 Acc: 0.8571 96\n","loss  231  -  tensor(0.3721, grad_fn=<NllLossBackward>)\n","Train Loss 231: 0.3721 Acc: 0.8661 97\n","loss  232  -  tensor(0.3715, grad_fn=<NllLossBackward>)\n","Train Loss 232: 0.3715 Acc: 0.8661 97\n","loss  233  -  tensor(0.3709, grad_fn=<NllLossBackward>)\n","Train Loss 233: 0.3709 Acc: 0.8661 97\n","loss  234  -  tensor(0.3704, grad_fn=<NllLossBackward>)\n","Train Loss 234: 0.3704 Acc: 0.8661 97\n","loss  235  -  tensor(0.3698, grad_fn=<NllLossBackward>)\n","Train Loss 235: 0.3698 Acc: 0.8661 97\n","loss  236  -  tensor(0.3693, grad_fn=<NllLossBackward>)\n","Train Loss 236: 0.3693 Acc: 0.8661 97\n","loss  237  -  tensor(0.3687, grad_fn=<NllLossBackward>)\n","Train Loss 237: 0.3687 Acc: 0.8661 97\n","loss  238  -  tensor(0.3682, grad_fn=<NllLossBackward>)\n","Train Loss 238: 0.3682 Acc: 0.8661 97\n","loss  239  -  tensor(0.3676, grad_fn=<NllLossBackward>)\n","Train Loss 239: 0.3676 Acc: 0.8661 97\n","loss  240  -  tensor(0.3671, grad_fn=<NllLossBackward>)\n","Train Loss 240: 0.3671 Acc: 0.8661 97\n","loss  241  -  tensor(0.3665, grad_fn=<NllLossBackward>)\n","Train Loss 241: 0.3665 Acc: 0.8661 97\n","loss  242  -  tensor(0.3660, grad_fn=<NllLossBackward>)\n","Train Loss 242: 0.3660 Acc: 0.8661 97\n","loss  243  -  tensor(0.3655, grad_fn=<NllLossBackward>)\n","Train Loss 243: 0.3655 Acc: 0.8661 97\n","loss  244  -  tensor(0.3649, grad_fn=<NllLossBackward>)\n","Train Loss 244: 0.3649 Acc: 0.8661 97\n","loss  245  -  tensor(0.3644, grad_fn=<NllLossBackward>)\n","Train Loss 245: 0.3644 Acc: 0.8661 97\n","loss  246  -  tensor(0.3639, grad_fn=<NllLossBackward>)\n","Train Loss 246: 0.3639 Acc: 0.8661 97\n","loss  247  -  tensor(0.3634, grad_fn=<NllLossBackward>)\n","Train Loss 247: 0.3634 Acc: 0.8661 97\n","loss  248  -  tensor(0.3628, grad_fn=<NllLossBackward>)\n","Train Loss 248: 0.3628 Acc: 0.8661 97\n","loss  249  -  tensor(0.3623, grad_fn=<NllLossBackward>)\n","Train Loss 249: 0.3623 Acc: 0.8661 97\n","loss  250  -  tensor(0.3618, grad_fn=<NllLossBackward>)\n","Train Loss 250: 0.3618 Acc: 0.8661 97\n","loss  251  -  tensor(0.3613, grad_fn=<NllLossBackward>)\n","Train Loss 251: 0.3613 Acc: 0.8661 97\n","loss  252  -  tensor(0.3608, grad_fn=<NllLossBackward>)\n","Train Loss 252: 0.3608 Acc: 0.8661 97\n","loss  253  -  tensor(0.3603, grad_fn=<NllLossBackward>)\n","Train Loss 253: 0.3603 Acc: 0.8661 97\n","loss  254  -  tensor(0.3598, grad_fn=<NllLossBackward>)\n","Train Loss 254: 0.3598 Acc: 0.8661 97\n","loss  255  -  tensor(0.3593, grad_fn=<NllLossBackward>)\n","Train Loss 255: 0.3593 Acc: 0.8661 97\n","loss  256  -  tensor(0.3588, grad_fn=<NllLossBackward>)\n","Train Loss 256: 0.3588 Acc: 0.8661 97\n","loss  257  -  tensor(0.3583, grad_fn=<NllLossBackward>)\n","Train Loss 257: 0.3583 Acc: 0.8661 97\n","loss  258  -  tensor(0.3578, grad_fn=<NllLossBackward>)\n","Train Loss 258: 0.3578 Acc: 0.8661 97\n","loss  259  -  tensor(0.3573, grad_fn=<NllLossBackward>)\n","Train Loss 259: 0.3573 Acc: 0.8661 97\n","loss  260  -  tensor(0.3568, grad_fn=<NllLossBackward>)\n","Train Loss 260: 0.3568 Acc: 0.8661 97\n","loss  261  -  tensor(0.3563, grad_fn=<NllLossBackward>)\n","Train Loss 261: 0.3563 Acc: 0.8661 97\n","loss  262  -  tensor(0.3558, grad_fn=<NllLossBackward>)\n","Train Loss 262: 0.3558 Acc: 0.8661 97\n","loss  263  -  tensor(0.3553, grad_fn=<NllLossBackward>)\n","Train Loss 263: 0.3553 Acc: 0.8661 97\n","loss  264  -  tensor(0.3549, grad_fn=<NllLossBackward>)\n","Train Loss 264: 0.3549 Acc: 0.8661 97\n","loss  265  -  tensor(0.3544, grad_fn=<NllLossBackward>)\n","Train Loss 265: 0.3544 Acc: 0.8661 97\n","loss  266  -  tensor(0.3539, grad_fn=<NllLossBackward>)\n","Train Loss 266: 0.3539 Acc: 0.8661 97\n","loss  267  -  tensor(0.3534, grad_fn=<NllLossBackward>)\n","Train Loss 267: 0.3534 Acc: 0.8661 97\n","loss  268  -  tensor(0.3530, grad_fn=<NllLossBackward>)\n","Train Loss 268: 0.3530 Acc: 0.8661 97\n","loss  269  -  tensor(0.3525, grad_fn=<NllLossBackward>)\n","Train Loss 269: 0.3525 Acc: 0.8661 97\n","loss  270  -  tensor(0.3520, grad_fn=<NllLossBackward>)\n","Train Loss 270: 0.3520 Acc: 0.8661 97\n","loss  271  -  tensor(0.3515, grad_fn=<NllLossBackward>)\n","Train Loss 271: 0.3515 Acc: 0.8661 97\n","loss  272  -  tensor(0.3511, grad_fn=<NllLossBackward>)\n","Train Loss 272: 0.3511 Acc: 0.8661 97\n","loss  273  -  tensor(0.3506, grad_fn=<NllLossBackward>)\n","Train Loss 273: 0.3506 Acc: 0.8661 97\n","loss  274  -  tensor(0.3502, grad_fn=<NllLossBackward>)\n","Train Loss 274: 0.3502 Acc: 0.8661 97\n","loss  275  -  tensor(0.3497, grad_fn=<NllLossBackward>)\n","Train Loss 275: 0.3497 Acc: 0.8661 97\n","loss  276  -  tensor(0.3493, grad_fn=<NllLossBackward>)\n","Train Loss 276: 0.3493 Acc: 0.8661 97\n","loss  277  -  tensor(0.3488, grad_fn=<NllLossBackward>)\n","Train Loss 277: 0.3488 Acc: 0.8661 97\n","loss  278  -  tensor(0.3483, grad_fn=<NllLossBackward>)\n","Train Loss 278: 0.3483 Acc: 0.8750 98\n","loss  279  -  tensor(0.3479, grad_fn=<NllLossBackward>)\n","Train Loss 279: 0.3479 Acc: 0.8750 98\n","loss  280  -  tensor(0.3475, grad_fn=<NllLossBackward>)\n","Train Loss 280: 0.3475 Acc: 0.8750 98\n","loss  281  -  tensor(0.3470, grad_fn=<NllLossBackward>)\n","Train Loss 281: 0.3470 Acc: 0.8750 98\n","loss  282  -  tensor(0.3466, grad_fn=<NllLossBackward>)\n","Train Loss 282: 0.3466 Acc: 0.8750 98\n","loss  283  -  tensor(0.3461, grad_fn=<NllLossBackward>)\n","Train Loss 283: 0.3461 Acc: 0.8839 99\n","loss  284  -  tensor(0.3457, grad_fn=<NllLossBackward>)\n","Train Loss 284: 0.3457 Acc: 0.8839 99\n","loss  285  -  tensor(0.3452, grad_fn=<NllLossBackward>)\n","Train Loss 285: 0.3452 Acc: 0.8839 99\n","loss  286  -  tensor(0.3448, grad_fn=<NllLossBackward>)\n","Train Loss 286: 0.3448 Acc: 0.8839 99\n","loss  287  -  tensor(0.3444, grad_fn=<NllLossBackward>)\n","Train Loss 287: 0.3444 Acc: 0.8839 99\n","loss  288  -  tensor(0.3440, grad_fn=<NllLossBackward>)\n","Train Loss 288: 0.3440 Acc: 0.8839 99\n","loss  289  -  tensor(0.3435, grad_fn=<NllLossBackward>)\n","Train Loss 289: 0.3435 Acc: 0.8839 99\n","loss  290  -  tensor(0.3431, grad_fn=<NllLossBackward>)\n","Train Loss 290: 0.3431 Acc: 0.8839 99\n","loss  291  -  tensor(0.3427, grad_fn=<NllLossBackward>)\n","Train Loss 291: 0.3427 Acc: 0.8839 99\n","loss  292  -  tensor(0.3423, grad_fn=<NllLossBackward>)\n","Train Loss 292: 0.3423 Acc: 0.8839 99\n","loss  293  -  tensor(0.3418, grad_fn=<NllLossBackward>)\n","Train Loss 293: 0.3418 Acc: 0.8839 99\n","loss  294  -  tensor(0.3414, grad_fn=<NllLossBackward>)\n","Train Loss 294: 0.3414 Acc: 0.8839 99\n","loss  295  -  tensor(0.3410, grad_fn=<NllLossBackward>)\n","Train Loss 295: 0.3410 Acc: 0.8839 99\n","loss  296  -  tensor(0.3406, grad_fn=<NllLossBackward>)\n","Train Loss 296: 0.3406 Acc: 0.8839 99\n","loss  297  -  tensor(0.3402, grad_fn=<NllLossBackward>)\n","Train Loss 297: 0.3402 Acc: 0.8839 99\n","loss  298  -  tensor(0.3398, grad_fn=<NllLossBackward>)\n","Train Loss 298: 0.3398 Acc: 0.8839 99\n","loss  299  -  tensor(0.3393, grad_fn=<NllLossBackward>)\n","Train Loss 299: 0.3393 Acc: 0.8839 99\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XYiK6cjqWPrv","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1629784824760,"user_tz":-540,"elapsed":642,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"6b3b799b-0b1e-4d65-84e6-c32459ae94d6"},"source":["import matplotlib.pyplot as plt\n","\n","fig = plt.figure()\n","ax = fig.add_subplot()\n","ax.plot(list(range(len(acc))),acc )\n","ax.set_xlabel('#epoch')\n","ax.set_ylabel('accuracy')\n","fig.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"Ugf3YAFeu9Rz"},"source":["C++のコードは手作業で修正しています。"]},{"cell_type":"code","metadata":{"id":"ERcQTqId1ymt"},"source":["!g++ -std=c++14 ./src/cse1_opt.cpp ./src/cse1_param.cpp -I ../../ctorch/lib -lblas -o ./bin/cse1_opt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Yvq5MIhdz-X","executionInfo":{"status":"ok","timestamp":1629782250740,"user_tz":-540,"elapsed":2900,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"5403ad03-74a2-42c0-e4fc-73483fd2d73f"},"source":["!./bin/cse1_opt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["### forward computation ...\n","ashape1123\n","get_classes\n","epoch 0 - loss 1.16189 - accuracy 0.151786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 1 - loss 1.13564 - accuracy 0.151786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 2 - loss 1.11045 - accuracy 0.142857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 3 - loss 1.08629 - accuracy 0.151786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 4 - loss 1.06314 - accuracy 0.169643\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 5 - loss 1.04096 - accuracy 0.169643\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 6 - loss 1.01971 - accuracy 0.214286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 7 - loss 0.999362 - accuracy 0.276786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 8 - loss 0.979881 - accuracy 0.375\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 9 - loss 0.961233 - accuracy 0.446429\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 10 - loss 0.943388 - accuracy 0.482143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 11 - loss 0.926308 - accuracy 0.535714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 12 - loss 0.909962 - accuracy 0.571429\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 13 - loss 0.894315 - accuracy 0.625\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 14 - loss 0.879338 - accuracy 0.678571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 15 - loss 0.864998 - accuracy 0.705357\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 16 - loss 0.851266 - accuracy 0.714286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 17 - loss 0.838112 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 18 - loss 0.825509 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 19 - loss 0.81343 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 20 - loss 0.801849 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 21 - loss 0.790739 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 22 - loss 0.780077 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 23 - loss 0.769839 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 24 - loss 0.760005 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 25 - loss 0.750553 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 26 - loss 0.741464 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 27 - loss 0.732719 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 28 - loss 0.7243 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 29 - loss 0.71619 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 30 - loss 0.708374 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 31 - loss 0.700835 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 32 - loss 0.693561 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 33 - loss 0.686537 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 34 - loss 0.679752 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 35 - loss 0.673194 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 36 - loss 0.66685 - accuracy 0.732143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 37 - loss 0.660711 - accuracy 0.732143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 38 - loss 0.654765 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 39 - loss 0.649004 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 40 - loss 0.643418 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 41 - loss 0.638 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 42 - loss 0.632741 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 43 - loss 0.627634 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 44 - loss 0.622672 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 45 - loss 0.617848 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 46 - loss 0.613156 - accuracy 0.75\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 47 - loss 0.60859 - accuracy 0.75\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 48 - loss 0.604144 - accuracy 0.75\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 49 - loss 0.599813 - accuracy 0.758929\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 50 - loss 0.595593 - accuracy 0.758929\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 51 - loss 0.591477 - accuracy 0.758929\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 52 - loss 0.587463 - accuracy 0.758929\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 53 - loss 0.583544 - accuracy 0.758929\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 54 - loss 0.579718 - accuracy 0.767857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 55 - loss 0.57598 - accuracy 0.767857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 56 - loss 0.572327 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 57 - loss 0.568756 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 58 - loss 0.565263 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 59 - loss 0.561845 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 60 - loss 0.5585 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 61 - loss 0.555224 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 62 - loss 0.552016 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 63 - loss 0.548871 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 64 - loss 0.54579 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 65 - loss 0.542768 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 66 - loss 0.539803 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 67 - loss 0.536895 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 68 - loss 0.53404 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 69 - loss 0.531238 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 70 - loss 0.528486 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 71 - loss 0.525783 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 72 - loss 0.523127 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 73 - loss 0.520516 - accuracy 0.794643\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 74 - loss 0.517949 - accuracy 0.794643\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 75 - loss 0.515425 - accuracy 0.794643\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 76 - loss 0.512942 - accuracy 0.794643\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 77 - loss 0.510499 - accuracy 0.794643\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 78 - loss 0.508096 - accuracy 0.794643\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 79 - loss 0.50573 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 80 - loss 0.503401 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 81 - loss 0.501108 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 82 - loss 0.498849 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 83 - loss 0.496624 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 84 - loss 0.494431 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 85 - loss 0.49227 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 86 - loss 0.490141 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 87 - loss 0.488041 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 88 - loss 0.485971 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 89 - loss 0.48393 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 90 - loss 0.481916 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 91 - loss 0.47993 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 92 - loss 0.477971 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 93 - loss 0.476037 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 94 - loss 0.474128 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 95 - loss 0.472244 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 96 - loss 0.470384 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 97 - loss 0.468547 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 98 - loss 0.466734 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 99 - loss 0.464943 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 100 - loss 0.463174 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 101 - loss 0.461426 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 102 - loss 0.4597 - accuracy 0.830357\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 103 - loss 0.457994 - accuracy 0.830357\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 104 - loss 0.456308 - accuracy 0.830357\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 105 - loss 0.454642 - accuracy 0.830357\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 106 - loss 0.452996 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 107 - loss 0.451368 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 108 - loss 0.449759 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 109 - loss 0.448169 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 110 - loss 0.446596 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 111 - loss 0.445041 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 112 - loss 0.443502 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 113 - loss 0.441981 - accuracy 0.830357\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 114 - loss 0.440476 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 115 - loss 0.438988 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 116 - loss 0.437515 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 117 - loss 0.436058 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 118 - loss 0.434615 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 119 - loss 0.433188 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 120 - loss 0.431775 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 121 - loss 0.430377 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 122 - loss 0.428993 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 123 - loss 0.427623 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 124 - loss 0.426266 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 125 - loss 0.424924 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 126 - loss 0.423594 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 127 - loss 0.422278 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 128 - loss 0.420974 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 129 - loss 0.419683 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 130 - loss 0.418405 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 131 - loss 0.417139 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 132 - loss 0.415884 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 133 - loss 0.414643 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 134 - loss 0.413413 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 135 - loss 0.412195 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 136 - loss 0.410988 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 137 - loss 0.409792 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 138 - loss 0.408608 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 139 - loss 0.407433 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 140 - loss 0.40627 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 141 - loss 0.405117 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 142 - loss 0.403975 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 143 - loss 0.402843 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 144 - loss 0.40172 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 145 - loss 0.400608 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 146 - loss 0.399506 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 147 - loss 0.398413 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 148 - loss 0.39733 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 149 - loss 0.396256 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 150 - loss 0.395191 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 151 - loss 0.394136 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 152 - loss 0.39309 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 153 - loss 0.392052 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 154 - loss 0.391024 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 155 - loss 0.390003 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 156 - loss 0.388992 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 157 - loss 0.38799 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 158 - loss 0.386997 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 159 - loss 0.386011 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 160 - loss 0.385034 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 161 - loss 0.384064 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 162 - loss 0.383103 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 163 - loss 0.382149 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 164 - loss 0.381203 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 165 - loss 0.380265 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 166 - loss 0.379334 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 167 - loss 0.37841 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 168 - loss 0.377494 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 169 - loss 0.376585 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 170 - loss 0.375683 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 171 - loss 0.374788 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 172 - loss 0.3739 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 173 - loss 0.373019 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 174 - loss 0.372145 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 175 - loss 0.371277 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 176 - loss 0.370417 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 177 - loss 0.369563 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 178 - loss 0.368715 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 179 - loss 0.367874 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 180 - loss 0.36704 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 181 - loss 0.366211 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 182 - loss 0.365389 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 183 - loss 0.364573 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 184 - loss 0.363763 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 185 - loss 0.362959 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 186 - loss 0.362161 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 187 - loss 0.361368 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 188 - loss 0.360581 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 189 - loss 0.3598 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 190 - loss 0.359025 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 191 - loss 0.358255 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 192 - loss 0.357491 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 193 - loss 0.356732 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 194 - loss 0.355978 - accuracy 0.883929\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 195 - loss 0.35523 - accuracy 0.883929\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 196 - loss 0.354486 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 197 - loss 0.353749 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 198 - loss 0.353016 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 199 - loss 0.352288 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 200 - loss 0.351565 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 201 - loss 0.350847 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 202 - loss 0.350135 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 203 - loss 0.349427 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 204 - loss 0.348724 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 205 - loss 0.348026 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 206 - loss 0.347333 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 207 - loss 0.346644 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 208 - loss 0.34596 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 209 - loss 0.34528 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 210 - loss 0.344605 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 211 - loss 0.343935 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 212 - loss 0.343269 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 213 - loss 0.342607 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 214 - loss 0.341949 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 215 - loss 0.341296 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 216 - loss 0.340647 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 217 - loss 0.340002 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 218 - loss 0.339362 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 219 - loss 0.338725 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 220 - loss 0.338093 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 221 - loss 0.337464 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 222 - loss 0.33684 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 223 - loss 0.336219 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 224 - loss 0.335603 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 225 - loss 0.33499 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 226 - loss 0.334381 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 227 - loss 0.333776 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 228 - loss 0.333175 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 229 - loss 0.332577 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 230 - loss 0.331983 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 231 - loss 0.331393 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 232 - loss 0.330806 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 233 - loss 0.330223 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 234 - loss 0.329644 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 235 - loss 0.329068 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 236 - loss 0.328495 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 237 - loss 0.327926 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 238 - loss 0.32736 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 239 - loss 0.326798 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 240 - loss 0.326239 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 241 - loss 0.325683 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 242 - loss 0.325131 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 243 - loss 0.324581 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 244 - loss 0.324035 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 245 - loss 0.323493 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 246 - loss 0.322953 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 247 - loss 0.322415 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 248 - loss 0.321881 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 249 - loss 0.321349 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 250 - loss 0.320821 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 251 - loss 0.320296 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 252 - loss 0.319774 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 253 - loss 0.319255 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 254 - loss 0.318739 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 255 - loss 0.318226 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 256 - loss 0.317716 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 257 - loss 0.317209 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 258 - loss 0.316704 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 259 - loss 0.316202 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 260 - loss 0.315704 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 261 - loss 0.315207 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 262 - loss 0.314714 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 263 - loss 0.314223 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 264 - loss 0.313734 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 265 - loss 0.313249 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 266 - loss 0.312766 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 267 - loss 0.312285 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 268 - loss 0.311807 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 269 - loss 0.311332 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 270 - loss 0.310859 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 271 - loss 0.310388 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 272 - loss 0.309921 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 273 - loss 0.309455 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 274 - loss 0.308992 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 275 - loss 0.308531 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 276 - loss 0.308073 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 277 - loss 0.307616 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 278 - loss 0.307163 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 279 - loss 0.306711 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 280 - loss 0.306262 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 281 - loss 0.305815 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 282 - loss 0.305371 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 283 - loss 0.304928 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 284 - loss 0.304488 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 285 - loss 0.30405 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 286 - loss 0.303614 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 287 - loss 0.30318 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 288 - loss 0.302749 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 289 - loss 0.30232 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 290 - loss 0.301893 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 291 - loss 0.301468 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 292 - loss 0.301045 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 293 - loss 0.300624 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 294 - loss 0.300206 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 295 - loss 0.299789 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 296 - loss 0.299374 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 297 - loss 0.298961 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 298 - loss 0.298551 - accuracy 0.910714\n","### backward computation ...\n","### forward computation ...\n","ashape1123\n","get_classes\n","epoch 299 - loss 0.298142 - accuracy 0.910714\n","### backward computation ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ac-_IZA4d5dG","executionInfo":{"status":"ok","timestamp":1629782271148,"user_tz":-540,"elapsed":222,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"753c2226-3369-4b45-b47b-e96b0a1f64a3"},"source":["f = open('./cse1.out', 'r')\n","\n","loss = []\n","acc=[]\n","\n","datalist = f.readlines()\n","for data in datalist:\n","  #print(data)\n","  ds = data.split(',')\n","  loss.append( float(ds[0]) )\n","  acc.append( float(ds[1]) )\n","\n","f.close()\n","\n","print(\"epoch =\", len(loss))\n","print( loss )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch = 300\n","[1.161893, 1.13564, 1.110451, 1.086295, 1.06314, 1.040958, 1.01971, 0.999362, 0.979881, 0.961233, 0.943388, 0.926308, 0.909962, 0.894315, 0.879338, 0.864998, 0.851266, 0.838112, 0.825509, 0.81343, 0.801849, 0.790739, 0.780077, 0.769839, 0.760005, 0.750553, 0.741464, 0.732719, 0.7243, 0.71619, 0.708374, 0.700835, 0.693561, 0.686537, 0.679752, 0.673194, 0.66685, 0.660711, 0.654765, 0.649004, 0.643418, 0.638, 0.632741, 0.627634, 0.622672, 0.617848, 0.613156, 0.60859, 0.604144, 0.599813, 0.595593, 0.591477, 0.587463, 0.583544, 0.579718, 0.57598, 0.572327, 0.568756, 0.565263, 0.561845, 0.5585, 0.555224, 0.552016, 0.548871, 0.54579, 0.542768, 0.539803, 0.536895, 0.53404, 0.531238, 0.528486, 0.525783, 0.523127, 0.520516, 0.517949, 0.515425, 0.512942, 0.510499, 0.508096, 0.50573, 0.503401, 0.501108, 0.498849, 0.496624, 0.494431, 0.49227, 0.490141, 0.488041, 0.485971, 0.48393, 0.481916, 0.47993, 0.477971, 0.476037, 0.474128, 0.472244, 0.470384, 0.468547, 0.466734, 0.464943, 0.463174, 0.461426, 0.4597, 0.457994, 0.456308, 0.454642, 0.452996, 0.451368, 0.449759, 0.448169, 0.446596, 0.445041, 0.443502, 0.441981, 0.440476, 0.438988, 0.437515, 0.436058, 0.434615, 0.433188, 0.431775, 0.430377, 0.428993, 0.427623, 0.426266, 0.424924, 0.423594, 0.422278, 0.420974, 0.419683, 0.418405, 0.417139, 0.415884, 0.414643, 0.413413, 0.412195, 0.410988, 0.409792, 0.408608, 0.407433, 0.40627, 0.405117, 0.403975, 0.402843, 0.40172, 0.400608, 0.399506, 0.398413, 0.39733, 0.396256, 0.395191, 0.394136, 0.39309, 0.392052, 0.391024, 0.390003, 0.388992, 0.38799, 0.386997, 0.386011, 0.385034, 0.384064, 0.383103, 0.382149, 0.381203, 0.380265, 0.379334, 0.37841, 0.377494, 0.376585, 0.375683, 0.374788, 0.3739, 0.373019, 0.372145, 0.371277, 0.370417, 0.369563, 0.368715, 0.367874, 0.36704, 0.366211, 0.365389, 0.364573, 0.363763, 0.362959, 0.362161, 0.361368, 0.360581, 0.3598, 0.359025, 0.358255, 0.357491, 0.356732, 0.355978, 0.35523, 0.354486, 0.353749, 0.353016, 0.352288, 0.351565, 0.350847, 0.350135, 0.349427, 0.348724, 0.348026, 0.347333, 0.346644, 0.34596, 0.34528, 0.344605, 0.343935, 0.343269, 0.342607, 0.341949, 0.341296, 0.340647, 0.340002, 0.339362, 0.338725, 0.338093, 0.337464, 0.33684, 0.336219, 0.335603, 0.33499, 0.334381, 0.333776, 0.333175, 0.332577, 0.331983, 0.331393, 0.330806, 0.330223, 0.329644, 0.329068, 0.328495, 0.327926, 0.32736, 0.326798, 0.326239, 0.325683, 0.325131, 0.324581, 0.324035, 0.323493, 0.322953, 0.322415, 0.321881, 0.321349, 0.320821, 0.320296, 0.319774, 0.319255, 0.318739, 0.318226, 0.317716, 0.317209, 0.316704, 0.316202, 0.315704, 0.315207, 0.314714, 0.314223, 0.313734, 0.313249, 0.312766, 0.312285, 0.311807, 0.311332, 0.310859, 0.310388, 0.309921, 0.309455, 0.308992, 0.308531, 0.308073, 0.307616, 0.307163, 0.306711, 0.306262, 0.305815, 0.305371, 0.304928, 0.304488, 0.30405, 0.303614, 0.30318, 0.302749, 0.30232, 0.301893, 0.301468, 0.301045, 0.300624, 0.300206, 0.299789, 0.299374, 0.298961, 0.298551, 0.298142]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"xMQUBvFqfMWg","executionInfo":{"status":"ok","timestamp":1629782273317,"user_tz":-540,"elapsed":504,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"89924637-edb5-41dc-b999-85eb6db3af01"},"source":["# 交差エントロピー誤差\n","import matplotlib.pyplot as plt\n","\n","fig = plt.figure()\n","ax = fig.add_subplot()\n","ax.plot(list(range(len(loss))), loss)\n","ax.set_xlabel('epoch')\n","ax.set_ylabel('loss')\n","fig.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"iLif0DtvuYG5","executionInfo":{"status":"ok","timestamp":1629782276811,"user_tz":-540,"elapsed":401,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"271c4074-4402-48ea-f19d-8203f5e6255f"},"source":["# 正解率\n","fig = plt.figure()\n","ax = fig.add_subplot()\n","ax.plot(list(range(len(acc))), acc)\n","ax.set_xlabel('epoch')\n","ax.set_ylabel('accuracy')\n","fig.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"OSkuEd7zfizf"},"source":[""],"execution_count":null,"outputs":[]}]}
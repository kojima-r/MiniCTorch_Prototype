{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20211004_minictorch_cross_entropy_loss.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP9y7IcIfqZp1BVcy8tO5od"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Sml8FMQqbgEj"},"source":["cross entropy loss　分類問題"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ULQtL8IeXTg2","executionInfo":{"status":"ok","timestamp":1633340559605,"user_tz":-540,"elapsed":16680,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"5419b552-a376-4d08-e329-0c43a3e16682"},"source":["#　colaboraory用: Google drive をマウントする\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NujIexoFXUmL","executionInfo":{"status":"ok","timestamp":1633340561954,"user_tz":-540,"elapsed":237,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"232c7e00-fab1-4176-b735-09cba5fb7aeb"},"source":["# colaboratory用: フォルダを移る\n","%cd \"drive/My Drive/Colab Notebooks/\""],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks\n"]}]},{"cell_type":"markdown","metadata":{"id":"0DNl9nIDfY0l"},"source":["フォルダは自分の指定のものに変更してね。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gR9cSFPNfVA4","executionInfo":{"status":"ok","timestamp":1633340571823,"user_tz":-540,"elapsed":652,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"9b8a1f91-1f22-4ab1-99c9-0ea48d6d4482"},"source":["%cd \"ctorch210929/MiniCTorch_Prototype/jpynb\""],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks/ctorch210929/MiniCTorch_Prototype/jpynb\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPPGcVEQ7fwE","executionInfo":{"status":"ok","timestamp":1633340577949,"user_tz":-540,"elapsed":4293,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"31ea0b8a-16ba-4035-9ed3-1350bd3c75c5"},"source":["! pip install lark-parser"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lark-parser\n","  Downloading lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n","\u001b[?25l\r\u001b[K     |███▏                            | 10 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 20 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 30 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 40 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 71 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 92 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 102 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 103 kB 5.9 MB/s \n","\u001b[?25hInstalling collected packages: lark-parser\n","Successfully installed lark-parser-0.12.0\n"]}]},{"cell_type":"code","metadata":{"id":"vuIJaurj7brd","executionInfo":{"status":"ok","timestamp":1633340608193,"user_tz":-540,"elapsed":5142,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}}},"source":["import sys\n","sys.path.append(\"../minictorch\")\n","\n","import json\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import generator as GN\n","import converter as CV"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"UWUPb5h0Blqx","executionInfo":{"status":"ok","timestamp":1633340612351,"user_tz":-540,"elapsed":1239,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}}},"source":["from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing   import StandardScaler\n","\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fuAfJop8BpYI"},"source":["データ読み込み"]},{"cell_type":"code","metadata":{"id":"5FYRis-rBr_Y","executionInfo":{"status":"ok","timestamp":1633340616289,"user_tz":-540,"elapsed":286,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}}},"source":["# データ読み込み\n","iris = datasets.load_iris()\n","data   = iris['data']\n","target = iris['target']\n","\n","# 学習データと検証データに分割\n","x_train, x_valid, y_train, y_valid = train_test_split( data, target, shuffle=True )\n","\n","# 特徴量の標準化\n","scaler = StandardScaler()\n","scaler.fit( x_train )\n","\n","x_train = scaler.transform(x_train)\n","x_valid = scaler.transform(x_valid)\n","\n","# Tensor型に変換\n","# 学習に入れるときはfloat型 or long型になっている必要があるのここで変換してしまう\n","x_train = torch.from_numpy(x_train).float()\n","y_train = torch.from_numpy(y_train).long()\n","x_valid = torch.from_numpy(x_valid).float()\n","y_valid = torch.from_numpy(y_valid).long()\n","\n","#print('x_train : ', x_train.shape)\n","#print('y_train : ', y_train.shape)\n","#print('x_valid : ', x_valid.shape)\n","#print('y_valid : ', y_valid.shape)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cDivgOEAB-wC"},"source":["DataSetとDataLoaderの生成"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-AznF7ZCEEJ","executionInfo":{"status":"ok","timestamp":1633340620529,"user_tz":-540,"elapsed":620,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"247e1cd0-e6ff-410a-e87d-06a886ce2bbd"},"source":["train_dataset = TensorDataset(x_train, y_train)\n","valid_dataset = TensorDataset(x_valid, y_valid)\n","\n","# indexを指定すればデータを取り出すことができます。\n","index = 0\n","print( train_dataset.__getitem__(index)[0].size() )\n","print( train_dataset.__getitem__(index)[1] )\n","\n","\n","batch_size = 112\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","\n","# 動作確認\n","# こんな感じでバッチ単位で取り出す子ができます。\n","# イテレータに変換\n","batch_iterator = iter(train_dataloader)\n","\n","# 1番目の要素を取り出す\n","inputs, labels = next(batch_iterator)\n","print(inputs.size())\n","print(labels.size())\n","print(inputs)\n","print(labels)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4])\n","tensor(2)\n","torch.Size([112, 4])\n","torch.Size([112])\n","tensor([[-1.1056e+00, -1.4971e+00, -2.2779e-01, -2.1464e-01],\n","        [ 7.0825e-01,  2.9863e-01,  9.2402e-01,  1.5610e+00],\n","        [-8.6372e-01, -1.2727e+00, -4.0056e-01, -7.8050e-02],\n","        [ 9.5010e-01, -3.7479e-01,  5.2089e-01,  1.9512e-01],\n","        [-1.3820e-01,  2.9923e+00, -1.2644e+00, -1.0342e+00],\n","        [-9.8464e-01,  2.9863e-01, -1.4372e+00, -1.3073e+00],\n","        [ 5.8733e-01, -1.2727e+00,  7.5125e-01,  1.0146e+00],\n","        [ 2.2457e-01, -1.9461e+00,  7.5125e-01,  4.6830e-01],\n","        [ 1.4338e+00,  2.9863e-01,  5.7848e-01,  3.3171e-01],\n","        [-3.8004e-01, -1.2727e+00,  1.7534e-01,  1.9512e-01],\n","        [-1.1056e+00,  1.1965e+00, -1.3220e+00, -1.4439e+00],\n","        [ 8.2917e-01, -1.5032e-01,  8.6643e-01,  1.1512e+00],\n","        [ 1.0710e+00,  5.2310e-01,  1.1544e+00,  1.8342e+00],\n","        [ 1.1919e+00, -5.9926e-01,  6.3607e-01,  3.3171e-01],\n","        [ 3.4549e-01, -1.0482e+00,  1.0968e+00,  3.3171e-01],\n","        [ 1.0365e-01,  2.9863e-01,  6.3607e-01,  8.7806e-01],\n","        [ 1.6756e+00, -1.5032e-01,  1.2120e+00,  6.0489e-01],\n","        [-1.4683e+00,  2.9863e-01, -1.3220e+00, -1.3073e+00],\n","        [-3.8004e-01, -1.7216e+00,  1.7534e-01,  1.9512e-01],\n","        [ 1.1919e+00, -1.5032e-01,  1.0392e+00,  1.2878e+00],\n","        [-8.6372e-01,  7.4757e-01, -1.2644e+00, -1.3073e+00],\n","        [-9.8464e-01,  7.4757e-01, -1.2068e+00, -1.0342e+00],\n","        [-1.4683e+00,  1.1965e+00, -1.5524e+00, -1.3073e+00],\n","        [-1.1056e+00, -1.5032e-01, -1.3220e+00, -1.3073e+00],\n","        [ 2.2457e-01, -3.7479e-01,  4.6330e-01,  4.6830e-01],\n","        [ 2.2802e+00,  1.6455e+00,  1.7303e+00,  1.4244e+00],\n","        [-1.4683e+00,  7.4757e-01, -1.3220e+00, -1.1707e+00],\n","        [-2.5912e-01, -3.7479e-01, -5.5020e-02,  1.9512e-01],\n","        [ 4.6641e-01, -5.9926e-01,  6.3607e-01,  8.7806e-01],\n","        [-7.4280e-01,  9.7204e-01, -1.2644e+00, -1.3073e+00],\n","        [ 3.4549e-01, -5.9926e-01,  5.7848e-01,  5.8537e-02],\n","        [-7.4280e-01,  2.3189e+00, -1.2644e+00, -1.4439e+00],\n","        [ 1.3129e+00,  7.4156e-02,  9.8161e-01,  1.2878e+00],\n","        [-2.5912e-01, -5.9926e-01,  6.9366e-01,  1.1512e+00],\n","        [-1.5893e+00, -1.7216e+00, -1.3796e+00, -1.1707e+00],\n","        [ 3.4549e-01, -1.5032e-01,  5.2089e-01,  3.3171e-01],\n","        [-1.3820e-01, -1.5032e-01,  2.9052e-01,  5.8537e-02],\n","        [ 1.0365e-01, -1.5032e-01,  2.9052e-01,  4.6830e-01],\n","        [ 8.2917e-01,  2.9863e-01,  8.0884e-01,  1.1512e+00],\n","        [-3.8004e-01, -1.4971e+00,  6.0162e-02, -7.8050e-02],\n","        [-8.6372e-01,  1.6455e+00, -1.2644e+00, -1.1707e+00],\n","        [-1.2265e+00,  7.4156e-02, -1.2068e+00, -1.3073e+00],\n","        [ 7.0825e-01, -3.7479e-01,  3.4811e-01,  1.9512e-01],\n","        [ 5.8733e-01, -1.2727e+00,  6.9366e-01,  4.6830e-01],\n","        [-3.8004e-01,  9.7204e-01, -1.3796e+00, -1.3073e+00],\n","        [-1.7274e-02, -1.0482e+00,  1.7534e-01,  5.8537e-02],\n","        [-3.8004e-01, -1.4971e+00,  2.5710e-03, -2.1464e-01],\n","        [-8.6372e-01,  1.6455e+00, -1.0341e+00, -1.0342e+00],\n","        [ 8.2917e-01, -1.5032e-01,  1.2120e+00,  1.4244e+00],\n","        [-1.3474e+00,  2.9863e-01, -1.2068e+00, -1.3073e+00],\n","        [ 8.2917e-01, -5.9926e-01,  5.2089e-01,  4.6830e-01],\n","        [-9.8464e-01, -1.7216e+00, -2.2779e-01, -2.1464e-01],\n","        [-5.0096e-01,  1.8699e+00, -1.1492e+00, -1.0342e+00],\n","        [-6.2188e-01,  1.4210e+00, -1.2644e+00, -1.3073e+00],\n","        [-1.1056e+00, -1.2727e+00,  4.6330e-01,  7.4147e-01],\n","        [-1.7274e-02,  2.0944e+00, -1.4372e+00, -1.3073e+00],\n","        [-5.0096e-01,  1.4210e+00, -1.2644e+00, -1.3073e+00],\n","        [ 8.2917e-01, -1.5032e-01,  1.0392e+00,  8.7806e-01],\n","        [-1.3820e-01, -5.9926e-01,  2.3293e-01,  1.9512e-01],\n","        [ 2.2457e-01, -1.9461e+00,  1.7534e-01, -2.1464e-01],\n","        [ 1.6756e+00,  2.9863e-01,  1.3272e+00,  8.7806e-01],\n","        [ 3.4549e-01, -3.7479e-01,  5.7848e-01,  3.3171e-01],\n","        [ 1.9175e+00, -5.9926e-01,  1.3847e+00,  1.0146e+00],\n","        [ 1.0710e+00,  5.2310e-01,  1.1544e+00,  1.2878e+00],\n","        [-1.7102e+00,  2.9863e-01, -1.3796e+00, -1.3073e+00],\n","        [-1.4683e+00,  7.4156e-02, -1.2644e+00, -1.3073e+00],\n","        [ 2.1593e+00, -1.5032e-01,  1.6727e+00,  1.2878e+00],\n","        [ 3.4549e-01, -1.5032e-01,  6.9366e-01,  8.7806e-01],\n","        [-1.7102e+00, -3.7479e-01, -1.3220e+00, -1.3073e+00],\n","        [ 2.2802e+00, -5.9926e-01,  1.7303e+00,  1.1512e+00],\n","        [-5.0096e-01,  7.4757e-01, -1.2644e+00, -1.0342e+00],\n","        [ 2.2457e-01, -1.5032e-01,  6.3607e-01,  8.7806e-01],\n","        [ 1.0710e+00,  7.4156e-02,  1.0968e+00,  1.6976e+00],\n","        [ 2.2457e-01,  7.4757e-01,  4.6330e-01,  6.0489e-01],\n","        [-1.2265e+00,  7.4757e-01, -1.0341e+00, -1.3073e+00],\n","        [-7.4280e-01, -8.2373e-01,  1.1775e-01,  3.3171e-01],\n","        [ 1.0365e-01, -1.5032e-01,  8.0884e-01,  8.7806e-01],\n","        [-1.7274e-02, -8.2373e-01,  1.1775e-01,  5.8537e-02],\n","        [-1.2265e+00, -1.5032e-01, -1.3220e+00, -1.1707e+00],\n","        [-9.8464e-01,  1.1965e+00, -1.3220e+00, -1.3073e+00],\n","        [ 1.0710e+00,  7.4156e-02,  5.7848e-01,  4.6830e-01],\n","        [-9.8464e-01, -2.3950e+00, -1.1261e-01, -2.1464e-01],\n","        [-1.3820e-01, -3.7479e-01,  2.9052e-01,  1.9512e-01],\n","        [-2.5912e-01, -8.2373e-01,  2.9052e-01,  1.9512e-01],\n","        [ 7.0825e-01, -8.2373e-01,  9.2402e-01,  1.0146e+00],\n","        [ 1.0710e+00,  7.4156e-02,  4.0570e-01,  3.3171e-01],\n","        [ 1.3129e+00,  2.9863e-01,  1.1544e+00,  1.5610e+00],\n","        [ 1.3129e+00,  7.4156e-02,  8.0884e-01,  1.5610e+00],\n","        [ 2.5221e+00,  1.6455e+00,  1.5575e+00,  1.1512e+00],\n","        [ 7.0825e-01,  7.4156e-02,  1.0392e+00,  8.7806e-01],\n","        [-1.1056e+00,  7.4156e-02, -1.2644e+00, -1.3073e+00],\n","        [-1.1056e+00,  7.4156e-02, -1.2644e+00, -1.4439e+00],\n","        [-2.5912e-01, -1.5032e-01,  4.6330e-01,  4.6830e-01],\n","        [-1.3474e+00,  2.9863e-01, -1.3796e+00, -1.3073e+00],\n","        [ 2.2457e-01, -8.2373e-01,  8.0884e-01,  6.0489e-01],\n","        [ 5.8733e-01, -1.7216e+00,  4.0570e-01,  1.9512e-01],\n","        [ 5.8733e-01,  5.2310e-01,  1.3272e+00,  1.8342e+00],\n","        [-9.8464e-01,  9.7204e-01, -1.3796e+00, -1.1707e+00],\n","        [-5.0096e-01, -1.5032e-01,  4.6330e-01,  4.6830e-01],\n","        [ 1.3129e+00,  7.4156e-02,  6.9366e-01,  4.6830e-01],\n","        [-8.6372e-01,  1.6455e+00, -1.2068e+00, -1.3073e+00],\n","        [-7.4280e-01,  7.4757e-01, -1.3220e+00, -1.3073e+00],\n","        [ 5.8733e-01,  5.2310e-01,  5.7848e-01,  6.0489e-01],\n","        [ 3.4549e-01, -5.9926e-01,  1.7534e-01,  1.9512e-01],\n","        [ 5.8733e-01, -5.9926e-01,  8.0884e-01,  4.6830e-01],\n","        [ 7.0825e-01, -5.9926e-01,  1.0968e+00,  1.4244e+00],\n","        [-1.2265e+00,  7.4757e-01, -1.2068e+00, -1.3073e+00],\n","        [-3.8004e-01,  2.5433e+00, -1.3220e+00, -1.3073e+00],\n","        [-9.8464e-01,  9.7204e-01, -1.2068e+00, -7.6098e-01],\n","        [-2.5912e-01, -1.5032e-01,  2.3293e-01,  1.9512e-01],\n","        [-9.8464e-01, -1.5032e-01, -1.2068e+00, -1.3073e+00],\n","        [ 1.7965e+00, -3.7479e-01,  1.4999e+00,  8.7806e-01]])\n","tensor([1, 2, 1, 1, 0, 0, 2, 2, 1, 1, 0, 2, 2, 1, 2, 1, 2, 0, 1, 2, 0, 0, 0, 0,\n","        1, 2, 0, 1, 2, 0, 1, 0, 2, 2, 0, 1, 1, 1, 2, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n","        2, 0, 1, 1, 0, 0, 2, 0, 0, 2, 1, 1, 2, 1, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2,\n","        2, 1, 0, 1, 2, 1, 0, 0, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 0, 0, 1, 0, 1, 1,\n","        2, 0, 1, 1, 0, 0, 1, 1, 2, 2, 0, 0, 0, 1, 0, 2])\n"]}]},{"cell_type":"markdown","metadata":{"id":"8Io-p4ogJysT"},"source":["ニューラルネットワークの定義"]},{"cell_type":"code","metadata":{"id":"FMLT7mbxauCN","executionInfo":{"status":"ok","timestamp":1633340637355,"user_tz":-540,"elapsed":238,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}}},"source":["class Net(nn.Module):    \n","    def __init__(self,t):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(4, 64)\n","        self.fc2 = nn.Linear(64, 3)\n","        self.target = t\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        #x = F.log_softmax(x, dim=1)\n","        #return x\n","        \n","        #print(x);\n","        #print(self.target);\n","        self.out = x\n","\n","        loss = nn.CrossEntropyLoss()\n","        #loss = nn.NLLLoss()\n","        output = loss(x,self.target)\n","        return output\n","        \n","\n","class Net2(nn.Module):    \n","    def __init__(self):\n","        super(Net2, self).__init__()\n","        self.fc1 = nn.Linear(4, 64)\n","        self.fc2 = nn.Linear(64, 3)\n","        #self.fc1 = nn.Linear(4, 128)\n","        #self.fc2 = nn.Linear(128, 3)\n","        #self.fc3 = nn.Linear(64, 3)\n","        #self.fc4 = nn.Linear(128, 3)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        #x = F.relu(self.fc2(x))\n","        #x = F.relu(self.fc3(x))\n","        x = self.fc2(x)\n","        #x = F.softmax(x, dim=1)\n","        return x"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"_yu6J3uzDVsp","executionInfo":{"status":"ok","timestamp":1633340640711,"user_tz":-540,"elapsed":226,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}}},"source":["def generate_json( json_path, input, target ):\n","\n","    model = Net( target )\n","    model.eval()\n","    with torch.no_grad():\n","        print(\"[SAVE]\", json_path )\n","        GN.generate_minictorch_file( model, input, json_path )\n","\n","    return model"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gavYhJ2Z6tft","executionInfo":{"status":"ok","timestamp":1633340650241,"user_tz":-540,"elapsed":872,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"aafee5f2-3986-4f1e-9d35-bc6e47d84280"},"source":["torch.manual_seed( 1 )\n","\n","print(\"inputs\",inputs)\n","print(\"target\",labels)\n","inputs.requires_grad = True\n","\n","project = 'cse1'\n","json_path = '../network/' + project +'.json'\n","\n","model = generate_json( json_path, inputs, labels )\n","\n","with torch.set_grad_enabled(True):\n","\n","  output = model( inputs )\n","  print(\"output\",output)\n","\n","  model.zero_grad()\n","  #torch.zeros_like(inputs.grad)\n","\n","  output.backward()\n","  print(\"output grad\",output.grad)\n","  #print(\"fc1 grad\",model.fc1.weight.grad)\n","  #print(\"fc2 grad\",model.fc2.weight.grad)\n","  print(\"input grad\",inputs.grad)\n","\n","  # ラベルを予測\n","  #print(\"output\", model.out, inputs.size(0))\n","  _, preds = torch.max( model.out, 1 )\n","\n","  # イテレーション結果の計算\n","  epoch_loss = output * inputs.size(0)\n","\n","  # 正解数の合計を更新\n","  epoch_corrects = torch.sum( preds == labels.data )\n","\n","  epoch_loss = epoch_loss / float(inputs.size(0))\n","  epoch_acc  = epoch_corrects.double() / float(inputs.size(0))\n","\n","  epoch=1\n","  print('Train Loss {}: {:.4f} Acc: {:.4f}'.format( epoch, epoch_loss, epoch_acc ))"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs tensor([[-1.1056e+00, -1.4971e+00, -2.2779e-01, -2.1464e-01],\n","        [ 7.0825e-01,  2.9863e-01,  9.2402e-01,  1.5610e+00],\n","        [-8.6372e-01, -1.2727e+00, -4.0056e-01, -7.8050e-02],\n","        [ 9.5010e-01, -3.7479e-01,  5.2089e-01,  1.9512e-01],\n","        [-1.3820e-01,  2.9923e+00, -1.2644e+00, -1.0342e+00],\n","        [-9.8464e-01,  2.9863e-01, -1.4372e+00, -1.3073e+00],\n","        [ 5.8733e-01, -1.2727e+00,  7.5125e-01,  1.0146e+00],\n","        [ 2.2457e-01, -1.9461e+00,  7.5125e-01,  4.6830e-01],\n","        [ 1.4338e+00,  2.9863e-01,  5.7848e-01,  3.3171e-01],\n","        [-3.8004e-01, -1.2727e+00,  1.7534e-01,  1.9512e-01],\n","        [-1.1056e+00,  1.1965e+00, -1.3220e+00, -1.4439e+00],\n","        [ 8.2917e-01, -1.5032e-01,  8.6643e-01,  1.1512e+00],\n","        [ 1.0710e+00,  5.2310e-01,  1.1544e+00,  1.8342e+00],\n","        [ 1.1919e+00, -5.9926e-01,  6.3607e-01,  3.3171e-01],\n","        [ 3.4549e-01, -1.0482e+00,  1.0968e+00,  3.3171e-01],\n","        [ 1.0365e-01,  2.9863e-01,  6.3607e-01,  8.7806e-01],\n","        [ 1.6756e+00, -1.5032e-01,  1.2120e+00,  6.0489e-01],\n","        [-1.4683e+00,  2.9863e-01, -1.3220e+00, -1.3073e+00],\n","        [-3.8004e-01, -1.7216e+00,  1.7534e-01,  1.9512e-01],\n","        [ 1.1919e+00, -1.5032e-01,  1.0392e+00,  1.2878e+00],\n","        [-8.6372e-01,  7.4757e-01, -1.2644e+00, -1.3073e+00],\n","        [-9.8464e-01,  7.4757e-01, -1.2068e+00, -1.0342e+00],\n","        [-1.4683e+00,  1.1965e+00, -1.5524e+00, -1.3073e+00],\n","        [-1.1056e+00, -1.5032e-01, -1.3220e+00, -1.3073e+00],\n","        [ 2.2457e-01, -3.7479e-01,  4.6330e-01,  4.6830e-01],\n","        [ 2.2802e+00,  1.6455e+00,  1.7303e+00,  1.4244e+00],\n","        [-1.4683e+00,  7.4757e-01, -1.3220e+00, -1.1707e+00],\n","        [-2.5912e-01, -3.7479e-01, -5.5020e-02,  1.9512e-01],\n","        [ 4.6641e-01, -5.9926e-01,  6.3607e-01,  8.7806e-01],\n","        [-7.4280e-01,  9.7204e-01, -1.2644e+00, -1.3073e+00],\n","        [ 3.4549e-01, -5.9926e-01,  5.7848e-01,  5.8537e-02],\n","        [-7.4280e-01,  2.3189e+00, -1.2644e+00, -1.4439e+00],\n","        [ 1.3129e+00,  7.4156e-02,  9.8161e-01,  1.2878e+00],\n","        [-2.5912e-01, -5.9926e-01,  6.9366e-01,  1.1512e+00],\n","        [-1.5893e+00, -1.7216e+00, -1.3796e+00, -1.1707e+00],\n","        [ 3.4549e-01, -1.5032e-01,  5.2089e-01,  3.3171e-01],\n","        [-1.3820e-01, -1.5032e-01,  2.9052e-01,  5.8537e-02],\n","        [ 1.0365e-01, -1.5032e-01,  2.9052e-01,  4.6830e-01],\n","        [ 8.2917e-01,  2.9863e-01,  8.0884e-01,  1.1512e+00],\n","        [-3.8004e-01, -1.4971e+00,  6.0162e-02, -7.8050e-02],\n","        [-8.6372e-01,  1.6455e+00, -1.2644e+00, -1.1707e+00],\n","        [-1.2265e+00,  7.4156e-02, -1.2068e+00, -1.3073e+00],\n","        [ 7.0825e-01, -3.7479e-01,  3.4811e-01,  1.9512e-01],\n","        [ 5.8733e-01, -1.2727e+00,  6.9366e-01,  4.6830e-01],\n","        [-3.8004e-01,  9.7204e-01, -1.3796e+00, -1.3073e+00],\n","        [-1.7274e-02, -1.0482e+00,  1.7534e-01,  5.8537e-02],\n","        [-3.8004e-01, -1.4971e+00,  2.5710e-03, -2.1464e-01],\n","        [-8.6372e-01,  1.6455e+00, -1.0341e+00, -1.0342e+00],\n","        [ 8.2917e-01, -1.5032e-01,  1.2120e+00,  1.4244e+00],\n","        [-1.3474e+00,  2.9863e-01, -1.2068e+00, -1.3073e+00],\n","        [ 8.2917e-01, -5.9926e-01,  5.2089e-01,  4.6830e-01],\n","        [-9.8464e-01, -1.7216e+00, -2.2779e-01, -2.1464e-01],\n","        [-5.0096e-01,  1.8699e+00, -1.1492e+00, -1.0342e+00],\n","        [-6.2188e-01,  1.4210e+00, -1.2644e+00, -1.3073e+00],\n","        [-1.1056e+00, -1.2727e+00,  4.6330e-01,  7.4147e-01],\n","        [-1.7274e-02,  2.0944e+00, -1.4372e+00, -1.3073e+00],\n","        [-5.0096e-01,  1.4210e+00, -1.2644e+00, -1.3073e+00],\n","        [ 8.2917e-01, -1.5032e-01,  1.0392e+00,  8.7806e-01],\n","        [-1.3820e-01, -5.9926e-01,  2.3293e-01,  1.9512e-01],\n","        [ 2.2457e-01, -1.9461e+00,  1.7534e-01, -2.1464e-01],\n","        [ 1.6756e+00,  2.9863e-01,  1.3272e+00,  8.7806e-01],\n","        [ 3.4549e-01, -3.7479e-01,  5.7848e-01,  3.3171e-01],\n","        [ 1.9175e+00, -5.9926e-01,  1.3847e+00,  1.0146e+00],\n","        [ 1.0710e+00,  5.2310e-01,  1.1544e+00,  1.2878e+00],\n","        [-1.7102e+00,  2.9863e-01, -1.3796e+00, -1.3073e+00],\n","        [-1.4683e+00,  7.4156e-02, -1.2644e+00, -1.3073e+00],\n","        [ 2.1593e+00, -1.5032e-01,  1.6727e+00,  1.2878e+00],\n","        [ 3.4549e-01, -1.5032e-01,  6.9366e-01,  8.7806e-01],\n","        [-1.7102e+00, -3.7479e-01, -1.3220e+00, -1.3073e+00],\n","        [ 2.2802e+00, -5.9926e-01,  1.7303e+00,  1.1512e+00],\n","        [-5.0096e-01,  7.4757e-01, -1.2644e+00, -1.0342e+00],\n","        [ 2.2457e-01, -1.5032e-01,  6.3607e-01,  8.7806e-01],\n","        [ 1.0710e+00,  7.4156e-02,  1.0968e+00,  1.6976e+00],\n","        [ 2.2457e-01,  7.4757e-01,  4.6330e-01,  6.0489e-01],\n","        [-1.2265e+00,  7.4757e-01, -1.0341e+00, -1.3073e+00],\n","        [-7.4280e-01, -8.2373e-01,  1.1775e-01,  3.3171e-01],\n","        [ 1.0365e-01, -1.5032e-01,  8.0884e-01,  8.7806e-01],\n","        [-1.7274e-02, -8.2373e-01,  1.1775e-01,  5.8537e-02],\n","        [-1.2265e+00, -1.5032e-01, -1.3220e+00, -1.1707e+00],\n","        [-9.8464e-01,  1.1965e+00, -1.3220e+00, -1.3073e+00],\n","        [ 1.0710e+00,  7.4156e-02,  5.7848e-01,  4.6830e-01],\n","        [-9.8464e-01, -2.3950e+00, -1.1261e-01, -2.1464e-01],\n","        [-1.3820e-01, -3.7479e-01,  2.9052e-01,  1.9512e-01],\n","        [-2.5912e-01, -8.2373e-01,  2.9052e-01,  1.9512e-01],\n","        [ 7.0825e-01, -8.2373e-01,  9.2402e-01,  1.0146e+00],\n","        [ 1.0710e+00,  7.4156e-02,  4.0570e-01,  3.3171e-01],\n","        [ 1.3129e+00,  2.9863e-01,  1.1544e+00,  1.5610e+00],\n","        [ 1.3129e+00,  7.4156e-02,  8.0884e-01,  1.5610e+00],\n","        [ 2.5221e+00,  1.6455e+00,  1.5575e+00,  1.1512e+00],\n","        [ 7.0825e-01,  7.4156e-02,  1.0392e+00,  8.7806e-01],\n","        [-1.1056e+00,  7.4156e-02, -1.2644e+00, -1.3073e+00],\n","        [-1.1056e+00,  7.4156e-02, -1.2644e+00, -1.4439e+00],\n","        [-2.5912e-01, -1.5032e-01,  4.6330e-01,  4.6830e-01],\n","        [-1.3474e+00,  2.9863e-01, -1.3796e+00, -1.3073e+00],\n","        [ 2.2457e-01, -8.2373e-01,  8.0884e-01,  6.0489e-01],\n","        [ 5.8733e-01, -1.7216e+00,  4.0570e-01,  1.9512e-01],\n","        [ 5.8733e-01,  5.2310e-01,  1.3272e+00,  1.8342e+00],\n","        [-9.8464e-01,  9.7204e-01, -1.3796e+00, -1.1707e+00],\n","        [-5.0096e-01, -1.5032e-01,  4.6330e-01,  4.6830e-01],\n","        [ 1.3129e+00,  7.4156e-02,  6.9366e-01,  4.6830e-01],\n","        [-8.6372e-01,  1.6455e+00, -1.2068e+00, -1.3073e+00],\n","        [-7.4280e-01,  7.4757e-01, -1.3220e+00, -1.3073e+00],\n","        [ 5.8733e-01,  5.2310e-01,  5.7848e-01,  6.0489e-01],\n","        [ 3.4549e-01, -5.9926e-01,  1.7534e-01,  1.9512e-01],\n","        [ 5.8733e-01, -5.9926e-01,  8.0884e-01,  4.6830e-01],\n","        [ 7.0825e-01, -5.9926e-01,  1.0968e+00,  1.4244e+00],\n","        [-1.2265e+00,  7.4757e-01, -1.2068e+00, -1.3073e+00],\n","        [-3.8004e-01,  2.5433e+00, -1.3220e+00, -1.3073e+00],\n","        [-9.8464e-01,  9.7204e-01, -1.2068e+00, -7.6098e-01],\n","        [-2.5912e-01, -1.5032e-01,  2.3293e-01,  1.9512e-01],\n","        [-9.8464e-01, -1.5032e-01, -1.2068e+00, -1.3073e+00],\n","        [ 1.7965e+00, -3.7479e-01,  1.4999e+00,  8.7806e-01]])\n","target tensor([1, 2, 1, 1, 0, 0, 2, 2, 1, 1, 0, 2, 2, 1, 2, 1, 2, 0, 1, 2, 0, 0, 0, 0,\n","        1, 2, 0, 1, 2, 0, 1, 0, 2, 2, 0, 1, 1, 1, 2, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n","        2, 0, 1, 1, 0, 0, 2, 0, 0, 2, 1, 1, 2, 1, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2,\n","        2, 1, 0, 1, 2, 1, 0, 0, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 0, 0, 1, 0, 1, 1,\n","        2, 0, 1, 1, 0, 0, 1, 1, 2, 2, 0, 0, 0, 1, 0, 2])\n","[SAVE] ../network/cse1.json\n","skip: Net/Linear[fc1]/weight/26\n","skip: Net/Linear[fc1]/weight/26\n","skip: Net/Linear[fc2]/weight/29\n","skip: Net/Linear[fc2]/weight/29\n","output tensor(1.1532, grad_fn=<NllLossBackward>)\n","output grad None\n","input grad tensor([[ 7.9293e-04, -5.7419e-04,  1.3710e-03,  5.2718e-04],\n","        [ 2.8373e-04,  9.6314e-04, -3.7548e-04,  8.6162e-04],\n","        [ 9.8446e-04, -3.7612e-04,  1.2001e-03,  4.6012e-04],\n","        [-6.0979e-04, -1.3087e-04, -2.7115e-04, -1.2591e-03],\n","        [-1.7070e-03,  7.7304e-04,  8.2798e-04,  4.4904e-04],\n","        [-1.3251e-03, -6.4532e-04,  2.9761e-04, -3.5744e-04],\n","        [ 1.1628e-03,  6.9115e-04, -9.9639e-04,  7.3654e-04],\n","        [ 5.5293e-04,  1.8178e-04, -1.1493e-03,  1.6210e-04],\n","        [-5.2240e-04, -1.3683e-04, -6.5972e-04, -1.2304e-03],\n","        [ 5.7108e-04,  2.3983e-04,  5.5580e-04, -1.3695e-04],\n","        [-1.4136e-03, -5.8214e-04,  3.2669e-04, -3.3281e-04],\n","        [ 4.4966e-04,  1.2475e-03, -9.3096e-04,  9.8314e-04],\n","        [ 2.0987e-04,  1.0209e-03,  3.1236e-05,  7.0046e-04],\n","        [-8.2530e-04, -1.3737e-04, -5.9396e-04, -5.3168e-04],\n","        [ 1.3733e-03,  7.2465e-04, -1.0570e-03,  9.1069e-04],\n","        [ 2.3552e-05, -6.6890e-04, -3.0891e-04, -1.5896e-03],\n","        [ 1.3580e-03,  1.0152e-03, -1.6669e-04,  2.5715e-05],\n","        [-1.4634e-03, -6.1074e-04,  4.2950e-04, -3.2281e-04],\n","        [ 5.1678e-04,  1.8555e-04,  6.6684e-04,  1.3771e-04],\n","        [ 1.1070e-03,  1.0538e-03, -5.5930e-04,  5.1026e-04],\n","        [-1.5051e-03, -6.4450e-04,  1.1776e-04, -1.7312e-04],\n","        [-1.3465e-03, -6.0431e-04,  3.4091e-04, -3.0760e-04],\n","        [-1.6296e-03, -5.9019e-04,  3.9432e-04, -3.6212e-04],\n","        [-1.2351e-03, -4.9722e-04,  4.0144e-04, -5.0584e-04],\n","        [-1.7605e-04, -3.7005e-04,  1.6372e-04, -1.4192e-03],\n","        [ 1.1048e-03,  8.7198e-04,  4.7271e-04,  2.8681e-04],\n","        [-1.3589e-03, -4.6765e-04,  5.0376e-04, -4.8326e-04],\n","        [ 1.0977e-03, -5.6457e-04,  1.3203e-03, -3.1919e-04],\n","        [ 5.9112e-04,  1.1165e-03, -1.0086e-03,  8.3477e-04],\n","        [-1.4761e-03, -6.3903e-04,  1.1892e-04, -1.6795e-04],\n","        [-9.3753e-04,  8.2019e-06, -1.3433e-04, -1.6008e-03],\n","        [-1.5183e-03,  3.7874e-04,  3.6228e-04,  3.5366e-04],\n","        [ 8.9692e-04,  7.0213e-04, -3.3091e-04,  4.2153e-04],\n","        [ 8.4655e-04,  1.0989e-03, -1.0664e-03,  6.7248e-04],\n","        [-2.0784e-03, -1.0133e-03,  1.6481e-03, -5.6533e-04],\n","        [-7.7878e-05, -6.1536e-04, -9.2530e-06, -1.6541e-03],\n","        [ 8.5398e-04, -1.2843e-03,  6.2367e-04, -8.2171e-04],\n","        [ 1.5499e-05, -5.9309e-04, -1.1311e-04, -1.5329e-03],\n","        [ 6.3688e-04,  7.3200e-04, -6.2673e-04,  6.7532e-04],\n","        [ 3.7533e-04,  3.5641e-04,  9.8403e-04,  2.1612e-06],\n","        [-1.5148e-03,  2.5545e-04,  2.6661e-04,  4.2509e-04],\n","        [-1.2299e-03, -5.0726e-04,  3.8366e-04, -4.9403e-04],\n","        [-7.2077e-04,  3.1643e-05,  1.2567e-04, -1.0734e-03],\n","        [-6.5608e-04,  1.1647e-04, -1.7209e-04, -7.9327e-04],\n","        [-1.4755e-03, -1.8947e-04,  3.5395e-05, -1.5091e-04],\n","        [ 3.6788e-04,  4.2403e-04,  1.0049e-03, -9.8345e-05],\n","        [-2.6984e-04,  8.1180e-04,  1.1116e-03,  4.4405e-04],\n","        [-1.5178e-03,  2.5291e-04,  2.6902e-04,  4.2654e-04],\n","        [ 4.5186e-04,  1.4833e-03, -3.6509e-04,  7.9275e-04],\n","        [-1.4472e-03, -6.2621e-04,  4.4131e-04, -3.1468e-04],\n","        [-3.8755e-04, -3.9066e-04, -8.0767e-05, -1.1964e-03],\n","        [ 5.5946e-04, -1.4965e-05,  9.6922e-04,  4.0676e-04],\n","        [-1.4249e-03,  2.9900e-04,  3.6376e-04,  3.4610e-04],\n","        [-1.0698e-03, -2.3508e-04, -1.6769e-04,  6.0494e-06],\n","        [ 7.3208e-04,  5.9543e-04, -1.5800e-03,  4.1416e-04],\n","        [-1.4437e-03, -4.8970e-05,  1.4897e-04, -2.0214e-04],\n","        [-1.5602e-03, -1.3306e-04,  5.2837e-05, -1.2095e-04],\n","        [ 1.1817e-03,  9.5610e-04, -5.8804e-04,  6.1185e-04],\n","        [ 5.3945e-04, -1.6781e-04,  1.2097e-03, -8.3408e-04],\n","        [ 6.4146e-05,  8.4551e-04,  9.8355e-04,  6.9147e-04],\n","        [ 1.4596e-03,  9.4009e-04, -2.6987e-04, -1.9470e-05],\n","        [-2.7671e-04, -4.9590e-04,  3.3776e-04, -1.6183e-03],\n","        [ 1.4065e-03,  8.0834e-04, -7.9712e-04,  5.8990e-04],\n","        [ 4.8227e-04,  9.0164e-04, -1.0040e-04,  7.3007e-04],\n","        [-1.3567e-03, -4.2727e-04,  3.7857e-04, -4.6984e-04],\n","        [-1.3073e-03, -4.5877e-04,  4.1091e-04, -4.6175e-04],\n","        [ 9.1782e-04,  4.8738e-04, -3.5500e-05,  3.8840e-04],\n","        [ 8.4062e-04,  9.7214e-04, -1.0441e-03,  8.6938e-04],\n","        [-1.3984e-03, -8.2097e-04,  7.7634e-04, -6.3199e-04],\n","        [ 1.4079e-03,  8.2208e-04, -7.5471e-04,  6.4062e-04],\n","        [-1.8694e-03, -5.1315e-04,  3.2578e-04, -2.9157e-04],\n","        [ 8.3809e-04,  9.6966e-04, -1.0407e-03,  8.6829e-04],\n","        [ 2.3156e-04,  1.1621e-03, -1.1865e-04,  7.4326e-04],\n","        [ 8.0733e-04, -1.0471e-03, -7.7208e-04, -7.5401e-04],\n","        [-1.6205e-03, -5.0836e-04, -1.0066e-04, -4.1242e-05],\n","        [ 1.0594e-03, -2.9059e-04,  1.2743e-03, -6.2669e-05],\n","        [ 8.1592e-04,  9.5265e-04, -1.0094e-03,  8.7472e-04],\n","        [ 5.4112e-04, -1.0341e-04,  1.2077e-03, -7.1421e-04],\n","        [-1.2409e-03, -4.9682e-04,  3.9642e-04, -5.0736e-04],\n","        [-1.3765e-03, -5.8563e-04,  3.2932e-04, -3.2052e-04],\n","        [-7.4002e-04, -2.2480e-04, -4.0953e-04, -7.6093e-04],\n","        [-9.0846e-05,  3.2142e-04,  1.1779e-03,  3.8247e-04],\n","        [ 4.8414e-04, -8.0521e-04,  9.0888e-04, -9.0923e-04],\n","        [ 4.7262e-04,  9.2117e-05,  1.1335e-03, -5.1629e-04],\n","        [ 1.3054e-03,  6.8118e-04, -9.5473e-04,  6.8905e-04],\n","        [-5.0532e-04, -1.1948e-04, -6.1175e-04, -1.0420e-03],\n","        [ 3.2052e-04,  1.1023e-03, -3.5767e-05,  6.4916e-04],\n","        [ 3.7767e-04,  5.6775e-04, -7.0863e-04,  8.5164e-04],\n","        [ 7.0599e-04,  5.6965e-04,  8.3682e-04,  6.7565e-04],\n","        [ 7.0229e-04,  1.1912e-03, -7.6121e-04,  9.0636e-04],\n","        [-1.3762e-03, -6.8770e-04,  4.2038e-04, -3.3981e-04],\n","        [-1.3817e-03, -6.8122e-04,  4.1269e-04, -3.4526e-04],\n","        [ 5.1171e-04, -1.1608e-03,  8.4566e-05, -1.0071e-03],\n","        [-1.3412e-03, -4.9048e-04,  4.1911e-04, -2.9274e-04],\n","        [-3.4372e-04, -2.7280e-04, -8.6354e-05, -1.3975e-03],\n","        [-2.5210e-04,  9.5679e-04,  3.2099e-05,  1.3159e-04],\n","        [ 3.6564e-04,  1.1418e-03, -7.0308e-05,  7.8435e-04],\n","        [-1.3633e-03, -5.9277e-04,  3.3377e-04, -3.1505e-04],\n","        [ 7.7434e-04, -1.2471e-03,  8.9851e-04, -5.9362e-04],\n","        [-5.0640e-04, -1.2359e-04, -6.3928e-04, -1.0325e-03],\n","        [-1.4596e-03,  3.1758e-04,  3.6785e-04,  3.5168e-04],\n","        [-1.4384e-03, -6.7763e-04,  8.0960e-05, -2.0535e-04],\n","        [ 2.6076e-05, -3.4777e-04, -5.3066e-04, -1.3564e-03],\n","        [-4.5252e-04, -3.0183e-04,  2.3405e-04, -1.0255e-03],\n","        [ 1.1098e-03,  7.7241e-04, -6.2644e-04,  8.7937e-04],\n","        [ 7.0504e-04,  9.9138e-04, -7.4279e-04,  7.6843e-04],\n","        [-1.4556e-03, -4.6052e-04,  1.2626e-04, -1.8166e-04],\n","        [-1.4420e-03,  3.6849e-04,  3.5805e-04,  3.3750e-04],\n","        [-1.5166e-03, -3.3653e-04,  5.1324e-04,  4.5360e-05],\n","        [ 8.5289e-04, -1.2727e-03,  6.3542e-04, -8.0451e-04],\n","        [-1.3452e-03, -3.8119e-04,  2.9729e-04, -4.3452e-04],\n","        [ 1.1765e-03,  1.0959e-03, -4.8547e-04,  5.7103e-04]])\n","Train Loss 1: 1.1532 Acc: 0.1786\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n"]}]},{"cell_type":"code","metadata":{"id":"MN3gKf3D8iut"},"source":["\"\"\"\n","def convert_json( project, folder, model, input_x, json_path, rand_flag=0 ):\n","\n","    #folder = \"src\"\n","    cpp_fname   = project + \".cpp\"\n","    param_fname = project + \"_param.cpp\"\n","    cpp_path    = folder + \"/\" + cpp_fname\n","    param_path  = folder + \"/\" + param_fname\n","    make_path   = folder + \"/\" + \"Makefile\"\n","\n","    # load json file\n","    print( \"[JSON]\", json_path )\n","    fp = open( json_path )\n","    obj = json.load( fp )\n","\n","    # save parameter file\n","    code1 = CV.c_param_generator( obj, model, input_x )\n","    if len( code1 ) > 0:\n","       print( \"[PARAM]\", param_path )\n","       ofparam = open( param_path, \"w\" )\n","       ofparam.write( code1 )\n","\n","    # save cpp file\n","    print( \"[CPP]  \", cpp_path )\n","    code2 = CV.c_code_generator( obj, model, rand_flag )\n","\n","    #ofp=open(args.path+\"/\"+args.output,\"w\")\n","    ofp = open( cpp_path, \"w\" )\n","    ofp.write( code2 )\n","\n","    # save make file\n","    print( \"[MAKE] \", make_path )\n","    make_code = CV.makefile_generator( cpp_fname )\n","\n","    #makefp=open(args.path+\"/\"+\"Makefile\",\"w\")\n","    makefp = open( make_path, \"w\" )\n","    makefp.write( make_code )\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5EtFEsQ0sLW1","executionInfo":{"status":"ok","timestamp":1633340681827,"user_tz":-540,"elapsed":864,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"3907d42f-c44a-4b3b-dd6d-84b507b5312e"},"source":["CV.convert_json( project, \"../src\", model, inputs, json_path )"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[JSON] ../network/cse1.json\n","{'name': 'Net/Linear[fc1]/weight/35', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [3], 'sorted_id': 1}\n","{'name': 'Net/Linear[fc1]/bias/34', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [3], 'sorted_id': 2}\n","{'name': 'Net/Linear[fc2]/weight/38', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [7], 'sorted_id': 5}\n","{'name': 'Net/Linear[fc2]/bias/37', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [7], 'sorted_id': 6}\n","[PARAM] ../src/cse1_param.cpp\n","{'name': 'input/x', 'op': 'IO Node', 'in': [], 'output_id': 0, 'shape': [112, 4], 'out': [3], 'sorted_id': 0}\n","{'name': 'Net/Linear[fc1]/weight/35', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [3], 'sorted_id': 1}\n","Net/Linear[fc1]/weight/35  ->  fc1_weight\n","{'name': 'Net/Linear[fc1]/bias/34', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [3], 'sorted_id': 2}\n","Net/Linear[fc1]/bias/34  ->  fc1_bias\n","{'name': 'Net/Linear[fc1]/input.1', 'op': 'aten::linear', 'in': [0, 1, 2], 'output_id': 0, 'shape': [112, 64], 'out': [4], 'sorted_id': 3}\n","{'name': 'Net/input.3', 'op': 'aten::relu', 'in': [3], 'output_id': 0, 'shape': [112, 64], 'out': [7], 'sorted_id': 4}\n","{'name': 'Net/Linear[fc2]/weight/38', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [7], 'sorted_id': 5}\n","Net/Linear[fc2]/weight/38  ->  fc2_weight\n","{'name': 'Net/Linear[fc2]/bias/37', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [7], 'sorted_id': 6}\n","Net/Linear[fc2]/bias/37  ->  fc2_bias\n","{'name': 'Net/Linear[fc2]/input', 'op': 'aten::linear', 'in': [4, 5, 6], 'output_id': 0, 'shape': [112, 3], 'out': [12], 'sorted_id': 7}\n","{'name': 'Net/17', 'op': 'prim::Constant', 'in': [], 'output_id': 0, 'shape': [112], 'constant_value': [1.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 0.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 2.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 2.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0], 'out': [12], 'sorted_id': 8}\n","{'name': 'Net/18', 'op': 'prim::Constant', 'in': [], 'output_id': 0, 'shape': [], 'out': [12], 'sorted_id': 9}\n","{'name': 'Net/19', 'op': 'prim::Constant', 'in': [], 'output_id': 0, 'shape': [], 'constant_value': 1.0, 'out': [12], 'sorted_id': 10}\n","{'name': 'Net/20', 'op': 'prim::Constant', 'in': [], 'output_id': 0, 'shape': [], 'constant_value': -100.0, 'out': [12], 'sorted_id': 11}\n","{'name': 'Net/21', 'op': 'aten::cross_entropy_loss', 'in': [7, 8, 9, 10, 11], 'output_id': 0, 'shape': [], 'out': [13], 'sorted_id': 12}\n","{'name': 'output/output.1', 'op': 'IO Node', 'in': [12], 'output_id': 0, 'shape': [], 'out': [], 'sorted_id': 13}\n","[CPP]  ../src/cse1.cpp\n","[MAKE] ../src/Makefile\n"]}]},{"cell_type":"code","metadata":{"id":"zf1OiQzc9u5t","executionInfo":{"status":"ok","timestamp":1633340923914,"user_tz":-540,"elapsed":85331,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}}},"source":["!g++ -std=c++14 ../src/cse1.cpp ../src/cse1_param.cpp -I ../../../ctorch/lib -lcblas -o ../bin/cse1"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4bIvl823mZVt"},"source":["(注意) ctorch/libにxtensor関連のincludeを置いています。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I31lNv_hh4s2","executionInfo":{"status":"ok","timestamp":1633340970402,"user_tz":-540,"elapsed":626,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"84cb72df-1d3c-4628-82b2-5719cbb74511"},"source":["!../bin/cse1"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["### forward computation ...\n"," 1.153184\n","### backward computation ...\n","input_grad{{ 0.000793, -0.000574,  0.001371,  0.000527},\n"," { 0.000284,  0.000963, -0.000375,  0.000862},\n"," { 0.000984, -0.000376,  0.0012  ,  0.00046 },\n"," {-0.00061 , -0.000131, -0.000271, -0.001259},\n"," {-0.001707,  0.000773,  0.000828,  0.000449},\n"," {-0.001325, -0.000645,  0.000298, -0.000357},\n"," { 0.001163,  0.000691, -0.000996,  0.000737},\n"," { 0.000553,  0.000182, -0.001149,  0.000162},\n"," {-0.000522, -0.000137, -0.00066 , -0.00123 },\n"," { 0.000571,  0.00024 ,  0.000556, -0.000137},\n"," {-0.001414, -0.000582,  0.000327, -0.000333},\n"," { 0.00045 ,  0.001247, -0.000931,  0.000983},\n"," { 0.00021 ,  0.001021,  0.000031,  0.0007  },\n"," {-0.000825, -0.000137, -0.000594, -0.000532},\n"," { 0.001373,  0.000725, -0.001057,  0.000911},\n"," { 0.000024, -0.000669, -0.000309, -0.00159 },\n"," { 0.001358,  0.001015, -0.000167,  0.000026},\n"," {-0.001463, -0.000611,  0.000429, -0.000323},\n"," { 0.000517,  0.000186,  0.000667,  0.000138},\n"," { 0.001107,  0.001054, -0.000559,  0.00051 },\n"," {-0.001505, -0.000644,  0.000118, -0.000173},\n"," {-0.001346, -0.000604,  0.000341, -0.000308},\n"," {-0.00163 , -0.00059 ,  0.000394, -0.000362},\n"," {-0.001235, -0.000497,  0.000401, -0.000506},\n"," {-0.000176, -0.00037 ,  0.000164, -0.001419},\n"," { 0.001105,  0.000872,  0.000473,  0.000287},\n"," {-0.001359, -0.000468,  0.000504, -0.000483},\n"," { 0.001098, -0.000565,  0.00132 , -0.000319},\n"," { 0.000591,  0.001116, -0.001009,  0.000835},\n"," {-0.001476, -0.000639,  0.000119, -0.000168},\n"," {-0.000938,  0.000008, -0.000134, -0.001601},\n"," {-0.001518,  0.000379,  0.000362,  0.000354},\n"," { 0.000897,  0.000702, -0.000331,  0.000422},\n"," { 0.000847,  0.001099, -0.001066,  0.000672},\n"," {-0.002078, -0.001013,  0.001648, -0.000565},\n"," {-0.000078, -0.000615, -0.000009, -0.001654},\n"," { 0.000854, -0.001284,  0.000624, -0.000822},\n"," { 0.000015, -0.000593, -0.000113, -0.001533},\n"," { 0.000637,  0.000732, -0.000627,  0.000675},\n"," { 0.000375,  0.000356,  0.000984,  0.000002},\n"," {-0.001515,  0.000255,  0.000267,  0.000425},\n"," {-0.00123 , -0.000507,  0.000384, -0.000494},\n"," {-0.000721,  0.000032,  0.000126, -0.001073},\n"," {-0.000656,  0.000116, -0.000172, -0.000793},\n"," {-0.001475, -0.000189,  0.000035, -0.000151},\n"," { 0.000368,  0.000424,  0.001005, -0.000098},\n"," {-0.00027 ,  0.000812,  0.001112,  0.000444},\n"," {-0.001518,  0.000253,  0.000269,  0.000427},\n"," { 0.000452,  0.001483, -0.000365,  0.000793},\n"," {-0.001447, -0.000626,  0.000441, -0.000315},\n"," {-0.000388, -0.000391, -0.000081, -0.001196},\n"," { 0.000559, -0.000015,  0.000969,  0.000407},\n"," {-0.001425,  0.000299,  0.000364,  0.000346},\n"," {-0.00107 , -0.000235, -0.000168,  0.000006},\n"," { 0.000732,  0.000595, -0.00158 ,  0.000414},\n"," {-0.001444, -0.000049,  0.000149, -0.000202},\n"," {-0.00156 , -0.000133,  0.000053, -0.000121},\n"," { 0.001182,  0.000956, -0.000588,  0.000612},\n"," { 0.000539, -0.000168,  0.00121 , -0.000834},\n"," { 0.000064,  0.000846,  0.000984,  0.000691},\n"," { 0.00146 ,  0.00094 , -0.00027 , -0.000019},\n"," {-0.000277, -0.000496,  0.000338, -0.001618},\n"," { 0.001407,  0.000808, -0.000797,  0.00059 },\n"," { 0.000482,  0.000902, -0.0001  ,  0.00073 },\n"," {-0.001357, -0.000427,  0.000379, -0.00047 },\n"," {-0.001307, -0.000459,  0.000411, -0.000462},\n"," { 0.000918,  0.000487, -0.000036,  0.000388},\n"," { 0.000841,  0.000972, -0.001044,  0.000869},\n"," {-0.001398, -0.000821,  0.000776, -0.000632},\n"," { 0.001408,  0.000822, -0.000755,  0.000641},\n"," {-0.001869, -0.000513,  0.000326, -0.000292},\n"," { 0.000838,  0.00097 , -0.001041,  0.000868},\n"," { 0.000232,  0.001162, -0.000119,  0.000743},\n"," { 0.000807, -0.001047, -0.000772, -0.000754},\n"," {-0.00162 , -0.000508, -0.000101, -0.000041},\n"," { 0.001059, -0.000291,  0.001274, -0.000063},\n"," { 0.000816,  0.000953, -0.001009,  0.000875},\n"," { 0.000541, -0.000103,  0.001208, -0.000714},\n"," {-0.001241, -0.000497,  0.000396, -0.000507},\n"," {-0.001376, -0.000586,  0.000329, -0.000321},\n"," {-0.00074 , -0.000225, -0.00041 , -0.000761},\n"," {-0.000091,  0.000321,  0.001178,  0.000382},\n"," { 0.000484, -0.000805,  0.000909, -0.000909},\n"," { 0.000473,  0.000092,  0.001133, -0.000516},\n"," { 0.001305,  0.000681, -0.000955,  0.000689},\n"," {-0.000505, -0.000119, -0.000612, -0.001042},\n"," { 0.000321,  0.001102, -0.000036,  0.000649},\n"," { 0.000378,  0.000568, -0.000709,  0.000852},\n"," { 0.000706,  0.00057 ,  0.000837,  0.000676},\n"," { 0.000702,  0.001191, -0.000761,  0.000906},\n"," {-0.001376, -0.000688,  0.00042 , -0.00034 },\n"," {-0.001382, -0.000681,  0.000413, -0.000345},\n"," { 0.000512, -0.001161,  0.000085, -0.001007},\n"," {-0.001341, -0.00049 ,  0.000419, -0.000293},\n"," {-0.000344, -0.000273, -0.000086, -0.001397},\n"," {-0.000252,  0.000957,  0.000032,  0.000132},\n"," { 0.000366,  0.001142, -0.00007 ,  0.000784},\n"," {-0.001363, -0.000593,  0.000334, -0.000315},\n"," { 0.000774, -0.001247,  0.000899, -0.000594},\n"," {-0.000506, -0.000124, -0.000639, -0.001033},\n"," {-0.00146 ,  0.000318,  0.000368,  0.000352},\n"," {-0.001438, -0.000678,  0.000081, -0.000205},\n"," { 0.000026, -0.000348, -0.000531, -0.001356},\n"," {-0.000453, -0.000302,  0.000234, -0.001025},\n"," { 0.00111 ,  0.000772, -0.000626,  0.000879},\n"," { 0.000705,  0.000991, -0.000743,  0.000768},\n"," {-0.001456, -0.000461,  0.000126, -0.000182},\n"," {-0.001442,  0.000368,  0.000358,  0.000338},\n"," {-0.001517, -0.000337,  0.000513,  0.000045},\n"," { 0.000853, -0.001273,  0.000635, -0.000805},\n"," {-0.001345, -0.000381,  0.000297, -0.000435},\n"," { 0.001176,  0.001096, -0.000485,  0.000571}}\n"]}]},{"cell_type":"code","metadata":{"id":"-AaUeeTX6UmM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633340995744,"user_tz":-540,"elapsed":1421,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"d7bf1689-3299-4555-ce03-0c71edcd6ca0"},"source":["torch.manual_seed( 1 )\n","\n","#print(\"target\",target)\n","inputs.requires_grad = True\n","\n","#model = Net( labels )\n","model = Net2()\n","\n","num = inputs.size(0)\n","\n","project = 'test5'\n","#json_path = 'network/' + project +'.json'\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD( model.parameters(), lr=0.01 )\n","\n","num_epochs = 300\n","\n","acc = []\n","\n","for epoch in range(num_epochs):\n","  with torch.set_grad_enabled(True):\n","\n","    model.train()   # モデルを訓練モードに設定\n","\n","    outputs = model( inputs )\n","    #print(outputs)\n","    #print(labels)\n","\n","    #print(\"input grad\",inputs.grad)\n","\n","    loss = criterion( outputs, labels )\n","    print(\"loss \",epoch, \" - \",loss)\n","\n","    # ラベルを予測\n","    #print(\"output\", outputs, num )\n","    _, preds = torch.max( outputs, 1 )\n","    #print(labels)\n","    #print(preds)\n","\n","    optimizer.zero_grad()\n","\n","    # 逆伝搬の計算\n","    loss.backward()\n","                    \n","    # パラメータの更新\n","    optimizer.step()\n","\n","    # イテレーション結果の計算\n","    epoch_loss = loss.item() * float(num)\n","\n","    # 正解数の合計を更新\n","    epoch_corrects = torch.sum( preds == labels )\n","\n","    epoch_loss = epoch_loss / float(num)\n","    epoch_acc  = epoch_corrects.double() / float(num)\n","    print('Train Loss {}: {:.4f} Acc: {:.4f} {}'.format( epoch, epoch_loss, epoch_acc, epoch_corrects ))\n","\n","    acc.append( epoch_acc )"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["loss  0  -  tensor(1.1532, grad_fn=<NllLossBackward>)\n","Train Loss 0: 1.1532 Acc: 0.1786 20\n","loss  1  -  tensor(1.1300, grad_fn=<NllLossBackward>)\n","Train Loss 1: 1.1300 Acc: 0.1786 20\n","loss  2  -  tensor(1.1077, grad_fn=<NllLossBackward>)\n","Train Loss 2: 1.1077 Acc: 0.1607 18\n","loss  3  -  tensor(1.0862, grad_fn=<NllLossBackward>)\n","Train Loss 3: 1.0862 Acc: 0.1607 18\n","loss  4  -  tensor(1.0656, grad_fn=<NllLossBackward>)\n","Train Loss 4: 1.0656 Acc: 0.1607 18\n","loss  5  -  tensor(1.0458, grad_fn=<NllLossBackward>)\n","Train Loss 5: 1.0458 Acc: 0.1786 20\n","loss  6  -  tensor(1.0268, grad_fn=<NllLossBackward>)\n","Train Loss 6: 1.0268 Acc: 0.1964 22\n","loss  7  -  tensor(1.0086, grad_fn=<NllLossBackward>)\n","Train Loss 7: 1.0086 Acc: 0.2143 24\n","loss  8  -  tensor(0.9910, grad_fn=<NllLossBackward>)\n","Train Loss 8: 0.9910 Acc: 0.2768 31\n","loss  9  -  tensor(0.9742, grad_fn=<NllLossBackward>)\n","Train Loss 9: 0.9742 Acc: 0.3214 36\n","loss  10  -  tensor(0.9581, grad_fn=<NllLossBackward>)\n","Train Loss 10: 0.9581 Acc: 0.4107 46\n","loss  11  -  tensor(0.9426, grad_fn=<NllLossBackward>)\n","Train Loss 11: 0.9426 Acc: 0.4286 48\n","loss  12  -  tensor(0.9278, grad_fn=<NllLossBackward>)\n","Train Loss 12: 0.9278 Acc: 0.5089 57\n","loss  13  -  tensor(0.9136, grad_fn=<NllLossBackward>)\n","Train Loss 13: 0.9136 Acc: 0.5536 62\n","loss  14  -  tensor(0.8999, grad_fn=<NllLossBackward>)\n","Train Loss 14: 0.8999 Acc: 0.6161 69\n","loss  15  -  tensor(0.8868, grad_fn=<NllLossBackward>)\n","Train Loss 15: 0.8868 Acc: 0.6429 72\n","loss  16  -  tensor(0.8742, grad_fn=<NllLossBackward>)\n","Train Loss 16: 0.8742 Acc: 0.6339 71\n","loss  17  -  tensor(0.8622, grad_fn=<NllLossBackward>)\n","Train Loss 17: 0.8622 Acc: 0.6607 74\n","loss  18  -  tensor(0.8506, grad_fn=<NllLossBackward>)\n","Train Loss 18: 0.8506 Acc: 0.6786 76\n","loss  19  -  tensor(0.8394, grad_fn=<NllLossBackward>)\n","Train Loss 19: 0.8394 Acc: 0.6786 76\n","loss  20  -  tensor(0.8287, grad_fn=<NllLossBackward>)\n","Train Loss 20: 0.8287 Acc: 0.6786 76\n","loss  21  -  tensor(0.8184, grad_fn=<NllLossBackward>)\n","Train Loss 21: 0.8184 Acc: 0.6786 76\n","loss  22  -  tensor(0.8085, grad_fn=<NllLossBackward>)\n","Train Loss 22: 0.8085 Acc: 0.6786 76\n","loss  23  -  tensor(0.7990, grad_fn=<NllLossBackward>)\n","Train Loss 23: 0.7990 Acc: 0.6786 76\n","loss  24  -  tensor(0.7898, grad_fn=<NllLossBackward>)\n","Train Loss 24: 0.7898 Acc: 0.6786 76\n","loss  25  -  tensor(0.7809, grad_fn=<NllLossBackward>)\n","Train Loss 25: 0.7809 Acc: 0.6786 76\n","loss  26  -  tensor(0.7724, grad_fn=<NllLossBackward>)\n","Train Loss 26: 0.7724 Acc: 0.6875 77\n","loss  27  -  tensor(0.7642, grad_fn=<NllLossBackward>)\n","Train Loss 27: 0.7642 Acc: 0.6964 78\n","loss  28  -  tensor(0.7563, grad_fn=<NllLossBackward>)\n","Train Loss 28: 0.7563 Acc: 0.6964 78\n","loss  29  -  tensor(0.7486, grad_fn=<NllLossBackward>)\n","Train Loss 29: 0.7486 Acc: 0.6964 78\n","loss  30  -  tensor(0.7412, grad_fn=<NllLossBackward>)\n","Train Loss 30: 0.7412 Acc: 0.6964 78\n","loss  31  -  tensor(0.7341, grad_fn=<NllLossBackward>)\n","Train Loss 31: 0.7341 Acc: 0.7143 80\n","loss  32  -  tensor(0.7271, grad_fn=<NllLossBackward>)\n","Train Loss 32: 0.7271 Acc: 0.7232 81\n","loss  33  -  tensor(0.7204, grad_fn=<NllLossBackward>)\n","Train Loss 33: 0.7204 Acc: 0.7232 81\n","loss  34  -  tensor(0.7140, grad_fn=<NllLossBackward>)\n","Train Loss 34: 0.7140 Acc: 0.7232 81\n","loss  35  -  tensor(0.7077, grad_fn=<NllLossBackward>)\n","Train Loss 35: 0.7077 Acc: 0.7232 81\n","loss  36  -  tensor(0.7016, grad_fn=<NllLossBackward>)\n","Train Loss 36: 0.7016 Acc: 0.7232 81\n","loss  37  -  tensor(0.6957, grad_fn=<NllLossBackward>)\n","Train Loss 37: 0.6957 Acc: 0.7232 81\n","loss  38  -  tensor(0.6900, grad_fn=<NllLossBackward>)\n","Train Loss 38: 0.6900 Acc: 0.7232 81\n","loss  39  -  tensor(0.6844, grad_fn=<NllLossBackward>)\n","Train Loss 39: 0.6844 Acc: 0.7232 81\n","loss  40  -  tensor(0.6790, grad_fn=<NllLossBackward>)\n","Train Loss 40: 0.6790 Acc: 0.7232 81\n","loss  41  -  tensor(0.6738, grad_fn=<NllLossBackward>)\n","Train Loss 41: 0.6738 Acc: 0.7321 82\n","loss  42  -  tensor(0.6687, grad_fn=<NllLossBackward>)\n","Train Loss 42: 0.6687 Acc: 0.7321 82\n","loss  43  -  tensor(0.6637, grad_fn=<NllLossBackward>)\n","Train Loss 43: 0.6637 Acc: 0.7411 83\n","loss  44  -  tensor(0.6589, grad_fn=<NllLossBackward>)\n","Train Loss 44: 0.6589 Acc: 0.7411 83\n","loss  45  -  tensor(0.6542, grad_fn=<NllLossBackward>)\n","Train Loss 45: 0.6542 Acc: 0.7411 83\n","loss  46  -  tensor(0.6496, grad_fn=<NllLossBackward>)\n","Train Loss 46: 0.6496 Acc: 0.7411 83\n","loss  47  -  tensor(0.6452, grad_fn=<NllLossBackward>)\n","Train Loss 47: 0.6452 Acc: 0.7411 83\n","loss  48  -  tensor(0.6408, grad_fn=<NllLossBackward>)\n","Train Loss 48: 0.6408 Acc: 0.7411 83\n","loss  49  -  tensor(0.6366, grad_fn=<NllLossBackward>)\n","Train Loss 49: 0.6366 Acc: 0.7411 83\n","loss  50  -  tensor(0.6324, grad_fn=<NllLossBackward>)\n","Train Loss 50: 0.6324 Acc: 0.7411 83\n","loss  51  -  tensor(0.6284, grad_fn=<NllLossBackward>)\n","Train Loss 51: 0.6284 Acc: 0.7411 83\n","loss  52  -  tensor(0.6244, grad_fn=<NllLossBackward>)\n","Train Loss 52: 0.6244 Acc: 0.7411 83\n","loss  53  -  tensor(0.6205, grad_fn=<NllLossBackward>)\n","Train Loss 53: 0.6205 Acc: 0.7411 83\n","loss  54  -  tensor(0.6168, grad_fn=<NllLossBackward>)\n","Train Loss 54: 0.6168 Acc: 0.7589 85\n","loss  55  -  tensor(0.6131, grad_fn=<NllLossBackward>)\n","Train Loss 55: 0.6131 Acc: 0.7679 86\n","loss  56  -  tensor(0.6095, grad_fn=<NllLossBackward>)\n","Train Loss 56: 0.6095 Acc: 0.7679 86\n","loss  57  -  tensor(0.6059, grad_fn=<NllLossBackward>)\n","Train Loss 57: 0.6059 Acc: 0.7679 86\n","loss  58  -  tensor(0.6025, grad_fn=<NllLossBackward>)\n","Train Loss 58: 0.6025 Acc: 0.7679 86\n","loss  59  -  tensor(0.5991, grad_fn=<NllLossBackward>)\n","Train Loss 59: 0.5991 Acc: 0.7679 86\n","loss  60  -  tensor(0.5957, grad_fn=<NllLossBackward>)\n","Train Loss 60: 0.5957 Acc: 0.7768 87\n","loss  61  -  tensor(0.5925, grad_fn=<NllLossBackward>)\n","Train Loss 61: 0.5925 Acc: 0.7679 86\n","loss  62  -  tensor(0.5893, grad_fn=<NllLossBackward>)\n","Train Loss 62: 0.5893 Acc: 0.7679 86\n","loss  63  -  tensor(0.5862, grad_fn=<NllLossBackward>)\n","Train Loss 63: 0.5862 Acc: 0.7768 87\n","loss  64  -  tensor(0.5831, grad_fn=<NllLossBackward>)\n","Train Loss 64: 0.5831 Acc: 0.7768 87\n","loss  65  -  tensor(0.5801, grad_fn=<NllLossBackward>)\n","Train Loss 65: 0.5801 Acc: 0.7768 87\n","loss  66  -  tensor(0.5771, grad_fn=<NllLossBackward>)\n","Train Loss 66: 0.5771 Acc: 0.7768 87\n","loss  67  -  tensor(0.5742, grad_fn=<NllLossBackward>)\n","Train Loss 67: 0.5742 Acc: 0.7857 88\n","loss  68  -  tensor(0.5714, grad_fn=<NllLossBackward>)\n","Train Loss 68: 0.5714 Acc: 0.7857 88\n","loss  69  -  tensor(0.5686, grad_fn=<NllLossBackward>)\n","Train Loss 69: 0.5686 Acc: 0.7857 88\n","loss  70  -  tensor(0.5658, grad_fn=<NllLossBackward>)\n","Train Loss 70: 0.5658 Acc: 0.7946 89\n","loss  71  -  tensor(0.5631, grad_fn=<NllLossBackward>)\n","Train Loss 71: 0.5631 Acc: 0.8036 90\n","loss  72  -  tensor(0.5604, grad_fn=<NllLossBackward>)\n","Train Loss 72: 0.5604 Acc: 0.8036 90\n","loss  73  -  tensor(0.5578, grad_fn=<NllLossBackward>)\n","Train Loss 73: 0.5578 Acc: 0.8036 90\n","loss  74  -  tensor(0.5553, grad_fn=<NllLossBackward>)\n","Train Loss 74: 0.5553 Acc: 0.8036 90\n","loss  75  -  tensor(0.5527, grad_fn=<NllLossBackward>)\n","Train Loss 75: 0.5527 Acc: 0.8036 90\n","loss  76  -  tensor(0.5502, grad_fn=<NllLossBackward>)\n","Train Loss 76: 0.5502 Acc: 0.8036 90\n","loss  77  -  tensor(0.5478, grad_fn=<NllLossBackward>)\n","Train Loss 77: 0.5478 Acc: 0.8036 90\n","loss  78  -  tensor(0.5454, grad_fn=<NllLossBackward>)\n","Train Loss 78: 0.5454 Acc: 0.8036 90\n","loss  79  -  tensor(0.5430, grad_fn=<NllLossBackward>)\n","Train Loss 79: 0.5430 Acc: 0.8036 90\n","loss  80  -  tensor(0.5406, grad_fn=<NllLossBackward>)\n","Train Loss 80: 0.5406 Acc: 0.8125 91\n","loss  81  -  tensor(0.5383, grad_fn=<NllLossBackward>)\n","Train Loss 81: 0.5383 Acc: 0.8125 91\n","loss  82  -  tensor(0.5361, grad_fn=<NllLossBackward>)\n","Train Loss 82: 0.5361 Acc: 0.8125 91\n","loss  83  -  tensor(0.5338, grad_fn=<NllLossBackward>)\n","Train Loss 83: 0.5338 Acc: 0.8125 91\n","loss  84  -  tensor(0.5316, grad_fn=<NllLossBackward>)\n","Train Loss 84: 0.5316 Acc: 0.8214 92\n","loss  85  -  tensor(0.5295, grad_fn=<NllLossBackward>)\n","Train Loss 85: 0.5295 Acc: 0.8125 91\n","loss  86  -  tensor(0.5273, grad_fn=<NllLossBackward>)\n","Train Loss 86: 0.5273 Acc: 0.8125 91\n","loss  87  -  tensor(0.5252, grad_fn=<NllLossBackward>)\n","Train Loss 87: 0.5252 Acc: 0.8125 91\n","loss  88  -  tensor(0.5231, grad_fn=<NllLossBackward>)\n","Train Loss 88: 0.5231 Acc: 0.8125 91\n","loss  89  -  tensor(0.5211, grad_fn=<NllLossBackward>)\n","Train Loss 89: 0.5211 Acc: 0.8125 91\n","loss  90  -  tensor(0.5190, grad_fn=<NllLossBackward>)\n","Train Loss 90: 0.5190 Acc: 0.8125 91\n","loss  91  -  tensor(0.5170, grad_fn=<NllLossBackward>)\n","Train Loss 91: 0.5170 Acc: 0.8125 91\n","loss  92  -  tensor(0.5151, grad_fn=<NllLossBackward>)\n","Train Loss 92: 0.5151 Acc: 0.8125 91\n","loss  93  -  tensor(0.5131, grad_fn=<NllLossBackward>)\n","Train Loss 93: 0.5131 Acc: 0.8125 91\n","loss  94  -  tensor(0.5112, grad_fn=<NllLossBackward>)\n","Train Loss 94: 0.5112 Acc: 0.8125 91\n","loss  95  -  tensor(0.5093, grad_fn=<NllLossBackward>)\n","Train Loss 95: 0.5093 Acc: 0.8125 91\n","loss  96  -  tensor(0.5074, grad_fn=<NllLossBackward>)\n","Train Loss 96: 0.5074 Acc: 0.8125 91\n","loss  97  -  tensor(0.5056, grad_fn=<NllLossBackward>)\n","Train Loss 97: 0.5056 Acc: 0.8125 91\n","loss  98  -  tensor(0.5038, grad_fn=<NllLossBackward>)\n","Train Loss 98: 0.5038 Acc: 0.8125 91\n","loss  99  -  tensor(0.5020, grad_fn=<NllLossBackward>)\n","Train Loss 99: 0.5020 Acc: 0.8125 91\n","loss  100  -  tensor(0.5002, grad_fn=<NllLossBackward>)\n","Train Loss 100: 0.5002 Acc: 0.8125 91\n","loss  101  -  tensor(0.4984, grad_fn=<NllLossBackward>)\n","Train Loss 101: 0.4984 Acc: 0.8125 91\n","loss  102  -  tensor(0.4967, grad_fn=<NllLossBackward>)\n","Train Loss 102: 0.4967 Acc: 0.8125 91\n","loss  103  -  tensor(0.4950, grad_fn=<NllLossBackward>)\n","Train Loss 103: 0.4950 Acc: 0.8125 91\n","loss  104  -  tensor(0.4933, grad_fn=<NllLossBackward>)\n","Train Loss 104: 0.4933 Acc: 0.8125 91\n","loss  105  -  tensor(0.4916, grad_fn=<NllLossBackward>)\n","Train Loss 105: 0.4916 Acc: 0.8125 91\n","loss  106  -  tensor(0.4900, grad_fn=<NllLossBackward>)\n","Train Loss 106: 0.4900 Acc: 0.8214 92\n","loss  107  -  tensor(0.4883, grad_fn=<NllLossBackward>)\n","Train Loss 107: 0.4883 Acc: 0.8214 92\n","loss  108  -  tensor(0.4867, grad_fn=<NllLossBackward>)\n","Train Loss 108: 0.4867 Acc: 0.8214 92\n","loss  109  -  tensor(0.4851, grad_fn=<NllLossBackward>)\n","Train Loss 109: 0.4851 Acc: 0.8214 92\n","loss  110  -  tensor(0.4835, grad_fn=<NllLossBackward>)\n","Train Loss 110: 0.4835 Acc: 0.8214 92\n","loss  111  -  tensor(0.4820, grad_fn=<NllLossBackward>)\n","Train Loss 111: 0.4820 Acc: 0.8214 92\n","loss  112  -  tensor(0.4804, grad_fn=<NllLossBackward>)\n","Train Loss 112: 0.4804 Acc: 0.8214 92\n","loss  113  -  tensor(0.4789, grad_fn=<NllLossBackward>)\n","Train Loss 113: 0.4789 Acc: 0.8214 92\n","loss  114  -  tensor(0.4774, grad_fn=<NllLossBackward>)\n","Train Loss 114: 0.4774 Acc: 0.8214 92\n","loss  115  -  tensor(0.4759, grad_fn=<NllLossBackward>)\n","Train Loss 115: 0.4759 Acc: 0.8214 92\n","loss  116  -  tensor(0.4744, grad_fn=<NllLossBackward>)\n","Train Loss 116: 0.4744 Acc: 0.8214 92\n","loss  117  -  tensor(0.4729, grad_fn=<NllLossBackward>)\n","Train Loss 117: 0.4729 Acc: 0.8304 93\n","loss  118  -  tensor(0.4715, grad_fn=<NllLossBackward>)\n","Train Loss 118: 0.4715 Acc: 0.8304 93\n","loss  119  -  tensor(0.4701, grad_fn=<NllLossBackward>)\n","Train Loss 119: 0.4701 Acc: 0.8393 94\n","loss  120  -  tensor(0.4687, grad_fn=<NllLossBackward>)\n","Train Loss 120: 0.4687 Acc: 0.8393 94\n","loss  121  -  tensor(0.4673, grad_fn=<NllLossBackward>)\n","Train Loss 121: 0.4673 Acc: 0.8393 94\n","loss  122  -  tensor(0.4659, grad_fn=<NllLossBackward>)\n","Train Loss 122: 0.4659 Acc: 0.8393 94\n","loss  123  -  tensor(0.4645, grad_fn=<NllLossBackward>)\n","Train Loss 123: 0.4645 Acc: 0.8393 94\n","loss  124  -  tensor(0.4631, grad_fn=<NllLossBackward>)\n","Train Loss 124: 0.4631 Acc: 0.8393 94\n","loss  125  -  tensor(0.4618, grad_fn=<NllLossBackward>)\n","Train Loss 125: 0.4618 Acc: 0.8393 94\n","loss  126  -  tensor(0.4605, grad_fn=<NllLossBackward>)\n","Train Loss 126: 0.4605 Acc: 0.8393 94\n","loss  127  -  tensor(0.4591, grad_fn=<NllLossBackward>)\n","Train Loss 127: 0.4591 Acc: 0.8393 94\n","loss  128  -  tensor(0.4578, grad_fn=<NllLossBackward>)\n","Train Loss 128: 0.4578 Acc: 0.8393 94\n","loss  129  -  tensor(0.4565, grad_fn=<NllLossBackward>)\n","Train Loss 129: 0.4565 Acc: 0.8393 94\n","loss  130  -  tensor(0.4553, grad_fn=<NllLossBackward>)\n","Train Loss 130: 0.4553 Acc: 0.8393 94\n","loss  131  -  tensor(0.4540, grad_fn=<NllLossBackward>)\n","Train Loss 131: 0.4540 Acc: 0.8393 94\n","loss  132  -  tensor(0.4527, grad_fn=<NllLossBackward>)\n","Train Loss 132: 0.4527 Acc: 0.8393 94\n","loss  133  -  tensor(0.4515, grad_fn=<NllLossBackward>)\n","Train Loss 133: 0.4515 Acc: 0.8393 94\n","loss  134  -  tensor(0.4503, grad_fn=<NllLossBackward>)\n","Train Loss 134: 0.4503 Acc: 0.8393 94\n","loss  135  -  tensor(0.4490, grad_fn=<NllLossBackward>)\n","Train Loss 135: 0.4490 Acc: 0.8482 95\n","loss  136  -  tensor(0.4478, grad_fn=<NllLossBackward>)\n","Train Loss 136: 0.4478 Acc: 0.8482 95\n","loss  137  -  tensor(0.4466, grad_fn=<NllLossBackward>)\n","Train Loss 137: 0.4466 Acc: 0.8482 95\n","loss  138  -  tensor(0.4455, grad_fn=<NllLossBackward>)\n","Train Loss 138: 0.4455 Acc: 0.8482 95\n","loss  139  -  tensor(0.4443, grad_fn=<NllLossBackward>)\n","Train Loss 139: 0.4443 Acc: 0.8482 95\n","loss  140  -  tensor(0.4431, grad_fn=<NllLossBackward>)\n","Train Loss 140: 0.4431 Acc: 0.8482 95\n","loss  141  -  tensor(0.4420, grad_fn=<NllLossBackward>)\n","Train Loss 141: 0.4420 Acc: 0.8482 95\n","loss  142  -  tensor(0.4408, grad_fn=<NllLossBackward>)\n","Train Loss 142: 0.4408 Acc: 0.8482 95\n","loss  143  -  tensor(0.4397, grad_fn=<NllLossBackward>)\n","Train Loss 143: 0.4397 Acc: 0.8482 95\n","loss  144  -  tensor(0.4386, grad_fn=<NllLossBackward>)\n","Train Loss 144: 0.4386 Acc: 0.8482 95\n","loss  145  -  tensor(0.4375, grad_fn=<NllLossBackward>)\n","Train Loss 145: 0.4375 Acc: 0.8571 96\n","loss  146  -  tensor(0.4364, grad_fn=<NllLossBackward>)\n","Train Loss 146: 0.4364 Acc: 0.8571 96\n","loss  147  -  tensor(0.4353, grad_fn=<NllLossBackward>)\n","Train Loss 147: 0.4353 Acc: 0.8571 96\n","loss  148  -  tensor(0.4342, grad_fn=<NllLossBackward>)\n","Train Loss 148: 0.4342 Acc: 0.8571 96\n","loss  149  -  tensor(0.4331, grad_fn=<NllLossBackward>)\n","Train Loss 149: 0.4331 Acc: 0.8571 96\n","loss  150  -  tensor(0.4321, grad_fn=<NllLossBackward>)\n","Train Loss 150: 0.4321 Acc: 0.8571 96\n","loss  151  -  tensor(0.4310, grad_fn=<NllLossBackward>)\n","Train Loss 151: 0.4310 Acc: 0.8571 96\n","loss  152  -  tensor(0.4300, grad_fn=<NllLossBackward>)\n","Train Loss 152: 0.4300 Acc: 0.8661 97\n","loss  153  -  tensor(0.4289, grad_fn=<NllLossBackward>)\n","Train Loss 153: 0.4289 Acc: 0.8661 97\n","loss  154  -  tensor(0.4279, grad_fn=<NllLossBackward>)\n","Train Loss 154: 0.4279 Acc: 0.8661 97\n","loss  155  -  tensor(0.4269, grad_fn=<NllLossBackward>)\n","Train Loss 155: 0.4269 Acc: 0.8661 97\n","loss  156  -  tensor(0.4259, grad_fn=<NllLossBackward>)\n","Train Loss 156: 0.4259 Acc: 0.8661 97\n","loss  157  -  tensor(0.4249, grad_fn=<NllLossBackward>)\n","Train Loss 157: 0.4249 Acc: 0.8661 97\n","loss  158  -  tensor(0.4239, grad_fn=<NllLossBackward>)\n","Train Loss 158: 0.4239 Acc: 0.8661 97\n","loss  159  -  tensor(0.4229, grad_fn=<NllLossBackward>)\n","Train Loss 159: 0.4229 Acc: 0.8661 97\n","loss  160  -  tensor(0.4219, grad_fn=<NllLossBackward>)\n","Train Loss 160: 0.4219 Acc: 0.8661 97\n","loss  161  -  tensor(0.4210, grad_fn=<NllLossBackward>)\n","Train Loss 161: 0.4210 Acc: 0.8661 97\n","loss  162  -  tensor(0.4200, grad_fn=<NllLossBackward>)\n","Train Loss 162: 0.4200 Acc: 0.8661 97\n","loss  163  -  tensor(0.4191, grad_fn=<NllLossBackward>)\n","Train Loss 163: 0.4191 Acc: 0.8661 97\n","loss  164  -  tensor(0.4181, grad_fn=<NllLossBackward>)\n","Train Loss 164: 0.4181 Acc: 0.8661 97\n","loss  165  -  tensor(0.4172, grad_fn=<NllLossBackward>)\n","Train Loss 165: 0.4172 Acc: 0.8661 97\n","loss  166  -  tensor(0.4163, grad_fn=<NllLossBackward>)\n","Train Loss 166: 0.4163 Acc: 0.8661 97\n","loss  167  -  tensor(0.4153, grad_fn=<NllLossBackward>)\n","Train Loss 167: 0.4153 Acc: 0.8661 97\n","loss  168  -  tensor(0.4144, grad_fn=<NllLossBackward>)\n","Train Loss 168: 0.4144 Acc: 0.8661 97\n","loss  169  -  tensor(0.4135, grad_fn=<NllLossBackward>)\n","Train Loss 169: 0.4135 Acc: 0.8661 97\n","loss  170  -  tensor(0.4126, grad_fn=<NllLossBackward>)\n","Train Loss 170: 0.4126 Acc: 0.8661 97\n","loss  171  -  tensor(0.4117, grad_fn=<NllLossBackward>)\n","Train Loss 171: 0.4117 Acc: 0.8661 97\n","loss  172  -  tensor(0.4108, grad_fn=<NllLossBackward>)\n","Train Loss 172: 0.4108 Acc: 0.8661 97\n","loss  173  -  tensor(0.4100, grad_fn=<NllLossBackward>)\n","Train Loss 173: 0.4100 Acc: 0.8661 97\n","loss  174  -  tensor(0.4091, grad_fn=<NllLossBackward>)\n","Train Loss 174: 0.4091 Acc: 0.8661 97\n","loss  175  -  tensor(0.4082, grad_fn=<NllLossBackward>)\n","Train Loss 175: 0.4082 Acc: 0.8661 97\n","loss  176  -  tensor(0.4074, grad_fn=<NllLossBackward>)\n","Train Loss 176: 0.4074 Acc: 0.8661 97\n","loss  177  -  tensor(0.4065, grad_fn=<NllLossBackward>)\n","Train Loss 177: 0.4065 Acc: 0.8750 98\n","loss  178  -  tensor(0.4057, grad_fn=<NllLossBackward>)\n","Train Loss 178: 0.4057 Acc: 0.8750 98\n","loss  179  -  tensor(0.4048, grad_fn=<NllLossBackward>)\n","Train Loss 179: 0.4048 Acc: 0.8750 98\n","loss  180  -  tensor(0.4040, grad_fn=<NllLossBackward>)\n","Train Loss 180: 0.4040 Acc: 0.8750 98\n","loss  181  -  tensor(0.4032, grad_fn=<NllLossBackward>)\n","Train Loss 181: 0.4032 Acc: 0.8750 98\n","loss  182  -  tensor(0.4024, grad_fn=<NllLossBackward>)\n","Train Loss 182: 0.4024 Acc: 0.8750 98\n","loss  183  -  tensor(0.4016, grad_fn=<NllLossBackward>)\n","Train Loss 183: 0.4016 Acc: 0.8750 98\n","loss  184  -  tensor(0.4008, grad_fn=<NllLossBackward>)\n","Train Loss 184: 0.4008 Acc: 0.8750 98\n","loss  185  -  tensor(0.4000, grad_fn=<NllLossBackward>)\n","Train Loss 185: 0.4000 Acc: 0.8750 98\n","loss  186  -  tensor(0.3992, grad_fn=<NllLossBackward>)\n","Train Loss 186: 0.3992 Acc: 0.8750 98\n","loss  187  -  tensor(0.3984, grad_fn=<NllLossBackward>)\n","Train Loss 187: 0.3984 Acc: 0.8750 98\n","loss  188  -  tensor(0.3976, grad_fn=<NllLossBackward>)\n","Train Loss 188: 0.3976 Acc: 0.8750 98\n","loss  189  -  tensor(0.3968, grad_fn=<NllLossBackward>)\n","Train Loss 189: 0.3968 Acc: 0.8750 98\n","loss  190  -  tensor(0.3960, grad_fn=<NllLossBackward>)\n","Train Loss 190: 0.3960 Acc: 0.8750 98\n","loss  191  -  tensor(0.3953, grad_fn=<NllLossBackward>)\n","Train Loss 191: 0.3953 Acc: 0.8750 98\n","loss  192  -  tensor(0.3945, grad_fn=<NllLossBackward>)\n","Train Loss 192: 0.3945 Acc: 0.8750 98\n","loss  193  -  tensor(0.3938, grad_fn=<NllLossBackward>)\n","Train Loss 193: 0.3938 Acc: 0.8750 98\n","loss  194  -  tensor(0.3930, grad_fn=<NllLossBackward>)\n","Train Loss 194: 0.3930 Acc: 0.8750 98\n","loss  195  -  tensor(0.3923, grad_fn=<NllLossBackward>)\n","Train Loss 195: 0.3923 Acc: 0.8750 98\n","loss  196  -  tensor(0.3915, grad_fn=<NllLossBackward>)\n","Train Loss 196: 0.3915 Acc: 0.8750 98\n","loss  197  -  tensor(0.3908, grad_fn=<NllLossBackward>)\n","Train Loss 197: 0.3908 Acc: 0.8750 98\n","loss  198  -  tensor(0.3901, grad_fn=<NllLossBackward>)\n","Train Loss 198: 0.3901 Acc: 0.8750 98\n","loss  199  -  tensor(0.3893, grad_fn=<NllLossBackward>)\n","Train Loss 199: 0.3893 Acc: 0.8839 99\n","loss  200  -  tensor(0.3886, grad_fn=<NllLossBackward>)\n","Train Loss 200: 0.3886 Acc: 0.8839 99\n","loss  201  -  tensor(0.3879, grad_fn=<NllLossBackward>)\n","Train Loss 201: 0.3879 Acc: 0.8839 99\n","loss  202  -  tensor(0.3872, grad_fn=<NllLossBackward>)\n","Train Loss 202: 0.3872 Acc: 0.8750 98\n","loss  203  -  tensor(0.3865, grad_fn=<NllLossBackward>)\n","Train Loss 203: 0.3865 Acc: 0.8750 98\n","loss  204  -  tensor(0.3858, grad_fn=<NllLossBackward>)\n","Train Loss 204: 0.3858 Acc: 0.8750 98\n","loss  205  -  tensor(0.3851, grad_fn=<NllLossBackward>)\n","Train Loss 205: 0.3851 Acc: 0.8750 98\n","loss  206  -  tensor(0.3844, grad_fn=<NllLossBackward>)\n","Train Loss 206: 0.3844 Acc: 0.8750 98\n","loss  207  -  tensor(0.3837, grad_fn=<NllLossBackward>)\n","Train Loss 207: 0.3837 Acc: 0.8750 98\n","loss  208  -  tensor(0.3831, grad_fn=<NllLossBackward>)\n","Train Loss 208: 0.3831 Acc: 0.8750 98\n","loss  209  -  tensor(0.3824, grad_fn=<NllLossBackward>)\n","Train Loss 209: 0.3824 Acc: 0.8750 98\n","loss  210  -  tensor(0.3817, grad_fn=<NllLossBackward>)\n","Train Loss 210: 0.3817 Acc: 0.8750 98\n","loss  211  -  tensor(0.3810, grad_fn=<NllLossBackward>)\n","Train Loss 211: 0.3810 Acc: 0.8750 98\n","loss  212  -  tensor(0.3804, grad_fn=<NllLossBackward>)\n","Train Loss 212: 0.3804 Acc: 0.8750 98\n","loss  213  -  tensor(0.3797, grad_fn=<NllLossBackward>)\n","Train Loss 213: 0.3797 Acc: 0.8750 98\n","loss  214  -  tensor(0.3791, grad_fn=<NllLossBackward>)\n","Train Loss 214: 0.3791 Acc: 0.8750 98\n","loss  215  -  tensor(0.3784, grad_fn=<NllLossBackward>)\n","Train Loss 215: 0.3784 Acc: 0.8750 98\n","loss  216  -  tensor(0.3778, grad_fn=<NllLossBackward>)\n","Train Loss 216: 0.3778 Acc: 0.8750 98\n","loss  217  -  tensor(0.3771, grad_fn=<NllLossBackward>)\n","Train Loss 217: 0.3771 Acc: 0.8750 98\n","loss  218  -  tensor(0.3765, grad_fn=<NllLossBackward>)\n","Train Loss 218: 0.3765 Acc: 0.8750 98\n","loss  219  -  tensor(0.3759, grad_fn=<NllLossBackward>)\n","Train Loss 219: 0.3759 Acc: 0.8750 98\n","loss  220  -  tensor(0.3752, grad_fn=<NllLossBackward>)\n","Train Loss 220: 0.3752 Acc: 0.8750 98\n","loss  221  -  tensor(0.3746, grad_fn=<NllLossBackward>)\n","Train Loss 221: 0.3746 Acc: 0.8750 98\n","loss  222  -  tensor(0.3740, grad_fn=<NllLossBackward>)\n","Train Loss 222: 0.3740 Acc: 0.8750 98\n","loss  223  -  tensor(0.3734, grad_fn=<NllLossBackward>)\n","Train Loss 223: 0.3734 Acc: 0.8750 98\n","loss  224  -  tensor(0.3727, grad_fn=<NllLossBackward>)\n","Train Loss 224: 0.3727 Acc: 0.8750 98\n","loss  225  -  tensor(0.3721, grad_fn=<NllLossBackward>)\n","Train Loss 225: 0.3721 Acc: 0.8750 98\n","loss  226  -  tensor(0.3715, grad_fn=<NllLossBackward>)\n","Train Loss 226: 0.3715 Acc: 0.8750 98\n","loss  227  -  tensor(0.3709, grad_fn=<NllLossBackward>)\n","Train Loss 227: 0.3709 Acc: 0.8750 98\n","loss  228  -  tensor(0.3703, grad_fn=<NllLossBackward>)\n","Train Loss 228: 0.3703 Acc: 0.8750 98\n","loss  229  -  tensor(0.3697, grad_fn=<NllLossBackward>)\n","Train Loss 229: 0.3697 Acc: 0.8929 100\n","loss  230  -  tensor(0.3691, grad_fn=<NllLossBackward>)\n","Train Loss 230: 0.3691 Acc: 0.8929 100\n","loss  231  -  tensor(0.3685, grad_fn=<NllLossBackward>)\n","Train Loss 231: 0.3685 Acc: 0.8929 100\n","loss  232  -  tensor(0.3680, grad_fn=<NllLossBackward>)\n","Train Loss 232: 0.3680 Acc: 0.8929 100\n","loss  233  -  tensor(0.3674, grad_fn=<NllLossBackward>)\n","Train Loss 233: 0.3674 Acc: 0.8929 100\n","loss  234  -  tensor(0.3668, grad_fn=<NllLossBackward>)\n","Train Loss 234: 0.3668 Acc: 0.8929 100\n","loss  235  -  tensor(0.3662, grad_fn=<NllLossBackward>)\n","Train Loss 235: 0.3662 Acc: 0.8929 100\n","loss  236  -  tensor(0.3657, grad_fn=<NllLossBackward>)\n","Train Loss 236: 0.3657 Acc: 0.8929 100\n","loss  237  -  tensor(0.3651, grad_fn=<NllLossBackward>)\n","Train Loss 237: 0.3651 Acc: 0.8929 100\n","loss  238  -  tensor(0.3645, grad_fn=<NllLossBackward>)\n","Train Loss 238: 0.3645 Acc: 0.8929 100\n","loss  239  -  tensor(0.3640, grad_fn=<NllLossBackward>)\n","Train Loss 239: 0.3640 Acc: 0.8929 100\n","loss  240  -  tensor(0.3634, grad_fn=<NllLossBackward>)\n","Train Loss 240: 0.3634 Acc: 0.8929 100\n","loss  241  -  tensor(0.3629, grad_fn=<NllLossBackward>)\n","Train Loss 241: 0.3629 Acc: 0.8929 100\n","loss  242  -  tensor(0.3623, grad_fn=<NllLossBackward>)\n","Train Loss 242: 0.3623 Acc: 0.8929 100\n","loss  243  -  tensor(0.3618, grad_fn=<NllLossBackward>)\n","Train Loss 243: 0.3618 Acc: 0.8929 100\n","loss  244  -  tensor(0.3612, grad_fn=<NllLossBackward>)\n","Train Loss 244: 0.3612 Acc: 0.8929 100\n","loss  245  -  tensor(0.3607, grad_fn=<NllLossBackward>)\n","Train Loss 245: 0.3607 Acc: 0.8929 100\n","loss  246  -  tensor(0.3601, grad_fn=<NllLossBackward>)\n","Train Loss 246: 0.3601 Acc: 0.8929 100\n","loss  247  -  tensor(0.3596, grad_fn=<NllLossBackward>)\n","Train Loss 247: 0.3596 Acc: 0.8929 100\n","loss  248  -  tensor(0.3591, grad_fn=<NllLossBackward>)\n","Train Loss 248: 0.3591 Acc: 0.8929 100\n","loss  249  -  tensor(0.3585, grad_fn=<NllLossBackward>)\n","Train Loss 249: 0.3585 Acc: 0.8929 100\n","loss  250  -  tensor(0.3580, grad_fn=<NllLossBackward>)\n","Train Loss 250: 0.3580 Acc: 0.8929 100\n","loss  251  -  tensor(0.3575, grad_fn=<NllLossBackward>)\n","Train Loss 251: 0.3575 Acc: 0.8929 100\n","loss  252  -  tensor(0.3569, grad_fn=<NllLossBackward>)\n","Train Loss 252: 0.3569 Acc: 0.8929 100\n","loss  253  -  tensor(0.3564, grad_fn=<NllLossBackward>)\n","Train Loss 253: 0.3564 Acc: 0.8929 100\n","loss  254  -  tensor(0.3559, grad_fn=<NllLossBackward>)\n","Train Loss 254: 0.3559 Acc: 0.8929 100\n","loss  255  -  tensor(0.3554, grad_fn=<NllLossBackward>)\n","Train Loss 255: 0.3554 Acc: 0.8929 100\n","loss  256  -  tensor(0.3549, grad_fn=<NllLossBackward>)\n","Train Loss 256: 0.3549 Acc: 0.8929 100\n","loss  257  -  tensor(0.3544, grad_fn=<NllLossBackward>)\n","Train Loss 257: 0.3544 Acc: 0.8929 100\n","loss  258  -  tensor(0.3539, grad_fn=<NllLossBackward>)\n","Train Loss 258: 0.3539 Acc: 0.8929 100\n","loss  259  -  tensor(0.3534, grad_fn=<NllLossBackward>)\n","Train Loss 259: 0.3534 Acc: 0.9018 101\n","loss  260  -  tensor(0.3529, grad_fn=<NllLossBackward>)\n","Train Loss 260: 0.3529 Acc: 0.9018 101\n","loss  261  -  tensor(0.3524, grad_fn=<NllLossBackward>)\n","Train Loss 261: 0.3524 Acc: 0.9018 101\n","loss  262  -  tensor(0.3519, grad_fn=<NllLossBackward>)\n","Train Loss 262: 0.3519 Acc: 0.9018 101\n","loss  263  -  tensor(0.3514, grad_fn=<NllLossBackward>)\n","Train Loss 263: 0.3514 Acc: 0.9018 101\n","loss  264  -  tensor(0.3509, grad_fn=<NllLossBackward>)\n","Train Loss 264: 0.3509 Acc: 0.9018 101\n","loss  265  -  tensor(0.3504, grad_fn=<NllLossBackward>)\n","Train Loss 265: 0.3504 Acc: 0.9018 101\n","loss  266  -  tensor(0.3499, grad_fn=<NllLossBackward>)\n","Train Loss 266: 0.3499 Acc: 0.9018 101\n","loss  267  -  tensor(0.3494, grad_fn=<NllLossBackward>)\n","Train Loss 267: 0.3494 Acc: 0.9018 101\n","loss  268  -  tensor(0.3490, grad_fn=<NllLossBackward>)\n","Train Loss 268: 0.3490 Acc: 0.9018 101\n","loss  269  -  tensor(0.3485, grad_fn=<NllLossBackward>)\n","Train Loss 269: 0.3485 Acc: 0.9018 101\n","loss  270  -  tensor(0.3480, grad_fn=<NllLossBackward>)\n","Train Loss 270: 0.3480 Acc: 0.9018 101\n","loss  271  -  tensor(0.3475, grad_fn=<NllLossBackward>)\n","Train Loss 271: 0.3475 Acc: 0.9018 101\n","loss  272  -  tensor(0.3471, grad_fn=<NllLossBackward>)\n","Train Loss 272: 0.3471 Acc: 0.9018 101\n","loss  273  -  tensor(0.3466, grad_fn=<NllLossBackward>)\n","Train Loss 273: 0.3466 Acc: 0.9018 101\n","loss  274  -  tensor(0.3461, grad_fn=<NllLossBackward>)\n","Train Loss 274: 0.3461 Acc: 0.9018 101\n","loss  275  -  tensor(0.3457, grad_fn=<NllLossBackward>)\n","Train Loss 275: 0.3457 Acc: 0.9018 101\n","loss  276  -  tensor(0.3452, grad_fn=<NllLossBackward>)\n","Train Loss 276: 0.3452 Acc: 0.9018 101\n","loss  277  -  tensor(0.3448, grad_fn=<NllLossBackward>)\n","Train Loss 277: 0.3448 Acc: 0.9018 101\n","loss  278  -  tensor(0.3443, grad_fn=<NllLossBackward>)\n","Train Loss 278: 0.3443 Acc: 0.9018 101\n","loss  279  -  tensor(0.3438, grad_fn=<NllLossBackward>)\n","Train Loss 279: 0.3438 Acc: 0.9018 101\n","loss  280  -  tensor(0.3434, grad_fn=<NllLossBackward>)\n","Train Loss 280: 0.3434 Acc: 0.9018 101\n","loss  281  -  tensor(0.3429, grad_fn=<NllLossBackward>)\n","Train Loss 281: 0.3429 Acc: 0.9018 101\n","loss  282  -  tensor(0.3425, grad_fn=<NllLossBackward>)\n","Train Loss 282: 0.3425 Acc: 0.9018 101\n","loss  283  -  tensor(0.3420, grad_fn=<NllLossBackward>)\n","Train Loss 283: 0.3420 Acc: 0.9018 101\n","loss  284  -  tensor(0.3416, grad_fn=<NllLossBackward>)\n","Train Loss 284: 0.3416 Acc: 0.9018 101\n","loss  285  -  tensor(0.3412, grad_fn=<NllLossBackward>)\n","Train Loss 285: 0.3412 Acc: 0.9018 101\n","loss  286  -  tensor(0.3407, grad_fn=<NllLossBackward>)\n","Train Loss 286: 0.3407 Acc: 0.9018 101\n","loss  287  -  tensor(0.3403, grad_fn=<NllLossBackward>)\n","Train Loss 287: 0.3403 Acc: 0.9018 101\n","loss  288  -  tensor(0.3398, grad_fn=<NllLossBackward>)\n","Train Loss 288: 0.3398 Acc: 0.9018 101\n","loss  289  -  tensor(0.3394, grad_fn=<NllLossBackward>)\n","Train Loss 289: 0.3394 Acc: 0.9018 101\n","loss  290  -  tensor(0.3390, grad_fn=<NllLossBackward>)\n","Train Loss 290: 0.3390 Acc: 0.9018 101\n","loss  291  -  tensor(0.3386, grad_fn=<NllLossBackward>)\n","Train Loss 291: 0.3386 Acc: 0.9018 101\n","loss  292  -  tensor(0.3381, grad_fn=<NllLossBackward>)\n","Train Loss 292: 0.3381 Acc: 0.9018 101\n","loss  293  -  tensor(0.3377, grad_fn=<NllLossBackward>)\n","Train Loss 293: 0.3377 Acc: 0.9018 101\n","loss  294  -  tensor(0.3373, grad_fn=<NllLossBackward>)\n","Train Loss 294: 0.3373 Acc: 0.9018 101\n","loss  295  -  tensor(0.3369, grad_fn=<NllLossBackward>)\n","Train Loss 295: 0.3369 Acc: 0.9018 101\n","loss  296  -  tensor(0.3364, grad_fn=<NllLossBackward>)\n","Train Loss 296: 0.3364 Acc: 0.9018 101\n","loss  297  -  tensor(0.3360, grad_fn=<NllLossBackward>)\n","Train Loss 297: 0.3360 Acc: 0.9018 101\n","loss  298  -  tensor(0.3356, grad_fn=<NllLossBackward>)\n","Train Loss 298: 0.3356 Acc: 0.9018 101\n","loss  299  -  tensor(0.3352, grad_fn=<NllLossBackward>)\n","Train Loss 299: 0.3352 Acc: 0.9018 101\n"]}]},{"cell_type":"code","metadata":{"id":"XYiK6cjqWPrv","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1633340999436,"user_tz":-540,"elapsed":271,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"315210ac-5bfc-4a55-d911-3c13511b11a5"},"source":["import matplotlib.pyplot as plt\n","\n","fig = plt.figure()\n","ax = fig.add_subplot()\n","ax.plot(list(range(len(acc))),acc )\n","ax.set_xlabel('#epoch')\n","ax.set_ylabel('accuracy')\n","fig.show()"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5xdZX3v8c93JplcJvdkQMiFBAiXKFcHEKEqihWwJXjrCUoVq8ZWglq1p3DsQUvPedlq5bzUUjRaubRqCFRtqpGAiqJyMYOEhAQDQ0iYCbdkyOQymfv8zh97TdxMZjJ7wqzZe/b6vl+veWWvtZ69129lJfs3z/Os53kUEZiZWXZVFDsAMzMrLicCM7OMcyIwM8s4JwIzs4xzIjAzy7gxxQ5gqGbNmhXz588vdhhmZqPKww8/vDMiavo7NuoSwfz586mrqyt2GGZmo4qkbQMdc9OQmVnGORGYmWWcE4GZWcY5EZiZZZwTgZlZxjkRmJllXKqJQNJFkjZLqpd0TT/Hj5H0M0nrJf1C0pw04zEzs4OlNo5AUiVwI/BWoBFYK2lVRGzKK/bPwG0RcaukNwNfAP48rZjMzPrzyyd28PDWl4odxqDecvKRnDZ32rB/bpoDys4G6iNiC4CkFcBiID8RLAI+lby+F/hhivGYmR1kX3sXy77zO/a2dyEVO5pDO2LK+FGXCGYDDXnbjcA5fco8CrwT+ArwDmCypJkR0ZRfSNJSYCnAvHnzUgvYzIrrxT1ttHf1jOg5/3v9s+xt7+IHH3s9Z8ybPqLnLhXFnmLiM8C/SLoSuA/YDnT3LRQRy4HlALW1tV5SzawM/ezxF/jQrcWZPua0OVMzmwQg3USwHZibtz0n2XdARDxLrkaApEnAuyKiOcWYzKxE3f9UE+PGVPAPl72GkW6hed2xM0f4jKUlzUSwFlgoaQG5BLAEeG9+AUmzgJcioge4Fvh2ivGYWQH+6a7fs+6Z3O9jS86ey+LTZ4/Iedc1NHPK7Kn8We3cwQvbsErt8dGI6AKWAWuAx4GVEbFR0vWSLk2KvQnYLOkJ4Ejg/6YVj5kNbvPze7npF0+xY187T+9s4f/8+HE6RqDNvrO7h8e2706lI9QGl2ofQUSsBlb32Xdd3us7gTvTjMGsHHV197Bm4wu0dx3UpfaKrN7wPOPGVLDyo+eyvrGZK29ey5fv3syJr5o8rOfp64U97bR39XC6E0FRFLuz2MwOwx0PN3Lt9zek8tnvPWceM6qreMPCGk44chLfuG9LKufpa0yFqJ2f3Q7bYnIiMBtlIoJbfrOVk4+awtevOHPYP3/2tAkAVFSIH151Hjv2tg/7OfozadwYZk4aNyLnspdzIjAbZn9753r+83eNqX1+AN09wRffdSrHzKxO7TwAE6vGcMxMf02UO99hs2HUuGs/dzzcwPkLazhl9pTUzlM9bgyXnTEyT/NY+XMiMDsMd9Q18LtnDh7yUv/iXiTxj+88haOTJhazUudEYDZEL+5p49rvb2DC2ErGV1UedPz95x7jJGCjihOBWZ5nmvbT1HLoztH/Wvcs3RH899XnM39Wum30ZiPBicAs8dzuVi684Zd0dA8+gOrNJx3hJGBlw4nALPEfD26js6eHr11+BpPGH/q/xhke+GRlxInAysbm5/fy17evK+g3+v40vLSft5x0JH962tHDHJlZaXMisLJx0y/q2drUwgUnHnFY7z/5qClcdcFxwxyVWelzIrBRb29bJ7evbeDHG57jitcdw+f+9NXFDslsVHEisFFv+X1b+NrP6xk/toIPnDu/2OGYjTpOBPaK9PQEEmiYF3uNCPa0dhEcekG6zu7guw89w1tOOoKbrngtVWNSm1ndrGw5Edhha+vs5sIbfsm7XzuHT154wrB+9pfvfoJ/ube+4PIfPG+Bk4DZYXIisMP2o/XP0birlZt/s5WPvuE4JvQzyvZw7O/o4rYHtnL2ghlc/JpXDVp+RnUV5x2f7aUGzV4JJwIrWE9PcP2PNtG4qxWADdubmTphLLtbO7ni3x5i+sSqYTnPrv0d7Gnr4n++7URq588Yls80s4GlmggkXQR8BagEvhUR/9jn+DzgVmBaUuaaZFUzK0G/qt/JLfdv5diaasaPqWTWpHF8/C0LWbPxeX7/3F5aO1qH7VzvOnMOrz3Gi5SYjYTUEoGkSuBG4K1AI7BW0qqI2JRX7O/IrWV8k6RF5Ja1nJ9WTDawiOCBLU3sa+sasMy3fv00NZPHcdcn3vCy9vi3vXrw5hszK11p1gjOBuojYguApBXAYiA/EQTQO2n7VODZFOOxQ/jVkzt5/7d/O2i5T7/1BHfKmpWZNBPBbKAhb7sROKdPmc8Dd0u6GqgGLuzvgyQtBZYCzJs3b9gDNbjl/q3MmjSOm688i4GeBK2sEAuPmDSygZlZ6ordWXw5cEtEfFnSucC/S3pNRLxsspiIWA4sB6itrT30g+U2ZFt3tnDv5hf5+JsXcsqcqcUOx8xGWJp1/O3A3LztOcm+fB8CVgJExAPAeGBWijFZP257YBuVEu87x7UtsyxKs0awFlgoaQG5BLAEeG+fMs8AbwFukXQyuUSwI8WYLLG/o4uv/+IpWju7uaOugbefehRHTBlf7LDMrAhSSwQR0SVpGbCG3KOh346IjZKuB+oiYhXwaeCbkv6aXMfxlRHhpp8R8J0Hn+GrP69nYlUl48ZU8JE/OrbYIZlZkaTaR5CMCVjdZ991ea83AeelGUOWRQTN+zuZXl1FRLCtaT+d3T0EcOsDuZG7Kz96brHDNLMiK3ZnsaVoZV0D//uHG/nJJ/+ITc/u4ervPfKy4//rkpOLFJmZlRIngjIVESy/bwsd3T3cdv9W1jXuZsGsaj79x7nJ4SZWVR72Ai5mVl6cCMrE/U/t5J/XbKYn6WHp7O7hqR0tHDllHN/97TN0dgfXL341f3Kql2E0s5dzIigTN9z9BE/vbOGUOX9YVP2dZ8xm6RuP5Ut3bWbiuDG868w5RYzQzEqVE0EZ2NC4m7ptu/i7t5/Mh/t5+uffrjyrCFGZ2WjhSWPKwI/WP0tVZQXvqZ07eGEzsz6cCMrAIw3NLDp6ClMnjC12KGY2CjkRjHJd3T1saNzN6XOnDV7YzKwfTgSj3BMv7KO1s9uJwMwOmzuLS9RPN73Ajzc8N2i57cmykU4EZna4nAhKUEdXD9f+YANtnd0FrQP8phNrOGbmxBGIzMzKkRNBiWna186KtQ3s2NvOzR88y6N/zSx1TgQlJCL4i1vreLShmWNrqnnjwppih2RmGeBEUEIeaWjm0YZmrrrgOD7w+vlUVAywZqSZ2TByIhghEcHV33uER55pHrDM3rZOJo8bw8fedDzV43xrzGxk+NtmhDzS0MyP1j/H+cfP4shDrAT2phNrnATMbET5G2eE3Hr/ViaPG8M3/vy1/qI3s5LiAWUjoKu7h7s3vsClpx/tJGBmJSfVRCDpIkmbJdVLuqaf4/9P0rrk5wlJAzegj2K9o3/PXjCj2KGYmR0ktV9PJVUCNwJvBRqBtZJWJesUAxARf51X/mrgjLTiKaZ1Dbn85tG/ZlaK0qwRnA3UR8SWiOgAVgCLD1H+cuB7KcZTNOsadjF94ljmzfDoXzMrPWk2WM8GGvK2G4Fz+iso6RhgAfDzAY4vBZYCzJs3b3ijTEFE8LlVG9myowWA9Y3NnHnMdCSPCzCz0lMqncVLgDsjoru/gxGxPCJqI6K2pqb0R9s+9PRL3PbANnbsbae1s5sTjpzMe88u/QRmZtmUZo1gO5C/ZNacZF9/lgBXpRjLiLrlN1uZNnEs/7XsPMaPrSx2OGZmh5RmjWAtsFDSAklV5L7sV/UtJOkkYDrwQIqxjJjtza3cvel5lpw1z0nAzEaF1BJBRHQBy4A1wOPAyojYKOl6SZfmFV0CrIiISCuWkfQfD24D4IrXuSnIzEaHVEc3RcRqYHWffdf12f58mjEMt10tHVzy1V/xUktHv8c7unt426JXMWe6nxAys9HBw1yHaO3Wl3hudxvvee0cZkw6eNGYCoklZ83t551mZqXJiWCIHm1sZkyF+IfLXuM+ADMrC6Xy+Oiosa6hmZOOmuwkYGZlw4lgCHp6gvUNuz1VhJmVFSeCIdja1MLe9i5OneNEYGblw4lgCJ7emZsy4vgjJhU5EjOz4eNEMAS9iWDBzOoiR2JmNnycCIZgW9N+powfw7SJY4sdipnZsPHjowWICH7y2PM88cJe5s+q9iyiZlZWnAgKULdtFx/7zu8AuPS0o4scjZnZ8HLTUAGeadp/4PXcGROKGImZ2fBzIijA1qaWA69nVo8rYiRmZsPPTUMF2Nq0nznTJ/DZS07mgpOOKHY4ZmbDyomgAFt3trBgVjUXn3JUsUMxMxt2bhoaRESwtamF+R47YGZlyolgEC+1dLC3rYv5s5wIzKw8OREMonFXKwBzp/tpITMrT04Eg9jd2gnAjOqDF6ExMysHqSYCSRdJ2iypXtI1A5T5M0mbJG2U9N004zkce9pyiWDyeE8rYWblqaBEIOn7kt4uqeDEIakSuBG4GFgEXC5pUZ8yC4FrgfMi4tXAJwuOfITsbesCYMoEP2BlZuWp0C/2fwXeCzwp6R8lnVjAe84G6iNiS0R0ACuAxX3KfAS4MSJ2AUTEiwXGM2L2JE1DU1wjMLMyVVAiiIifRsT7gDOBrcBPJd0v6YOSBvqGnA005G03JvvynQCcIOk3kh6UdFF/HyRpqaQ6SXU7duwoJORhs6etk8oKMbHKS1OaWXkaSlPPTOBK4MPAI8BXyCWGe17B+ccAC4E3AZcD35R00PJfEbE8ImojorampuYVnG7o9rZ1MXn8GM84amZlq6CGb0k/AE4E/h3404h4Ljl0u6S6Ad62HZibtz0n2ZevEXgoIjqBpyU9QS4xrC0w/tTtae10s5CZlbVCawRfjYhFEfGFvCQAQETUDvCetcBCSQskVQFLgFV9yvyQXG0ASbPINRVtKTT4kbAnqRGYmZWrQhPBovwmG0nTJX3sUG+IiC5gGbAGeBxYGREbJV0v6dKk2BqgSdIm4F7gbyKiachXkaK9ba4RmFl5K/RX3Y9ExI29GxGxS9JHyD1NNKCIWA2s7rPvurzXAXwq+SlJe1q7mD9rYrHDMDNLTaE1gkrl9ZYmYwQyMdR2T1unB5OZWVkrtEZwF7mO4W8k2x9N9pW9vW1dbhoys7JWaCL4W3Jf/n+VbN8DfCuViEpId0+wr73Lo4rNrKwV9A0XET3ATclPZuxLppdw05CZlbNCxxEsBL5Abs6g8b37I+LYlOIqCb0Tzk3x46NmVsYK7Sy+mVxtoAu4ALgN+I+0gioVvVNQT5ngGoGZla9CE8GEiPgZoIjYFhGfB96eXliloTcRTHUiMLMyVmibR3syBfWTkpaRmypiUnphlYad+9oBmDUpE0/KmllGFVoj+AQwEfg48FrgCuADaQVVKpr2dQAws3pckSMxM0vPoDWCZPDY/4iIzwD7gA+mHlWJeKmlg8oKuWnIzMraoDWCiOgGzh+BWEpOU0s7M6qrqKjwFNRmVr4K7SN4RNIq4A6gpXdnRHw/lahKxM59Hcz0ovVmVuYKTQTjgSbgzXn7AijrRNC0r52Z7ig2szJX6MjizPQL5Gtq6eC06QctmGZmVlYKHVl8M7kawMtExF8Me0QlpGlfh2sEZlb2Cm0a+lHe6/HAO4Bnhz+c0tHW2c2+9i5mTfKjo2ZW3gptGvrP/G1J3wN+nUpEJeKllt4xBK4RmFl5K3RAWV8LgSMGKyTpIkmbJdVLuqaf41dK2iFpXfLz4cOMZ9gdGEzmGoGZlblC+wj28vI+gufJrVFwqPdUAjcCbwUagbWSVkXEpj5Fb4+IZYWHPDI8z5CZZUWhTUOTD+OzzwbqI2ILgKQVwGKgbyIoSS0dubUIqsdVFjkSM7N0FdQ0JOkdkqbmbU+TdNkgb5sNNORtNyb7+nqXpPWS7pQ0d4DzL5VUJ6lux44dhYT8iu3vTQRVXovAzMpboX0En4uI3b0bEdEMfG4Yzv/fwPyIOJXc8pe39lcoIpZHRG1E1NbU1AzDaQfX0t4NwETXCMyszBWaCPorN9ivytuB/N/w5yT7DoiIpohoTza/RW5m05LgGoGZZUWhiaBO0g2Sjkt+bgAeHuQ9a4GFkhZIqgKWAKvyC0g6Km/zUuDxQgNP276kRjBhrGsEZlbeCk0EVwMdwO3ACqANuOpQb4iILmAZsIbcF/zKiNgo6XpJlybFPi5po6RHya11cOXQLyEd+9u7qK6q9MyjZlb2Cn1qqAU4aBxAAe9bDazus++6vNfXAtcO9XNHQktHNxPHuVnIzMpfoU8N3SNpWt72dElr0gur+PZ35GoEZmblrtCmoVnJk0IARMQuChhZPJq1tHcz0R3FZpYBhSaCHknzejckzaef2UjLyf6OLg8mM7NMKPRX3s8Cv5b0S0DAHwFLU4uqBLS0dzHdE86ZWQYUVCOIiLuAWmAz8D3g00BrinEVXUtHt8cQmFkmFDrp3IeBT5AbFLYOeB3wAC9furKs7G/vYqI7i80sAwrtI/gEcBawLSIuAM4Amg/9ltGtpaObaj8+amYZUGgiaIuINgBJ4yLi98CJ6YVVfPs7XCMws2wo9FfexmQcwQ+BeyTtAralF1ZxdXT10NkdrhGYWSYUOrL4HcnLz0u6F5gK3JVaVEXW0t474ZxrBGZW/ob8K29E/DKNQEpJ76I0nmLCzLLgcNcsLmv7O3Izj/rxUTPLAieCfvQ2DXlRGjPLAieCfrzU0gHAlPFeuN7Myp8TQT/WN+6mQnDSqyYXOxQzs9Q5EfRjXUMzJxw52Y+PmlkmOBH0ERE82tjMaXOmDV7YzKwMOBH0sa1pP837Ozl9nhOBmWVDqolA0kWSNkuqlzTgUpeS3iUpJNWmGU8hnt7ZAsAJR7p/wMyyIbVEIKkSuBG4GFgEXC5pUT/lJpOb1O6htGIZit7BZJPHu3/AzLIhzRrB2UB9RGyJiA5gBbC4n3L/APwT0JZiLAXrHUw2YazHEJhZNqSZCGYDDXnbjcm+AySdCcyNiB8f6oMkLZVUJ6lux44dwx9pntYkEXjmUTPLiqJ1FkuqAG4gt9rZIUXE8oiojYjampqaVOPafyARuGnIzLIhzUSwHZibtz0n2ddrMvAa4BeStpJb9WxVsTuMWzu6kGD8WD9QZWbZkOa33VpgoaQFkqqAJcCq3oMRsTsiZkXE/IiYDzwIXBoRdSnGNKj9Hd1MGFuJpGKGYWY2YlJLBBHRBSwD1gCPAysjYqOk6yVdmtZ5X6n9nd3uKDazTEm1ITwiVgOr++y7boCyb0ozlkK1dnQzwR3FZpYhbgjvw2sVm1nWOBH00drZwwQ/MWRmGeJE0EdrRxcT3UdgZhniRNDH/o5uNw2ZWaY4EfThzmIzyxongj5cIzCzrHEi6CP31JA7i80sO5wI+mjtdNOQmWWLE0Gezu4eOrvDTw2ZWaY4EeQ5sBaBawRmliFOBHlaPQW1mWWQE0Ge/ckylX5qyMyyxIkgj5uGzCyLnAjytHZ6mUozyx4ngjxer9jMssiJIM+etk4AJo8fW+RIzMxGjhNBnqZ9HQDMrK4qciRmZiPHiSBP0752KgTTJjoRmFl2pJoIJF0kabOkeknX9HP8LyVtkLRO0q8lLUoznsHsbOlgRnUVlRVeuN7MsiO1RCCpErgRuBhYBFzezxf9dyPilIg4HfgicENa8RSiaV87M6vHFTMEM7MRl2aN4GygPiK2REQHsAJYnF8gIvbkbVYDkWI8g2ra18HMSW4WMrNsSTMRzAYa8rYbk30vI+kqSU+RqxF8vL8PkrRUUp2kuh07dqQSLEBTSwczJ7lGYGbZUvTO4oi4MSKOA/4W+LsByiyPiNqIqK2pqUktlp372v3EkJllTpqJYDswN297TrJvICuAy1KM55Dau7rZ29bFLDcNmVnGpJkI1gILJS2QVAUsAVblF5C0MG/z7cCTKcZzSC+15MYQzHBnsZllTGrzLUdEl6RlwBqgEvh2RGyUdD1QFxGrgGWSLgQ6gV3AB9KKZzAHBpO5RmBmGZPqxPsRsRpY3WffdXmvP5Hm+Ydi5752ADcNmVnmFL2zuFTsbs3NMzR1ghOBmWWLE0Fiz4FE4AnnzCxbnAgSe9pyq5NNHu9lKs0sW5wIEnvaOqkaU8H4sV6LwMyyxYkgsae1iymuDZhZBjkRJPa2dTLFC9KYWQY5EST2tHUx2R3FZpZBTgSJPa2dbhoys0xyIki4acjMssqJILGnrcuPjppZJjkRJPa2dTLFfQRmlkFOBOSmoG7r7HEfgZllkhMBsPfAqGLXCMwse5wI+EMimDLBNQIzyx4nAv4w4dzkca4RmFn2OBGQm2cIcGexmWWSEwF/WKbSU1CbWRalmggkXSRps6R6Sdf0c/xTkjZJWi/pZ5KOSTOegWx6dg9VlRXMnzWxGKc3Myuq1BKBpErgRuBiYBFwuaRFfYo9AtRGxKnAncAX04rnUB5paObko6cwboynoDaz7EmzRnA2UB8RWyKiA1gBLM4vEBH3RsT+ZPNBYE6K8fSrq7uHDY27OWPutJE+tZlZSUgzEcwGGvK2G5N9A/kQ8JP+DkhaKqlOUt2OHTuGMUR48sV9tHZ2c9rcqcP6uWZmo0VJdBZLugKoBb7U3/GIWB4RtRFRW1NTM6znXtfQDMDpc6cP6+eamY0WaY6g2g7Mzduek+x7GUkXAp8F3hgR7SnG0691zzQzbeJY5s90R7GZZVOaNYK1wEJJCyRVAUuAVfkFJJ0BfAO4NCJeTDGWAT3a2Mxpc6YhqRinNzMrutQSQUR0AcuANcDjwMqI2CjpekmXJsW+BEwC7pC0TtKqAT4uFS3tXTzxwl5Oc0exmWVYqpPrRMRqYHWffdflvb4wzfPnu++JHfzksec5eup4lr35eCSxvnE3PYGfGDKzTMvMLGvbmlr48fpn2dPWxSWnHsVxNZN4tDHXUewagZllWUk8NTQS/vzc+dzxl68H4NHkSaF1zzRzzMyJzKiuKmZoZmZFlZlEAHD8EZOorqo88MjouoZcR7GZWZZlKhFUVohT5kxlXUMzz+9u4/k9bZzuZiEzy7hMJQLIDRx7bPtu3v31+3Pb85wIzCzbMtNZ3Os9tXPY3txKd08PF5x4BKfO9tQSZpZtmUsEx9VM4muXn1HsMMzMSkbmmobMzOzlnAjMzDLOicDMLOOcCMzMMs6JwMws45wIzMwyzonAzCzjnAjMzDJOEVHsGIZE0g5g22G+fRawcxjDKSZfS2nytZQmXwscExH9Lvo+6hLBKyGpLiJqix3HcPC1lCZfS2nytRyam4bMzDLOicDMLOOylgiWFzuAYeRrKU2+ltLkazmETPURmJnZwbJWIzAzsz6cCMzMMi4ziUDSRZI2S6qXdE2x4xkqSVslbZC0TlJdsm+GpHskPZn8Ob3YcfZH0rclvSjpsbx9/caunK8m92m9pDOLF/nBBriWz0vantybdZIuyTt2bXItmyW9rThRH0zSXEn3StokaaOkTyT7R919OcS1jMb7Ml7SbyU9mlzL3yf7F0h6KIn5dklVyf5xyXZ9cnz+YZ04Isr+B6gEngKOBaqAR4FFxY5riNewFZjVZ98XgWuS19cA/1TsOAeI/Q3AmcBjg8UOXAL8BBDwOuChYsdfwLV8HvhMP2UXJf/WxgELkn+DlcW+hiS2o4Azk9eTgSeSeEfdfTnEtYzG+yJgUvJ6LPBQ8ve9EliS7P868FfJ648BX09eLwFuP5zzZqVGcDZQHxFbIqIDWAEsLnJMw2ExcGvy+lbgsiLGMqCIuA94qc/ugWJfDNwWOQ8C0yQdNTKRDm6AaxnIYmBFRLRHxNNAPbl/i0UXEc9FxO+S13uBx4HZjML7cohrGUgp35eIiH3J5tjkJ4A3A3cm+/vel977dSfwFkka6nmzkghmAw15240c+h9KKQrgbkkPS1qa7DsyIp5LXj8PHFmc0A7LQLGP1nu1LGky+XZeE92ouJakOeEMcr99jur70udaYBTeF0mVktYBLwL3kKuxNEdEV1IkP94D15Ic3w3MHOo5s5IIysH5EXEmcDFwlaQ35B+MXN1wVD4LPJpjT9wEHAecDjwHfLm44RRO0iTgP4FPRsSe/GOj7b70cy2j8r5ERHdEnA7MIVdTOSntc2YlEWwH5uZtz0n2jRoRsT3580XgB+T+gbzQWz1P/nyxeBEO2UCxj7p7FREvJP95e4Bv8odmhpK+FkljyX1xficivp/sHpX3pb9rGa33pVdENAP3AueSa4obkxzKj/fAtSTHpwJNQz1XVhLBWmBh0vNeRa5TZVWRYyqYpGpJk3tfA38MPEbuGj6QFPsA8F/FifCwDBT7KuD9yVMqrwN25zVVlKQ+beXvIHdvIHctS5InOxYAC4HfjnR8/Unakf8NeDwibsg7NOruy0DXMkrvS42kacnrCcBbyfV53Au8OynW97703q93Az9PanJDU+xe8pH6IffUwxPk2ts+W+x4hhj7seSecngU2NgbP7m2wJ8BTwI/BWYUO9YB4v8euap5J7n2zQ8NFDu5pyZuTO7TBqC22PEXcC3/nsS6PvmPeVRe+c8m17IZuLjY8efFdT65Zp/1wLrk55LReF8OcS2j8b6cCjySxPwYcF2y/1hyyaoeuAMYl+wfn2zXJ8ePPZzzeooJM7OMy0rTkJmZDcCJwMws45wIzMwyzonAzCzjnAjMzDLOicAsj6QvSLpA0mWSrh2hc26VNGskzmXWHycCs5c7B3gQeCNwX5FjMRsRTgRmgKQvSVoPnAU8AHwYuEnSdZKOk3RXMuHfrySdlLznFklfl1Qn6QlJf5LsHy/pZuXWj3hE0gXJ/kpJ/yzpsWQitKvzQrha0u+S96Q+t4xZvjGDFzErfxHxN5JWAu8HPgX8IiLOA5D0M+AvI+JJSecA/0puWmCA+eTmsDkOuFfS8cBVuY+MU5Iv9bslnQB8MCl/ekR0SZqRF8LOiDhT0seAz5BLRGYjwonA7A/OJDeNx0nk5nfpndHy9cAdedO8j8t7z8rITWr2pKQtyXvPB74GEBG/l7QNOAG4kNwiIl3Jsfx1DXonfUgjHIEAAAD4SURBVHsYeOfwX5rZwJwILPMknQ7cQm5Wx53AxNxurSPXV9AcuWmB+9N3jpbDnbOlPfmzG/+/tBHmPgLLvIhYl3zR9y5x+HPgbRFxekTsBp6W9B44sHbvaXlvf4+kCknHkZsYbDPwK+B9SfkTgHnJ/nuAj/ZOJ9ynacisaJwIzMhN/wvsSpp5ToqITXmH3wd8SFLv7K/5y5w+Q27Wx5+Q60doI9eHUCFpA3A7cGVEtAPfSsqvTz7rvWlfl1khPPuo2WGSdAvwo4i4c7CyZqXMNQIzs4xzjcDMLONcIzAzyzgnAjOzjHMiMDPLOCcCM7OMcyIwM8u4/w+lYILNQokMYAAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"Ugf3YAFeu9Rz"},"source":["C++のコードは手作業で修正しています。"]},{"cell_type":"code","metadata":{"id":"ERcQTqId1ymt","executionInfo":{"status":"ok","timestamp":1633341035072,"user_tz":-540,"elapsed":22671,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}}},"source":["!g++ -std=c++14 ../src/cse1_opt.cpp ../src/cse1_param.cpp -I ../../../ctorch/lib -lblas -o ../bin/cse1_opt"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Yvq5MIhdz-X","executionInfo":{"status":"ok","timestamp":1633341084978,"user_tz":-540,"elapsed":3474,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"2f3f11dc-9c1b-4706-a304-b5bd1b0babf2"},"source":["!../bin/cse1_opt"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["### forward computation ...\n","get_classes\n","epoch 0 - loss 1.15318 - accuracy 0.178571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 1 - loss 1.12999 - accuracy 0.178571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 2 - loss 1.10768 - accuracy 0.160714\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 3 - loss 1.08623 - accuracy 0.160714\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 4 - loss 1.06561 - accuracy 0.160714\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 5 - loss 1.04581 - accuracy 0.178571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 6 - loss 1.0268 - accuracy 0.196429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 7 - loss 1.00855 - accuracy 0.214286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 8 - loss 0.991038 - accuracy 0.276786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 9 - loss 0.974234 - accuracy 0.321429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 10 - loss 0.958115 - accuracy 0.410714\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 11 - loss 0.942649 - accuracy 0.428571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 12 - loss 0.927813 - accuracy 0.508929\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 13 - loss 0.913579 - accuracy 0.553571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 14 - loss 0.899921 - accuracy 0.616071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 15 - loss 0.886814 - accuracy 0.642857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 16 - loss 0.874234 - accuracy 0.633929\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 17 - loss 0.862156 - accuracy 0.660714\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 18 - loss 0.850557 - accuracy 0.678571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 19 - loss 0.839412 - accuracy 0.678571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 20 - loss 0.8287 - accuracy 0.678571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 21 - loss 0.8184 - accuracy 0.678571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 22 - loss 0.808493 - accuracy 0.678571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 23 - loss 0.798958 - accuracy 0.678571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 24 - loss 0.789777 - accuracy 0.678571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 25 - loss 0.780932 - accuracy 0.678571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 26 - loss 0.772408 - accuracy 0.6875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 27 - loss 0.764188 - accuracy 0.696429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 28 - loss 0.756256 - accuracy 0.696429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 29 - loss 0.748598 - accuracy 0.696429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 30 - loss 0.741201 - accuracy 0.696429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 31 - loss 0.734051 - accuracy 0.714286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 32 - loss 0.727136 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 33 - loss 0.720446 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 34 - loss 0.713968 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 35 - loss 0.707693 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 36 - loss 0.701611 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 37 - loss 0.695713 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 38 - loss 0.68999 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 39 - loss 0.684434 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 40 - loss 0.679037 - accuracy 0.723214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 41 - loss 0.673792 - accuracy 0.732143\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 42 - loss 0.668692 - accuracy 0.732143\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 43 - loss 0.663731 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 44 - loss 0.658902 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 45 - loss 0.654201 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 46 - loss 0.64962 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 47 - loss 0.645155 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 48 - loss 0.640802 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 49 - loss 0.636555 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 50 - loss 0.63241 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 51 - loss 0.628362 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 52 - loss 0.624409 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 53 - loss 0.620545 - accuracy 0.741071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 54 - loss 0.616769 - accuracy 0.758929\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 55 - loss 0.613075 - accuracy 0.767857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 56 - loss 0.609461 - accuracy 0.767857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 57 - loss 0.605924 - accuracy 0.767857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 58 - loss 0.602461 - accuracy 0.767857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 59 - loss 0.599069 - accuracy 0.767857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 60 - loss 0.595746 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 61 - loss 0.592489 - accuracy 0.767857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 62 - loss 0.589296 - accuracy 0.767857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 63 - loss 0.586165 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 64 - loss 0.583093 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 65 - loss 0.580079 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 66 - loss 0.577121 - accuracy 0.776786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 67 - loss 0.574216 - accuracy 0.785714\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 68 - loss 0.571363 - accuracy 0.785714\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 69 - loss 0.568561 - accuracy 0.785714\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 70 - loss 0.565807 - accuracy 0.794643\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 71 - loss 0.563101 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 72 - loss 0.56044 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 73 - loss 0.557824 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 74 - loss 0.555251 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 75 - loss 0.55272 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 76 - loss 0.550229 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 77 - loss 0.547778 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 78 - loss 0.545364 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 79 - loss 0.542988 - accuracy 0.803571\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 80 - loss 0.540649 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 81 - loss 0.538345 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 82 - loss 0.536075 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 83 - loss 0.533838 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 84 - loss 0.531634 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 85 - loss 0.529461 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 86 - loss 0.52732 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 87 - loss 0.525208 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 88 - loss 0.523126 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 89 - loss 0.521073 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 90 - loss 0.519048 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 91 - loss 0.517049 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 92 - loss 0.515078 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 93 - loss 0.513132 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 94 - loss 0.511212 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 95 - loss 0.509317 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 96 - loss 0.507445 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 97 - loss 0.505598 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 98 - loss 0.503774 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 99 - loss 0.501972 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 100 - loss 0.500193 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 101 - loss 0.498436 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 102 - loss 0.4967 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 103 - loss 0.494985 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 104 - loss 0.49329 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 105 - loss 0.491615 - accuracy 0.8125\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 106 - loss 0.48996 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 107 - loss 0.488325 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 108 - loss 0.486708 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 109 - loss 0.485109 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 110 - loss 0.483529 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 111 - loss 0.481967 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 112 - loss 0.480421 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 113 - loss 0.478894 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 114 - loss 0.477382 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 115 - loss 0.475888 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 116 - loss 0.47441 - accuracy 0.821429\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 117 - loss 0.472947 - accuracy 0.830357\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 118 - loss 0.4715 - accuracy 0.830357\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 119 - loss 0.470068 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 120 - loss 0.468652 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 121 - loss 0.46725 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 122 - loss 0.465863 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 123 - loss 0.46449 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 124 - loss 0.463131 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 125 - loss 0.461786 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 126 - loss 0.460455 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 127 - loss 0.459137 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 128 - loss 0.457832 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 129 - loss 0.456539 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 130 - loss 0.45526 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 131 - loss 0.453993 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 132 - loss 0.452739 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 133 - loss 0.451496 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 134 - loss 0.450266 - accuracy 0.839286\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 135 - loss 0.449047 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 136 - loss 0.44784 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 137 - loss 0.446644 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 138 - loss 0.445459 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 139 - loss 0.444286 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 140 - loss 0.443123 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 141 - loss 0.441971 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 142 - loss 0.44083 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 143 - loss 0.439699 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 144 - loss 0.438579 - accuracy 0.848214\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 145 - loss 0.437468 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 146 - loss 0.436368 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 147 - loss 0.435277 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 148 - loss 0.434197 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 149 - loss 0.433126 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 150 - loss 0.432064 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 151 - loss 0.431011 - accuracy 0.857143\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 152 - loss 0.429968 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 153 - loss 0.428933 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 154 - loss 0.427908 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 155 - loss 0.426891 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 156 - loss 0.425883 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 157 - loss 0.424883 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 158 - loss 0.423892 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 159 - loss 0.422909 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 160 - loss 0.421934 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 161 - loss 0.420967 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 162 - loss 0.420009 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 163 - loss 0.419059 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 164 - loss 0.418116 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 165 - loss 0.417181 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 166 - loss 0.416253 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 167 - loss 0.415333 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 168 - loss 0.41442 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 169 - loss 0.413515 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 170 - loss 0.412617 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 171 - loss 0.411726 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 172 - loss 0.410842 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 173 - loss 0.409966 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 174 - loss 0.409097 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 175 - loss 0.408234 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 176 - loss 0.407378 - accuracy 0.866071\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 177 - loss 0.406528 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 178 - loss 0.405685 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 179 - loss 0.404848 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 180 - loss 0.404018 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 181 - loss 0.403194 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 182 - loss 0.402375 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 183 - loss 0.401564 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 184 - loss 0.400758 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 185 - loss 0.399958 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 186 - loss 0.399164 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 187 - loss 0.398375 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 188 - loss 0.397593 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 189 - loss 0.396816 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 190 - loss 0.396045 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 191 - loss 0.39528 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 192 - loss 0.39452 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 193 - loss 0.393765 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 194 - loss 0.393016 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 195 - loss 0.392272 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 196 - loss 0.391533 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 197 - loss 0.390799 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 198 - loss 0.390071 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 199 - loss 0.389348 - accuracy 0.883929\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 200 - loss 0.388629 - accuracy 0.883929\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 201 - loss 0.387916 - accuracy 0.883929\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 202 - loss 0.387207 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 203 - loss 0.386503 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 204 - loss 0.385804 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 205 - loss 0.38511 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 206 - loss 0.38442 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 207 - loss 0.383735 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 208 - loss 0.383054 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 209 - loss 0.382378 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 210 - loss 0.381706 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 211 - loss 0.381039 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 212 - loss 0.380376 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 213 - loss 0.379718 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 214 - loss 0.379063 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 215 - loss 0.378414 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 216 - loss 0.377768 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 217 - loss 0.377126 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 218 - loss 0.376489 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 219 - loss 0.375855 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 220 - loss 0.375225 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 221 - loss 0.3746 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 222 - loss 0.373978 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 223 - loss 0.37336 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 224 - loss 0.372745 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 225 - loss 0.372135 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 226 - loss 0.371528 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 227 - loss 0.370925 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 228 - loss 0.370325 - accuracy 0.875\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 229 - loss 0.369729 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 230 - loss 0.369137 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 231 - loss 0.368548 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 232 - loss 0.367963 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 233 - loss 0.367381 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 234 - loss 0.366803 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 235 - loss 0.366229 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 236 - loss 0.365657 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 237 - loss 0.36509 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 238 - loss 0.364525 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 239 - loss 0.363964 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 240 - loss 0.363405 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 241 - loss 0.362851 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 242 - loss 0.362299 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 243 - loss 0.36175 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 244 - loss 0.361205 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 245 - loss 0.360662 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 246 - loss 0.360123 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 247 - loss 0.359587 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 248 - loss 0.359053 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 249 - loss 0.358523 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 250 - loss 0.357995 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 251 - loss 0.35747 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 252 - loss 0.356948 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 253 - loss 0.356429 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 254 - loss 0.355913 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 255 - loss 0.355399 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 256 - loss 0.354889 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 257 - loss 0.35438 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 258 - loss 0.353875 - accuracy 0.892857\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 259 - loss 0.353372 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 260 - loss 0.352872 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 261 - loss 0.352375 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 262 - loss 0.35188 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 263 - loss 0.351387 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 264 - loss 0.350898 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 265 - loss 0.35041 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 266 - loss 0.349926 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 267 - loss 0.349443 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 268 - loss 0.348964 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 269 - loss 0.348486 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 270 - loss 0.348011 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 271 - loss 0.347539 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 272 - loss 0.347068 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 273 - loss 0.346601 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 274 - loss 0.346135 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 275 - loss 0.345672 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 276 - loss 0.345211 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 277 - loss 0.344752 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 278 - loss 0.344296 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 279 - loss 0.343841 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 280 - loss 0.343389 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 281 - loss 0.342939 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 282 - loss 0.342491 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 283 - loss 0.342045 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 284 - loss 0.341601 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 285 - loss 0.341159 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 286 - loss 0.34072 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 287 - loss 0.340282 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 288 - loss 0.339846 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 289 - loss 0.339413 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 290 - loss 0.338981 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 291 - loss 0.338552 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 292 - loss 0.338125 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 293 - loss 0.3377 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 294 - loss 0.337276 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 295 - loss 0.336855 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 296 - loss 0.336436 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 297 - loss 0.336018 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 298 - loss 0.335602 - accuracy 0.901786\n","### backward computation ...\n","### forward computation ...\n","get_classes\n","epoch 299 - loss 0.335188 - accuracy 0.901786\n","### backward computation ...\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ac-_IZA4d5dG","executionInfo":{"status":"ok","timestamp":1633341089272,"user_tz":-540,"elapsed":229,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"0e1d286e-0e60-4623-eac8-767a9abcf112"},"source":["f = open('./cse1.out', 'r')\n","\n","loss = []\n","acc=[]\n","\n","datalist = f.readlines()\n","for data in datalist:\n","  #print(data)\n","  ds = data.split(',')\n","  loss.append( float(ds[0]) )\n","  acc.append( float(ds[1]) )\n","\n","f.close()\n","\n","print(\"epoch =\", len(loss))\n","print( loss )"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch = 300\n","[1.153184, 1.129989, 1.107678, 1.086227, 1.065614, 1.045813, 1.026801, 1.008551, 0.991038, 0.974234, 0.958115, 0.942649, 0.927813, 0.913579, 0.899921, 0.886814, 0.874234, 0.862156, 0.850557, 0.839412, 0.8287, 0.8184, 0.808493, 0.798958, 0.789777, 0.780932, 0.772408, 0.764188, 0.756256, 0.748598, 0.741201, 0.734051, 0.727136, 0.720446, 0.713968, 0.707693, 0.701611, 0.695713, 0.68999, 0.684434, 0.679037, 0.673792, 0.668692, 0.663731, 0.658902, 0.654201, 0.64962, 0.645155, 0.640802, 0.636555, 0.63241, 0.628362, 0.624409, 0.620545, 0.616769, 0.613075, 0.609461, 0.605924, 0.602461, 0.599069, 0.595746, 0.592489, 0.589296, 0.586165, 0.583093, 0.580079, 0.577121, 0.574216, 0.571363, 0.568561, 0.565807, 0.563101, 0.56044, 0.557824, 0.555251, 0.55272, 0.550229, 0.547778, 0.545364, 0.542988, 0.540649, 0.538345, 0.536075, 0.533838, 0.531634, 0.529461, 0.52732, 0.525208, 0.523126, 0.521073, 0.519048, 0.517049, 0.515078, 0.513132, 0.511212, 0.509317, 0.507445, 0.505598, 0.503774, 0.501972, 0.500193, 0.498436, 0.4967, 0.494985, 0.49329, 0.491615, 0.48996, 0.488325, 0.486708, 0.485109, 0.483529, 0.481967, 0.480421, 0.478894, 0.477382, 0.475888, 0.47441, 0.472947, 0.4715, 0.470068, 0.468652, 0.46725, 0.465863, 0.46449, 0.463131, 0.461786, 0.460455, 0.459137, 0.457832, 0.456539, 0.45526, 0.453993, 0.452739, 0.451496, 0.450266, 0.449047, 0.44784, 0.446644, 0.445459, 0.444286, 0.443123, 0.441971, 0.44083, 0.439699, 0.438579, 0.437468, 0.436368, 0.435277, 0.434197, 0.433126, 0.432064, 0.431011, 0.429968, 0.428933, 0.427908, 0.426891, 0.425883, 0.424883, 0.423892, 0.422909, 0.421934, 0.420967, 0.420009, 0.419059, 0.418116, 0.417181, 0.416253, 0.415333, 0.41442, 0.413515, 0.412617, 0.411726, 0.410842, 0.409966, 0.409097, 0.408234, 0.407378, 0.406528, 0.405685, 0.404848, 0.404018, 0.403194, 0.402375, 0.401564, 0.400758, 0.399958, 0.399164, 0.398375, 0.397593, 0.396816, 0.396045, 0.39528, 0.39452, 0.393765, 0.393016, 0.392272, 0.391533, 0.390799, 0.390071, 0.389348, 0.388629, 0.387916, 0.387207, 0.386503, 0.385804, 0.38511, 0.38442, 0.383735, 0.383054, 0.382378, 0.381706, 0.381039, 0.380376, 0.379718, 0.379063, 0.378414, 0.377768, 0.377126, 0.376489, 0.375855, 0.375225, 0.3746, 0.373978, 0.37336, 0.372745, 0.372135, 0.371528, 0.370925, 0.370325, 0.369729, 0.369137, 0.368548, 0.367963, 0.367381, 0.366803, 0.366229, 0.365657, 0.36509, 0.364525, 0.363964, 0.363405, 0.362851, 0.362299, 0.36175, 0.361205, 0.360662, 0.360123, 0.359587, 0.359053, 0.358523, 0.357995, 0.35747, 0.356948, 0.356429, 0.355913, 0.355399, 0.354889, 0.35438, 0.353875, 0.353372, 0.352872, 0.352375, 0.35188, 0.351387, 0.350898, 0.35041, 0.349926, 0.349443, 0.348964, 0.348486, 0.348011, 0.347539, 0.347068, 0.346601, 0.346135, 0.345672, 0.345211, 0.344752, 0.344296, 0.343841, 0.343389, 0.342939, 0.342491, 0.342045, 0.341601, 0.341159, 0.34072, 0.340282, 0.339846, 0.339413, 0.338981, 0.338552, 0.338125, 0.3377, 0.337276, 0.336855, 0.336436, 0.336018, 0.335602, 0.335188]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"xMQUBvFqfMWg","executionInfo":{"status":"ok","timestamp":1633341092040,"user_tz":-540,"elapsed":538,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"723350b4-458d-49fc-a804-7fc66f8c3420"},"source":["# 交差エントロピー誤差\n","import matplotlib.pyplot as plt\n","\n","fig = plt.figure()\n","ax = fig.add_subplot()\n","ax.plot(list(range(len(loss))), loss)\n","ax.set_xlabel('epoch')\n","ax.set_ylabel('loss')\n","fig.show()"],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"iLif0DtvuYG5","executionInfo":{"status":"ok","timestamp":1633341095642,"user_tz":-540,"elapsed":259,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"b0a5631a-3419-4376-ed70-7f8f3a5e66a9"},"source":["# 正解率\n","fig = plt.figure()\n","ax = fig.add_subplot()\n","ax.plot(list(range(len(acc))), acc)\n","ax.set_xlabel('epoch')\n","ax.set_ylabel('accuracy')\n","fig.show()"],"execution_count":23,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"OSkuEd7zfizf"},"source":[""],"execution_count":null,"outputs":[]}]}
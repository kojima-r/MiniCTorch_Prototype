{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20210824_minictorch_vae1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMONbnBvdbXRUg25iHLJQ8l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Sml8FMQqbgEj"},"source":["VAE　オートエンコーダー\n","\n","参考文献　我妻幸長　「はじめてのディープラーニング２」"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ULQtL8IeXTg2","executionInfo":{"status":"ok","timestamp":1629781883117,"user_tz":-540,"elapsed":17452,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"fbe25183-f556-4e7b-af25-3967b384a474"},"source":["#　colaboraory用: Google drive をマウントする\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NujIexoFXUmL","executionInfo":{"status":"ok","timestamp":1629781885998,"user_tz":-540,"elapsed":220,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"636fd586-572f-4533-85a6-352fb86ba14e"},"source":["# colaboratory用: フォルダを移る\n","%cd \"drive/My Drive/Colab Notebooks\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VeO__E-ukSYH"},"source":["フォルダは自分の指定のものに変更してね。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mNZTsylAkV9E","executionInfo":{"status":"ok","timestamp":1629781889670,"user_tz":-540,"elapsed":360,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"532654be-4ff9-4a1b-c4e1-df583b230ebf"},"source":["%cd \"ctorch210824/MiniCTorch_Prototype\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/ctorch210824/MiniCTorch_Prototype\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPPGcVEQ7fwE","executionInfo":{"status":"ok","timestamp":1629781899884,"user_tz":-540,"elapsed":4869,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"b399b25f-f029-411c-cba1-6cc98c1c3c57"},"source":["! pip install lark-parser"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting lark-parser\n","  Downloading lark-parser-0.11.3.tar.gz (229 kB)\n","\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 30 kB 37.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 40 kB 40.1 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 51 kB 40.6 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 61 kB 37.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 71 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 81 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 92 kB 29.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 102 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 112 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 122 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 133 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 143 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 153 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 163 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 174 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 184 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 194 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 204 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 215 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 225 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 229 kB 31.3 MB/s \n","\u001b[?25hBuilding wheels for collected packages: lark-parser\n","  Building wheel for lark-parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lark-parser: filename=lark_parser-0.11.3-py2.py3-none-any.whl size=99740 sha256=ad53399d2367cbf9b7f7ee3409e6c139b02c961a994fac6d57bc3b732858212c\n","  Stored in directory: /root/.cache/pip/wheels/2d/f0/65/7aea47a49de7acac0108aac16a7ee00eb996f872d978feb87e\n","Successfully built lark-parser\n","Installing collected packages: lark-parser\n","Successfully installed lark-parser-0.11.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vuIJaurj7brd"},"source":["import json\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import minictorch\n","import minictorch.generator as GN\n","import minictorch.converter as CV"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h8wYhBe0h2W3"},"source":["サンプルデータ"]},{"cell_type":"code","metadata":{"id":"7s2PhMkdEiCR"},"source":["import torch.utils as utils\n","from torchvision import datasets, transforms\n","from sklearn import datasets\n","\n","img_size = 8\n","n_in = img_size * img_size\n","n_mid = 16\n","n_out = n_in\n","n_z = 2\n","n_batch = 32\n","\n","digits_data = datasets.load_digits()\n","dd = np.asarray( digits_data.data, dtype=np.float32 )\n","dd /= 15\n","x_train = torch.from_numpy( dd ).clone()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2zC4Vrdrh5eb"},"source":["サンプルデータの図化"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":78},"id":"O8saDqw0Pwnd","executionInfo":{"status":"ok","timestamp":1629782698956,"user_tz":-540,"elapsed":798,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"20b39d0c-9d8b-4ee1-8395-692a099b2bb3"},"source":["import matplotlib.pyplot as plt\n","n_img = 16\n","\n","plt.figuｒe( figsize=(16,3))\n","for i in range(n_img):\n","  ax = plt.subplot(3,n_img,i+1)\n","  plt.imshow(dd[i].reshape(img_size,-1).tolist(),cmap=\"Greys_r\")\n","  ax.get_xaxis().set_visible(False)\n","  ax.get_yaxis().set_visible(False)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1152x216 with 16 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"8Io-p4ogJysT"},"source":["ニューラルネットワークの定義"]},{"cell_type":"code","metadata":{"id":"FMLT7mbxauCN"},"source":["def mean_squared_error( x0, x1 ):\n","    diff = x0 - x1\n","    y = (diff**2).sum() / len(diff)\n","    return y;\n","\n","def reconstruction( y, t ):\n","  eps = 1.0e-7\n","  e = t * torch.log(y+eps) + (1-t) * torch.log(1-y+eps) \n","  z = e.sum() / len(y)\n","  return z;\n","\n","def kl_divergence( mean, log_var ):\n","  g = 1 + log_var - mean**2 - torch.exp( log_var )\n","  y = 0.5 * g.sum() / len(mean)\n","  return y\n","\n","import torch.distributions as tdist\n","\n","class Net(torch.nn.Module):\n","  def __init__( self, n_in, n_mid, n_out, n_z, e ):\n","    super(Net, self).__init__()\n","    self.fc1 = nn.Linear(n_in, n_mid)\n","    self.fc2_mean = nn.Linear(n_mid, n_z)\n","    self.fc2_var  = nn.Linear(n_mid, n_z)\n","    self.fc3 = nn.Linear(n_z  ,n_mid)\n","    self.fc4 = nn.Linear(n_mid,n_out)\n","    self.eps = e\n","  \n","  def forward( self, x ):\n","    # encoder\n","    x2 = F.relu( self.fc1(x) )\n","    self.mean    = self.fc2_mean(x2)\n","    self.log_var = self.fc2_var(x2)\n","\n","    # reparametrization\n","    #print(self.mean.shape)\n","    eps = torch.randn( self.mean.shape, device=\"cpu\" )\n","    #z = self.mean + self.eps * torch.exp( 0.5 * self.log_var )\n","    #z = self.mean + 0.5 * torch.exp( 0.5 * self.log_var )\n","\n","    #eps = torch.randn( n_batch, n_z )\n","    #eps = torch.randn( self.mean.shape )\n","    #print(\"mean\",self.mean)\n","    #print(\"mean\",self.mean.size())\n","    #nm = tdist.Normal( 0.0, 1.0 ) #torch.tensor([0.0]), torch.tensor([1.0]))\n","    #eps = nm.sample([ n_batch, n_z ])\n","    #eps = torch.normal(0.0,1.0,[32,2])\n","    #print(\"eps\",eps.size())\n","    #print(\"eps\",eps)\n","    z = self.mean + eps * torch.exp( 0.5 * self.log_var )\n","    #z = self.mean + self.eps * torch.exp( 0.5 * self.log_var )\n","\n","    # decoder\n","    y = F.relu( self.fc3(z) )\n","    y = torch.sigmoid( self.fc4(y) )\n","    ##return y\n","    \n","    e1 = reconstruction( y, x )\n","    e2 = kl_divergence( self.mean, self.log_var )\n","    \n","    \"\"\"\n","    print(\"x.shape\",x.shape)\n","    print(\"y.shape\",y.shape)\n","    print(\"mean.shape\",self.mean.shape)\n","    print(\"var.shape\",self.log_var.shape)\n","    print(\"x\",x)\n","    print(\"y\",y)\n","    print(\"mean\",self.mean)\n","    print(\"var\",self.log_var)\n","    print(\"e1\",e1)\n","    print(\"e2\",e2)\n","    \"\"\"\n","    return -(e1+e2)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OlwgzYrdF81U"},"source":["def generate_json( json_path, input, e ):\n","\n","    model = Net( n_in, n_mid, n_out, n_z, e )\n","\n","    model.eval()\n","    with torch.no_grad():\n","        print(\"[SAVE]\", json_path )\n","        GN.generate_minictorch_file( model, input, json_path )\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gavYhJ2Z6tft","executionInfo":{"status":"ok","timestamp":1629782717161,"user_tz":-540,"elapsed":225,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"46187e77-2ca9-4fcc-aae6-de8433c85e5d"},"source":["torch.manual_seed( 1 )\n","\n","project = 'vae1'\n","json_path = 'network/' + project +'.json'\n","\n","x = x_train.clone().detach()\n","torch.reshape( x, (-1,n_in) )\n","\n","x = x[0:n_batch,:]\n","#print(type(x))\n","#print(x.shape)\n","x.requires_grad = True\n","\n","e = torch.randn( n_batch, 2 )\n","#print(\"eps\",e)\n","\n","model = generate_json( json_path, x, e )\n","\n","with torch.set_grad_enabled(True):\n","  model.eval()\n","  output = model( x )\n","  print(\"output\",output)\n","  #output.sum().backward()\n","  output.backward()\n","  #print(\"fc1_w\",model.fc1.weight.grad)\n","  #print(\"fc1_b\",model.fc1.bias.grad)\n","  #print(\"fc1_rx\",model.fc1)\n","  #print(\"fc1_z\",model.rz.grad)\n","  \"\"\"\n","  print(\"fc2_mean_w\",model.fc2_mean.weight.grad)\n","  print(\"fc2_mean_b\",model.fc2_mean.bias.grad)\n","  print(\"fc2_var_w\",model.fc2_var.weight.grad)\n","  print(\"fc2_var_b\",model.fc2_var.bias.grad)\n","  \"\"\"\n","  #print(\"output\",output.grad)\n","  print(\"input_grad\",x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[SAVE] network/vae1.json\n","skip: Net/Linear[fc1]/weight/104\n","skip: Net/Linear[fc1]/weight/104\n","skip: Net/Linear[fc2_mean]/weight/107\n","skip: Net/Linear[fc2_mean]/weight/107\n","skip: Net/Linear[fc2_var]/weight/110\n","skip: Net/Linear[fc2_var]/weight/110\n","skip: Net/Linear[fc3]/weight/113\n","skip: Net/Linear[fc3]/weight/113\n","skip: Net/Linear[fc4]/weight/116\n","skip: Net/Linear[fc4]/weight/116\n","output tensor(45.1495, grad_fn=<NegBackward>)\n","input_grad tensor([[-0.0050,  0.0045, -0.0101,  ..., -0.0009, -0.0027,  0.0052],\n","        [-0.0097,  0.0096, -0.0162,  ..., -0.0040, -0.0035,  0.0061],\n","        [-0.0067,  0.0045, -0.0166,  ..., -0.0063, -0.0080,  0.0040],\n","        ...,\n","        [-0.0070,  0.0067, -0.0158,  ..., -0.0049, -0.0066,  0.0048],\n","        [-0.0086,  0.0176, -0.0294,  ..., -0.0088, -0.0133,  0.0054],\n","        [-0.0086,  0.0026, -0.0065,  ...,  0.0042, -0.0036,  0.0108]])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n","  if __name__ == '__main__':\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MN3gKf3D8iut","executionInfo":{"status":"ok","timestamp":1629782721226,"user_tz":-540,"elapsed":591,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"b05cce82-8f49-44d6-ea6a-e7ccfbc4cfd8"},"source":["\"\"\"\n","def convert_json( project, folder, model, input_x, json_path, rand_flag=0 ):\n","\n","    #folder = \"src\"\n","    cpp_fname   = project + \".cpp\"\n","    param_fname = project + \"_param.cpp\"\n","    cpp_path    = folder + \"/\" + cpp_fname\n","    param_path  = folder + \"/\" + param_fname\n","    make_path   = folder + \"/\" + \"Makefile\"\n","\n","    # load json file\n","    print( \"[JSON]\", json_path )\n","    fp = open( json_path )\n","    obj = json.load( fp )\n","\n","    # save parameter file\n","    code1 = CV.c_param_generator( obj, model, input_x )\n","    if len( code1 ) > 0:\n","       print( \"[PARAM]\", param_path )\n","       ofparam = open( param_path, \"w\" )\n","       ofparam.write( code1 )\n","\n","    # save cpp file\n","    print( \"[CPP]  \", cpp_path )\n","    code2 = CV.c_code_generator( obj, model, rand_flag )\n","\n","    #ofp=open(args.path+\"/\"+args.output,\"w\")\n","    ofp = open( cpp_path, \"w\" )\n","    ofp.write( code2 )\n","\n","    # save make file\n","    print( \"[MAKE] \", make_path )\n","    make_code = CV.makefile_generator( cpp_fname )\n","\n","    #makefp=open(args.path+\"/\"+\"Makefile\",\"w\")\n","    makefp = open( make_path, \"w\" )\n","    makefp.write( make_code )\n","\"\"\"\n","CV.convert_json( project, \"src\", model, x, json_path )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[JSON] network/vae1.json\n","{'name': 'Net/Linear[fc1]/weight/128', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [3], 'sorted_id': 1}\n","{'name': 'Net/Linear[fc1]/bias/127', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [3], 'sorted_id': 2}\n","{'name': 'Net/Linear[fc2_mean]/weight/131', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [7], 'sorted_id': 5}\n","{'name': 'Net/Linear[fc2_mean]/bias/130', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [7], 'sorted_id': 6}\n","{'name': 'Net/Linear[fc2_var]/weight/134', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [11], 'sorted_id': 9}\n","{'name': 'Net/Linear[fc2_var]/bias/133', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [11], 'sorted_id': 10}\n","{'name': 'Net/Linear[fc3]/weight/137', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [20], 'sorted_id': 18}\n","{'name': 'Net/Linear[fc3]/bias/136', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [20], 'sorted_id': 19}\n","{'name': 'Net/Linear[fc4]/weight/140', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [24], 'sorted_id': 22}\n","{'name': 'Net/Linear[fc4]/bias/139', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [24], 'sorted_id': 23}\n","[PARAM] src/vae1_param.cpp\n","[CPP]   src/vae1.cpp\n","{'name': 'input/x', 'op': 'IO Node', 'in': [], 'shape': [32, 64], 'out': [3, 30, 33], 'sorted_id': 0}\n","{'name': 'Net/Linear[fc1]/weight/128', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [3], 'sorted_id': 1}\n","Net/Linear[fc1]/weight/128  ->  fc1_weight\n","{'name': 'Net/Linear[fc1]/bias/127', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [3], 'sorted_id': 2}\n","Net/Linear[fc1]/bias/127  ->  fc1_bias\n","{'name': 'Net/Linear[fc1]/input.1', 'op': 'aten::linear', 'in': [0, 1, 2], 'shape': [32, 16], 'out': [4], 'sorted_id': 3}\n","{'name': 'Net/input.3', 'op': 'aten::relu', 'in': [3], 'shape': [32, 16], 'out': [7, 11], 'sorted_id': 4}\n","{'name': 'Net/Linear[fc2_mean]/weight/131', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [7], 'sorted_id': 5}\n","Net/Linear[fc2_mean]/weight/131  ->  fc2_mean_weight\n","{'name': 'Net/Linear[fc2_mean]/bias/130', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [7], 'sorted_id': 6}\n","Net/Linear[fc2_mean]/bias/130  ->  fc2_mean_bias\n","{'name': 'Net/Linear[fc2_mean]/mean', 'op': 'aten::linear', 'in': [4, 5, 6], 'shape': [32, 2], 'out': [17, 52], 'sorted_id': 7}\n","{'name': 'Net/36', 'op': 'prim::Constant', 'in': [], 'shape': [32, 2], 'constant_value': [-1.5256, -0.7502, -0.654, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091, -0.7121, 0.3037, -0.7773, -0.2515, -0.2223, 1.6871, 0.2284, 0.4676, -0.697, -1.1608, 0.6995, 0.1991, 0.8657, 0.2444, -0.6629, 0.8073, 1.1017, -0.1759, -2.2456, -1.4465, 0.0612, -0.6177, -0.7981, -0.1316, 1.8793, -0.0721, 0.1578, -0.7735, 0.1991, 0.0457, 0.153, -0.4757, -0.111, 0.2927, -0.1578, -0.0288, 2.3571, -1.0373, 1.5748, -0.6298, -0.9274, 0.5451, 0.0663, -0.437, 0.7626, 0.4415, 1.1651, 2.0154, 0.1374, 0.9386, -0.186, -0.6446, 1.5392, -0.8696, -3.3312, -0.7479], 'out': [15], 'sorted_id': 8}\n","{'name': 'Net/Linear[fc2_var]/weight/134', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [11], 'sorted_id': 9}\n","Net/Linear[fc2_var]/weight/134  ->  fc2_var_weight\n","{'name': 'Net/Linear[fc2_var]/bias/133', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [11], 'sorted_id': 10}\n","Net/Linear[fc2_var]/bias/133  ->  fc2_var_bias\n","{'name': 'Net/Linear[fc2_var]/log_var', 'op': 'aten::linear', 'in': [4, 9, 10], 'shape': [32, 2], 'out': [55, 13, 50], 'sorted_id': 11}\n","{'name': 'Net/33', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 0.5, 'out': [13], 'sorted_id': 12}\n","{'name': 'Net/34', 'op': 'aten::mul', 'in': [11, 12], 'shape': [32, 2], 'out': [14], 'sorted_id': 13}\n","{'name': 'Net/35', 'op': 'aten::exp', 'in': [13], 'shape': [32, 2], 'out': [15], 'sorted_id': 14}\n","{'name': 'Net/37', 'op': 'aten::mul', 'in': [8, 14], 'shape': [32, 2], 'out': [17], 'sorted_id': 15}\n","{'name': 'Net/38', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1.0, 'out': [17], 'sorted_id': 16}\n","{'name': 'Net/input.5', 'op': 'aten::add', 'in': [7, 15, 16], 'shape': [32, 2], 'out': [20], 'sorted_id': 17}\n","{'name': 'Net/Linear[fc3]/weight/137', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [20], 'sorted_id': 18}\n","Net/Linear[fc3]/weight/137  ->  fc3_weight\n","{'name': 'Net/Linear[fc3]/bias/136', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [20], 'sorted_id': 19}\n","Net/Linear[fc3]/bias/136  ->  fc3_bias\n","{'name': 'Net/Linear[fc3]/input.7', 'op': 'aten::linear', 'in': [17, 18, 19], 'shape': [32, 16], 'out': [21], 'sorted_id': 20}\n","{'name': 'Net/input', 'op': 'aten::relu', 'in': [20], 'shape': [32, 16], 'out': [24], 'sorted_id': 21}\n","{'name': 'Net/Linear[fc4]/weight/140', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [24], 'sorted_id': 22}\n","Net/Linear[fc4]/weight/140  ->  fc4_weight\n","{'name': 'Net/Linear[fc4]/bias/139', 'op': 'prim::GetAttr', 'in': [], 'shape': [], 'out': [24], 'sorted_id': 23}\n","Net/Linear[fc4]/bias/139  ->  fc4_bias\n","{'name': 'Net/Linear[fc4]/141', 'op': 'aten::linear', 'in': [21, 22, 23], 'shape': [32, 64], 'out': [25], 'sorted_id': 24}\n","{'name': 'Net/y', 'op': 'aten::sigmoid', 'in': [24], 'shape': [32, 64], 'out': [28, 36], 'sorted_id': 25}\n","{'name': 'Net/44', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1e-07, 'out': [28], 'sorted_id': 26}\n","{'name': 'Net/45', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1.0, 'out': [28], 'sorted_id': 27}\n","{'name': 'Net/46', 'op': 'aten::add', 'in': [25, 26, 27], 'shape': [32, 64], 'out': [29], 'sorted_id': 28}\n","{'name': 'Net/47', 'op': 'aten::log', 'in': [28], 'shape': [32, 64], 'out': [30], 'sorted_id': 29}\n","{'name': 'Net/48', 'op': 'aten::mul', 'in': [0, 29], 'shape': [32, 64], 'out': [43], 'sorted_id': 30}\n","{'name': 'Net/49', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1.0, 'out': [33], 'sorted_id': 31}\n","{'name': 'Net/50', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1.0, 'out': [33], 'sorted_id': 32}\n","{'name': 'Net/51', 'op': 'aten::rsub', 'in': [0, 31, 32], 'shape': [32, 64], 'out': [41], 'sorted_id': 33}\n","{'name': 'Net/52', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1.0, 'out': [36], 'sorted_id': 34}\n","{'name': 'Net/53', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1.0, 'out': [36], 'sorted_id': 35}\n","{'name': 'Net/54', 'op': 'aten::rsub', 'in': [25, 34, 35], 'shape': [32, 64], 'out': [39], 'sorted_id': 36}\n","{'name': 'Net/55', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1e-07, 'out': [39], 'sorted_id': 37}\n","{'name': 'Net/56', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1.0, 'out': [39], 'sorted_id': 38}\n","{'name': 'Net/57', 'op': 'aten::add', 'in': [36, 37, 38], 'shape': [32, 64], 'out': [40], 'sorted_id': 39}\n","{'name': 'Net/58', 'op': 'aten::log', 'in': [39], 'shape': [32, 64], 'out': [41], 'sorted_id': 40}\n","{'name': 'Net/59', 'op': 'aten::mul', 'in': [33, 40], 'shape': [32, 64], 'out': [43], 'sorted_id': 41}\n","{'name': 'Net/60', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1.0, 'out': [43], 'sorted_id': 42}\n","{'name': 'Net/e', 'op': 'aten::add', 'in': [30, 41, 42], 'shape': [32, 64], 'out': [45], 'sorted_id': 43}\n","{'name': 'Net/62', 'op': 'prim::Constant', 'in': [], 'shape': [], 'out': [45], 'sorted_id': 44}\n","{'name': 'Net/63', 'op': 'aten::sum', 'in': [43, 44], 'shape': [], 'out': [47], 'sorted_id': 45}\n","{'name': 'Net/70', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 32.0, 'out': [47], 'sorted_id': 46}\n","{'name': 'Net/e1', 'op': 'aten::div', 'in': [45, 46], 'shape': [], 'out': [65], 'sorted_id': 47}\n","{'name': 'Net/72', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1.0, 'out': [50], 'sorted_id': 48}\n","{'name': 'Net/73', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1.0, 'out': [50], 'sorted_id': 49}\n","{'name': 'Net/74', 'op': 'aten::add', 'in': [11, 48, 49], 'shape': [32, 2], 'out': [54], 'sorted_id': 50}\n","{'name': 'Net/75', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 2.0, 'out': [52], 'sorted_id': 51}\n","{'name': 'Net/76', 'op': 'aten::pow', 'in': [7, 51], 'shape': [32, 2], 'out': [54], 'sorted_id': 52}\n","{'name': 'Net/77', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1.0, 'out': [54], 'sorted_id': 53}\n","{'name': 'Net/78', 'op': 'aten::sub', 'in': [50, 52, 53], 'shape': [32, 2], 'out': [57], 'sorted_id': 54}\n","{'name': 'Net/79', 'op': 'aten::exp', 'in': [11], 'shape': [32, 2], 'out': [57], 'sorted_id': 55}\n","{'name': 'Net/80', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1.0, 'out': [57], 'sorted_id': 56}\n","{'name': 'Net/g', 'op': 'aten::sub', 'in': [54, 55, 56], 'shape': [32, 2], 'out': [59], 'sorted_id': 57}\n","{'name': 'Net/82', 'op': 'prim::Constant', 'in': [], 'shape': [], 'out': [59], 'sorted_id': 58}\n","{'name': 'Net/83', 'op': 'aten::sum', 'in': [57, 58], 'shape': [], 'out': [61], 'sorted_id': 59}\n","{'name': 'Net/84', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 0.5, 'out': [61], 'sorted_id': 60}\n","{'name': 'Net/85', 'op': 'aten::mul', 'in': [59, 60], 'shape': [], 'out': [63], 'sorted_id': 61}\n","{'name': 'Net/92', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 32.0, 'out': [63], 'sorted_id': 62}\n","{'name': 'Net/e2', 'op': 'aten::div', 'in': [61, 62], 'shape': [], 'out': [65], 'sorted_id': 63}\n","{'name': 'Net/94', 'op': 'prim::Constant', 'in': [], 'shape': [], 'constant_value': 1.0, 'out': [65], 'sorted_id': 64}\n","{'name': 'Net/95', 'op': 'aten::add', 'in': [47, 63, 64], 'shape': [], 'out': [66], 'sorted_id': 65}\n","{'name': 'Net/96', 'op': 'aten::neg', 'in': [65], 'shape': [], 'out': [67], 'sorted_id': 66}\n","{'name': 'output/output.1', 'op': 'IO Node', 'in': [66], 'shape': [], 'out': [], 'sorted_id': 67}\n","[MAKE]  src/Makefile\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zf1OiQzc9u5t"},"source":["!g++ -std=c++14 ./src/vae1.cpp ./src/vae1_param.cpp -I ../../ctorch/lib -lcblas -o ./bin/vae1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h6HzWOMxmb6O"},"source":["(注意) ctorch/libにxtensor関連のincludeを置いています。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pgRfSvqb_Uiv","executionInfo":{"status":"ok","timestamp":1629782814951,"user_tz":-540,"elapsed":559,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"c6a0bab1-cc1f-4580-f26b-9e516a88afb9"},"source":["!./bin/vae1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["### forward computation ...\n","{ 45.149506}\n","### backward computation ...\n","input_grad{{-0.005013,  0.004498, -0.010097, ..., -0.000856, -0.002696,  0.005182},\n"," {-0.00967 ,  0.009581, -0.016173, ..., -0.003957, -0.003504,  0.006057},\n"," {-0.006696,  0.004511, -0.016579, ..., -0.006288, -0.008011,  0.003987},\n"," ..., \n"," {-0.00702 ,  0.006713, -0.015817, ..., -0.004912, -0.006611,  0.004784},\n"," {-0.008582,  0.017581, -0.029435, ..., -0.008818, -0.013347,  0.005445},\n"," {-0.008566,  0.002597, -0.006499, ...,  0.004227, -0.003623,  0.010843}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I-reeMWjGTzG","executionInfo":{"status":"ok","timestamp":1629782867189,"user_tz":-540,"elapsed":3190,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"e9385e67-0c2c-4098-97c4-dd1b7a8e34c6"},"source":["torch.manual_seed( 1 )\n","\n","project = 'vae1'\n","\n","x = x_train.clone().detach()\n","torch.reshape( x, (-1,n_in) )\n","\n","x = x[0:n_batch,:]\n","#print(type(x))\n","#print(x.shape)\n","x.requires_grad = True\n","\n","e = torch.randn( n_batch, 2 )\n","#print(\"eps\",e)\n","\n","net = Net( n_in, n_mid, n_out, n_z, e )\n","net.train()\n","\n","lr=0.01\n","opt = torch.optim.SGD(net.parameters(), lr)\n","#mse = torch.nn.MSELoss()\n","\n","epoch_loss = []\n","epoch_num = 1000; #201\n","for epoch in range(epoch_num):\n","\n","  loss = net( x )\n","\n","  print(\"loss\",loss)\n","\n","  opt.zero_grad()\n","  loss.backward()\n","  #print(\"fc1_w\",model.fc1.weight.grad)\n","  #print(\"fc1_b\",model.fc1.bias.grad)\n","  #print(\"fc1_rx\",model.fc1)\n","  #print(\"fc1_z\",model.rz.grad)\n","  \"\"\"\n","  print(\"fc2_mean_w\",model.fc2_mean.weight.grad)\n","  print(\"fc2_mean_b\",model.fc2_mean.bias.grad)\n","  print(\"fc2_var_w\",model.fc2_var.weight.grad)\n","  print(\"fc2_var_b\",model.fc2_var.bias.grad)\n","  \"\"\"\n","  #print(\"output\",output.grad)\n","\n","  opt.step()\n","  print(\"EPOCH: {} loss: {}\".format(epoch, loss))\n","  epoch_loss.append( loss )\n"," "],"execution_count":null,"outputs":[{"output_type":"stream","text":["loss tensor(44.9811, grad_fn=<NegBackward>)\n","EPOCH: 0 loss: 44.98109817504883\n","loss tensor(45.1398, grad_fn=<NegBackward>)\n","EPOCH: 1 loss: 45.13983154296875\n","loss tensor(44.7923, grad_fn=<NegBackward>)\n","EPOCH: 2 loss: 44.792320251464844\n","loss tensor(44.3199, grad_fn=<NegBackward>)\n","EPOCH: 3 loss: 44.3199462890625\n","loss tensor(44.0797, grad_fn=<NegBackward>)\n","EPOCH: 4 loss: 44.07967758178711\n","loss tensor(43.8538, grad_fn=<NegBackward>)\n","EPOCH: 5 loss: 43.85380554199219\n","loss tensor(43.5098, grad_fn=<NegBackward>)\n","EPOCH: 6 loss: 43.50981903076172\n","loss tensor(43.4258, grad_fn=<NegBackward>)\n","EPOCH: 7 loss: 43.425846099853516\n","loss tensor(43.4149, grad_fn=<NegBackward>)\n","EPOCH: 8 loss: 43.414878845214844\n","loss tensor(42.9851, grad_fn=<NegBackward>)\n","EPOCH: 9 loss: 42.98505783081055\n","loss tensor(42.5460, grad_fn=<NegBackward>)\n","EPOCH: 10 loss: 42.546043395996094\n","loss tensor(42.4692, grad_fn=<NegBackward>)\n","EPOCH: 11 loss: 42.46922302246094\n","loss tensor(42.1893, grad_fn=<NegBackward>)\n","EPOCH: 12 loss: 42.189308166503906\n","loss tensor(42.1226, grad_fn=<NegBackward>)\n","EPOCH: 13 loss: 42.12259292602539\n","loss tensor(41.8024, grad_fn=<NegBackward>)\n","EPOCH: 14 loss: 41.802364349365234\n","loss tensor(41.8372, grad_fn=<NegBackward>)\n","EPOCH: 15 loss: 41.837188720703125\n","loss tensor(41.4763, grad_fn=<NegBackward>)\n","EPOCH: 16 loss: 41.476261138916016\n","loss tensor(41.3293, grad_fn=<NegBackward>)\n","EPOCH: 17 loss: 41.32929611206055\n","loss tensor(40.8828, grad_fn=<NegBackward>)\n","EPOCH: 18 loss: 40.882774353027344\n","loss tensor(40.1393, grad_fn=<NegBackward>)\n","EPOCH: 19 loss: 40.139339447021484\n","loss tensor(40.1155, grad_fn=<NegBackward>)\n","EPOCH: 20 loss: 40.1154899597168\n","loss tensor(40.0241, grad_fn=<NegBackward>)\n","EPOCH: 21 loss: 40.024131774902344\n","loss tensor(39.5340, grad_fn=<NegBackward>)\n","EPOCH: 22 loss: 39.533973693847656\n","loss tensor(38.9356, grad_fn=<NegBackward>)\n","EPOCH: 23 loss: 38.93558120727539\n","loss tensor(39.6953, grad_fn=<NegBackward>)\n","EPOCH: 24 loss: 39.69529342651367\n","loss tensor(38.6906, grad_fn=<NegBackward>)\n","EPOCH: 25 loss: 38.6905632019043\n","loss tensor(37.9954, grad_fn=<NegBackward>)\n","EPOCH: 26 loss: 37.99536895751953\n","loss tensor(38.2790, grad_fn=<NegBackward>)\n","EPOCH: 27 loss: 38.27900314331055\n","loss tensor(37.9808, grad_fn=<NegBackward>)\n","EPOCH: 28 loss: 37.98082733154297\n","loss tensor(37.6208, grad_fn=<NegBackward>)\n","EPOCH: 29 loss: 37.62083435058594\n","loss tensor(37.1247, grad_fn=<NegBackward>)\n","EPOCH: 30 loss: 37.12470626831055\n","loss tensor(36.2619, grad_fn=<NegBackward>)\n","EPOCH: 31 loss: 36.26194381713867\n","loss tensor(36.4324, grad_fn=<NegBackward>)\n","EPOCH: 32 loss: 36.432373046875\n","loss tensor(36.0710, grad_fn=<NegBackward>)\n","EPOCH: 33 loss: 36.07096862792969\n","loss tensor(35.1426, grad_fn=<NegBackward>)\n","EPOCH: 34 loss: 35.14258575439453\n","loss tensor(36.0126, grad_fn=<NegBackward>)\n","EPOCH: 35 loss: 36.0125732421875\n","loss tensor(36.2729, grad_fn=<NegBackward>)\n","EPOCH: 36 loss: 36.27289962768555\n","loss tensor(35.5156, grad_fn=<NegBackward>)\n","EPOCH: 37 loss: 35.51559829711914\n","loss tensor(34.6158, grad_fn=<NegBackward>)\n","EPOCH: 38 loss: 34.61576461791992\n","loss tensor(34.7551, grad_fn=<NegBackward>)\n","EPOCH: 39 loss: 34.75514602661133\n","loss tensor(34.1790, grad_fn=<NegBackward>)\n","EPOCH: 40 loss: 34.17898178100586\n","loss tensor(33.3211, grad_fn=<NegBackward>)\n","EPOCH: 41 loss: 33.32108688354492\n","loss tensor(33.3905, grad_fn=<NegBackward>)\n","EPOCH: 42 loss: 33.39046859741211\n","loss tensor(32.7601, grad_fn=<NegBackward>)\n","EPOCH: 43 loss: 32.760101318359375\n","loss tensor(32.7333, grad_fn=<NegBackward>)\n","EPOCH: 44 loss: 32.73328399658203\n","loss tensor(31.6114, grad_fn=<NegBackward>)\n","EPOCH: 45 loss: 31.61139488220215\n","loss tensor(32.6237, grad_fn=<NegBackward>)\n","EPOCH: 46 loss: 32.623741149902344\n","loss tensor(31.8205, grad_fn=<NegBackward>)\n","EPOCH: 47 loss: 31.820526123046875\n","loss tensor(32.6784, grad_fn=<NegBackward>)\n","EPOCH: 48 loss: 32.678436279296875\n","loss tensor(31.7135, grad_fn=<NegBackward>)\n","EPOCH: 49 loss: 31.71352767944336\n","loss tensor(31.7882, grad_fn=<NegBackward>)\n","EPOCH: 50 loss: 31.788238525390625\n","loss tensor(31.0653, grad_fn=<NegBackward>)\n","EPOCH: 51 loss: 31.065330505371094\n","loss tensor(30.8312, grad_fn=<NegBackward>)\n","EPOCH: 52 loss: 30.831151962280273\n","loss tensor(31.1743, grad_fn=<NegBackward>)\n","EPOCH: 53 loss: 31.174341201782227\n","loss tensor(30.9280, grad_fn=<NegBackward>)\n","EPOCH: 54 loss: 30.92799949645996\n","loss tensor(31.2195, grad_fn=<NegBackward>)\n","EPOCH: 55 loss: 31.219491958618164\n","loss tensor(31.0124, grad_fn=<NegBackward>)\n","EPOCH: 56 loss: 31.012428283691406\n","loss tensor(30.3382, grad_fn=<NegBackward>)\n","EPOCH: 57 loss: 30.338226318359375\n","loss tensor(31.0244, grad_fn=<NegBackward>)\n","EPOCH: 58 loss: 31.024391174316406\n","loss tensor(30.4491, grad_fn=<NegBackward>)\n","EPOCH: 59 loss: 30.449077606201172\n","loss tensor(30.8845, grad_fn=<NegBackward>)\n","EPOCH: 60 loss: 30.884490966796875\n","loss tensor(29.9467, grad_fn=<NegBackward>)\n","EPOCH: 61 loss: 29.946727752685547\n","loss tensor(30.2228, grad_fn=<NegBackward>)\n","EPOCH: 62 loss: 30.222814559936523\n","loss tensor(30.3236, grad_fn=<NegBackward>)\n","EPOCH: 63 loss: 30.323612213134766\n","loss tensor(29.9887, grad_fn=<NegBackward>)\n","EPOCH: 64 loss: 29.988723754882812\n","loss tensor(30.1980, grad_fn=<NegBackward>)\n","EPOCH: 65 loss: 30.197999954223633\n","loss tensor(30.6868, grad_fn=<NegBackward>)\n","EPOCH: 66 loss: 30.68682861328125\n","loss tensor(30.1957, grad_fn=<NegBackward>)\n","EPOCH: 67 loss: 30.195693969726562\n","loss tensor(29.4615, grad_fn=<NegBackward>)\n","EPOCH: 68 loss: 29.461490631103516\n","loss tensor(29.8723, grad_fn=<NegBackward>)\n","EPOCH: 69 loss: 29.87228012084961\n","loss tensor(29.5380, grad_fn=<NegBackward>)\n","EPOCH: 70 loss: 29.53795051574707\n","loss tensor(29.6772, grad_fn=<NegBackward>)\n","EPOCH: 71 loss: 29.677221298217773\n","loss tensor(29.6245, grad_fn=<NegBackward>)\n","EPOCH: 72 loss: 29.624526977539062\n","loss tensor(29.6351, grad_fn=<NegBackward>)\n","EPOCH: 73 loss: 29.635059356689453\n","loss tensor(29.7997, grad_fn=<NegBackward>)\n","EPOCH: 74 loss: 29.79972267150879\n","loss tensor(29.5257, grad_fn=<NegBackward>)\n","EPOCH: 75 loss: 29.525714874267578\n","loss tensor(29.3762, grad_fn=<NegBackward>)\n","EPOCH: 76 loss: 29.37615203857422\n","loss tensor(29.4099, grad_fn=<NegBackward>)\n","EPOCH: 77 loss: 29.409923553466797\n","loss tensor(29.3230, grad_fn=<NegBackward>)\n","EPOCH: 78 loss: 29.323049545288086\n","loss tensor(30.0699, grad_fn=<NegBackward>)\n","EPOCH: 79 loss: 30.069887161254883\n","loss tensor(29.4227, grad_fn=<NegBackward>)\n","EPOCH: 80 loss: 29.422718048095703\n","loss tensor(29.6098, grad_fn=<NegBackward>)\n","EPOCH: 81 loss: 29.60981559753418\n","loss tensor(29.3480, grad_fn=<NegBackward>)\n","EPOCH: 82 loss: 29.348031997680664\n","loss tensor(28.9120, grad_fn=<NegBackward>)\n","EPOCH: 83 loss: 28.911968231201172\n","loss tensor(29.8665, grad_fn=<NegBackward>)\n","EPOCH: 84 loss: 29.866525650024414\n","loss tensor(29.1657, grad_fn=<NegBackward>)\n","EPOCH: 85 loss: 29.16565704345703\n","loss tensor(29.1825, grad_fn=<NegBackward>)\n","EPOCH: 86 loss: 29.18248176574707\n","loss tensor(29.0397, grad_fn=<NegBackward>)\n","EPOCH: 87 loss: 29.039688110351562\n","loss tensor(28.9269, grad_fn=<NegBackward>)\n","EPOCH: 88 loss: 28.92693328857422\n","loss tensor(29.1663, grad_fn=<NegBackward>)\n","EPOCH: 89 loss: 29.166261672973633\n","loss tensor(29.1014, grad_fn=<NegBackward>)\n","EPOCH: 90 loss: 29.10140609741211\n","loss tensor(28.7934, grad_fn=<NegBackward>)\n","EPOCH: 91 loss: 28.793418884277344\n","loss tensor(28.7186, grad_fn=<NegBackward>)\n","EPOCH: 92 loss: 28.718595504760742\n","loss tensor(28.8961, grad_fn=<NegBackward>)\n","EPOCH: 93 loss: 28.896085739135742\n","loss tensor(28.8561, grad_fn=<NegBackward>)\n","EPOCH: 94 loss: 28.856098175048828\n","loss tensor(28.5748, grad_fn=<NegBackward>)\n","EPOCH: 95 loss: 28.574800491333008\n","loss tensor(28.6889, grad_fn=<NegBackward>)\n","EPOCH: 96 loss: 28.688936233520508\n","loss tensor(28.8731, grad_fn=<NegBackward>)\n","EPOCH: 97 loss: 28.87314224243164\n","loss tensor(28.9194, grad_fn=<NegBackward>)\n","EPOCH: 98 loss: 28.9194393157959\n","loss tensor(29.0069, grad_fn=<NegBackward>)\n","EPOCH: 99 loss: 29.006877899169922\n","loss tensor(28.9421, grad_fn=<NegBackward>)\n","EPOCH: 100 loss: 28.942106246948242\n","loss tensor(28.8605, grad_fn=<NegBackward>)\n","EPOCH: 101 loss: 28.860485076904297\n","loss tensor(28.6303, grad_fn=<NegBackward>)\n","EPOCH: 102 loss: 28.630294799804688\n","loss tensor(28.7301, grad_fn=<NegBackward>)\n","EPOCH: 103 loss: 28.730051040649414\n","loss tensor(28.3779, grad_fn=<NegBackward>)\n","EPOCH: 104 loss: 28.377859115600586\n","loss tensor(28.8668, grad_fn=<NegBackward>)\n","EPOCH: 105 loss: 28.866764068603516\n","loss tensor(28.4952, grad_fn=<NegBackward>)\n","EPOCH: 106 loss: 28.49523162841797\n","loss tensor(28.5084, grad_fn=<NegBackward>)\n","EPOCH: 107 loss: 28.50840187072754\n","loss tensor(28.5148, grad_fn=<NegBackward>)\n","EPOCH: 108 loss: 28.514780044555664\n","loss tensor(28.4043, grad_fn=<NegBackward>)\n","EPOCH: 109 loss: 28.40427589416504\n","loss tensor(28.3828, grad_fn=<NegBackward>)\n","EPOCH: 110 loss: 28.38283920288086\n","loss tensor(28.6825, grad_fn=<NegBackward>)\n","EPOCH: 111 loss: 28.682537078857422\n","loss tensor(28.7019, grad_fn=<NegBackward>)\n","EPOCH: 112 loss: 28.701929092407227\n","loss tensor(28.9771, grad_fn=<NegBackward>)\n","EPOCH: 113 loss: 28.977052688598633\n","loss tensor(28.3411, grad_fn=<NegBackward>)\n","EPOCH: 114 loss: 28.341075897216797\n","loss tensor(29.0932, grad_fn=<NegBackward>)\n","EPOCH: 115 loss: 29.09318733215332\n","loss tensor(28.7397, grad_fn=<NegBackward>)\n","EPOCH: 116 loss: 28.73972511291504\n","loss tensor(28.8146, grad_fn=<NegBackward>)\n","EPOCH: 117 loss: 28.81464195251465\n","loss tensor(28.5722, grad_fn=<NegBackward>)\n","EPOCH: 118 loss: 28.572181701660156\n","loss tensor(28.1497, grad_fn=<NegBackward>)\n","EPOCH: 119 loss: 28.149675369262695\n","loss tensor(28.0730, grad_fn=<NegBackward>)\n","EPOCH: 120 loss: 28.072999954223633\n","loss tensor(28.6098, grad_fn=<NegBackward>)\n","EPOCH: 121 loss: 28.609764099121094\n","loss tensor(28.3966, grad_fn=<NegBackward>)\n","EPOCH: 122 loss: 28.396629333496094\n","loss tensor(28.0836, grad_fn=<NegBackward>)\n","EPOCH: 123 loss: 28.083576202392578\n","loss tensor(28.0412, grad_fn=<NegBackward>)\n","EPOCH: 124 loss: 28.041175842285156\n","loss tensor(28.8509, grad_fn=<NegBackward>)\n","EPOCH: 125 loss: 28.85094451904297\n","loss tensor(28.4239, grad_fn=<NegBackward>)\n","EPOCH: 126 loss: 28.42392349243164\n","loss tensor(28.3683, grad_fn=<NegBackward>)\n","EPOCH: 127 loss: 28.3682861328125\n","loss tensor(28.3197, grad_fn=<NegBackward>)\n","EPOCH: 128 loss: 28.319704055786133\n","loss tensor(28.1651, grad_fn=<NegBackward>)\n","EPOCH: 129 loss: 28.165132522583008\n","loss tensor(28.1100, grad_fn=<NegBackward>)\n","EPOCH: 130 loss: 28.109996795654297\n","loss tensor(28.9793, grad_fn=<NegBackward>)\n","EPOCH: 131 loss: 28.979263305664062\n","loss tensor(28.2809, grad_fn=<NegBackward>)\n","EPOCH: 132 loss: 28.280914306640625\n","loss tensor(28.0808, grad_fn=<NegBackward>)\n","EPOCH: 133 loss: 28.080821990966797\n","loss tensor(28.3228, grad_fn=<NegBackward>)\n","EPOCH: 134 loss: 28.32282066345215\n","loss tensor(28.0796, grad_fn=<NegBackward>)\n","EPOCH: 135 loss: 28.079565048217773\n","loss tensor(28.2561, grad_fn=<NegBackward>)\n","EPOCH: 136 loss: 28.256074905395508\n","loss tensor(27.9299, grad_fn=<NegBackward>)\n","EPOCH: 137 loss: 27.92994499206543\n","loss tensor(27.9389, grad_fn=<NegBackward>)\n","EPOCH: 138 loss: 27.938867568969727\n","loss tensor(28.5400, grad_fn=<NegBackward>)\n","EPOCH: 139 loss: 28.540029525756836\n","loss tensor(28.1313, grad_fn=<NegBackward>)\n","EPOCH: 140 loss: 28.13126564025879\n","loss tensor(28.3243, grad_fn=<NegBackward>)\n","EPOCH: 141 loss: 28.32430648803711\n","loss tensor(27.9213, grad_fn=<NegBackward>)\n","EPOCH: 142 loss: 27.921323776245117\n","loss tensor(28.0142, grad_fn=<NegBackward>)\n","EPOCH: 143 loss: 28.01424217224121\n","loss tensor(28.0920, grad_fn=<NegBackward>)\n","EPOCH: 144 loss: 28.09203338623047\n","loss tensor(28.2601, grad_fn=<NegBackward>)\n","EPOCH: 145 loss: 28.260086059570312\n","loss tensor(28.2027, grad_fn=<NegBackward>)\n","EPOCH: 146 loss: 28.202669143676758\n","loss tensor(28.1902, grad_fn=<NegBackward>)\n","EPOCH: 147 loss: 28.190248489379883\n","loss tensor(28.2120, grad_fn=<NegBackward>)\n","EPOCH: 148 loss: 28.2120361328125\n","loss tensor(28.1785, grad_fn=<NegBackward>)\n","EPOCH: 149 loss: 28.1784725189209\n","loss tensor(28.2937, grad_fn=<NegBackward>)\n","EPOCH: 150 loss: 28.293703079223633\n","loss tensor(28.2200, grad_fn=<NegBackward>)\n","EPOCH: 151 loss: 28.219968795776367\n","loss tensor(28.1047, grad_fn=<NegBackward>)\n","EPOCH: 152 loss: 28.104671478271484\n","loss tensor(28.0118, grad_fn=<NegBackward>)\n","EPOCH: 153 loss: 28.011810302734375\n","loss tensor(28.0707, grad_fn=<NegBackward>)\n","EPOCH: 154 loss: 28.07070541381836\n","loss tensor(28.1402, grad_fn=<NegBackward>)\n","EPOCH: 155 loss: 28.140153884887695\n","loss tensor(27.9300, grad_fn=<NegBackward>)\n","EPOCH: 156 loss: 27.930049896240234\n","loss tensor(28.3090, grad_fn=<NegBackward>)\n","EPOCH: 157 loss: 28.30900001525879\n","loss tensor(28.3586, grad_fn=<NegBackward>)\n","EPOCH: 158 loss: 28.358570098876953\n","loss tensor(28.3090, grad_fn=<NegBackward>)\n","EPOCH: 159 loss: 28.309011459350586\n","loss tensor(27.9905, grad_fn=<NegBackward>)\n","EPOCH: 160 loss: 27.99051284790039\n","loss tensor(28.0496, grad_fn=<NegBackward>)\n","EPOCH: 161 loss: 28.049640655517578\n","loss tensor(27.9310, grad_fn=<NegBackward>)\n","EPOCH: 162 loss: 27.9310245513916\n","loss tensor(27.7219, grad_fn=<NegBackward>)\n","EPOCH: 163 loss: 27.721914291381836\n","loss tensor(28.5244, grad_fn=<NegBackward>)\n","EPOCH: 164 loss: 28.524391174316406\n","loss tensor(28.1838, grad_fn=<NegBackward>)\n","EPOCH: 165 loss: 28.183807373046875\n","loss tensor(27.9323, grad_fn=<NegBackward>)\n","EPOCH: 166 loss: 27.932329177856445\n","loss tensor(27.9433, grad_fn=<NegBackward>)\n","EPOCH: 167 loss: 27.94333267211914\n","loss tensor(28.0256, grad_fn=<NegBackward>)\n","EPOCH: 168 loss: 28.025623321533203\n","loss tensor(28.1538, grad_fn=<NegBackward>)\n","EPOCH: 169 loss: 28.153818130493164\n","loss tensor(28.2687, grad_fn=<NegBackward>)\n","EPOCH: 170 loss: 28.268718719482422\n","loss tensor(28.0301, grad_fn=<NegBackward>)\n","EPOCH: 171 loss: 28.030107498168945\n","loss tensor(27.9960, grad_fn=<NegBackward>)\n","EPOCH: 172 loss: 27.996004104614258\n","loss tensor(28.1466, grad_fn=<NegBackward>)\n","EPOCH: 173 loss: 28.14658546447754\n","loss tensor(27.9109, grad_fn=<NegBackward>)\n","EPOCH: 174 loss: 27.910947799682617\n","loss tensor(27.7933, grad_fn=<NegBackward>)\n","EPOCH: 175 loss: 27.79327392578125\n","loss tensor(27.8858, grad_fn=<NegBackward>)\n","EPOCH: 176 loss: 27.885822296142578\n","loss tensor(27.8939, grad_fn=<NegBackward>)\n","EPOCH: 177 loss: 27.89392852783203\n","loss tensor(28.1395, grad_fn=<NegBackward>)\n","EPOCH: 178 loss: 28.13947296142578\n","loss tensor(28.0266, grad_fn=<NegBackward>)\n","EPOCH: 179 loss: 28.02660369873047\n","loss tensor(27.9786, grad_fn=<NegBackward>)\n","EPOCH: 180 loss: 27.978593826293945\n","loss tensor(27.8497, grad_fn=<NegBackward>)\n","EPOCH: 181 loss: 27.849651336669922\n","loss tensor(27.5722, grad_fn=<NegBackward>)\n","EPOCH: 182 loss: 27.572208404541016\n","loss tensor(27.8033, grad_fn=<NegBackward>)\n","EPOCH: 183 loss: 27.803321838378906\n","loss tensor(27.8959, grad_fn=<NegBackward>)\n","EPOCH: 184 loss: 27.89586067199707\n","loss tensor(27.8435, grad_fn=<NegBackward>)\n","EPOCH: 185 loss: 27.84348487854004\n","loss tensor(27.7443, grad_fn=<NegBackward>)\n","EPOCH: 186 loss: 27.744251251220703\n","loss tensor(27.7389, grad_fn=<NegBackward>)\n","EPOCH: 187 loss: 27.738901138305664\n","loss tensor(27.8891, grad_fn=<NegBackward>)\n","EPOCH: 188 loss: 27.88910675048828\n","loss tensor(27.6339, grad_fn=<NegBackward>)\n","EPOCH: 189 loss: 27.633874893188477\n","loss tensor(27.9094, grad_fn=<NegBackward>)\n","EPOCH: 190 loss: 27.909408569335938\n","loss tensor(27.7193, grad_fn=<NegBackward>)\n","EPOCH: 191 loss: 27.719303131103516\n","loss tensor(27.5814, grad_fn=<NegBackward>)\n","EPOCH: 192 loss: 27.58135223388672\n","loss tensor(27.9457, grad_fn=<NegBackward>)\n","EPOCH: 193 loss: 27.94571876525879\n","loss tensor(27.6597, grad_fn=<NegBackward>)\n","EPOCH: 194 loss: 27.659746170043945\n","loss tensor(27.9457, grad_fn=<NegBackward>)\n","EPOCH: 195 loss: 27.94566535949707\n","loss tensor(27.6100, grad_fn=<NegBackward>)\n","EPOCH: 196 loss: 27.60997772216797\n","loss tensor(28.2909, grad_fn=<NegBackward>)\n","EPOCH: 197 loss: 28.290878295898438\n","loss tensor(28.0862, grad_fn=<NegBackward>)\n","EPOCH: 198 loss: 28.08623695373535\n","loss tensor(27.5718, grad_fn=<NegBackward>)\n","EPOCH: 199 loss: 27.57181167602539\n","loss tensor(27.7110, grad_fn=<NegBackward>)\n","EPOCH: 200 loss: 27.711013793945312\n","loss tensor(27.7764, grad_fn=<NegBackward>)\n","EPOCH: 201 loss: 27.776365280151367\n","loss tensor(27.8087, grad_fn=<NegBackward>)\n","EPOCH: 202 loss: 27.808734893798828\n","loss tensor(27.6899, grad_fn=<NegBackward>)\n","EPOCH: 203 loss: 27.68985939025879\n","loss tensor(27.8410, grad_fn=<NegBackward>)\n","EPOCH: 204 loss: 27.841026306152344\n","loss tensor(27.7147, grad_fn=<NegBackward>)\n","EPOCH: 205 loss: 27.71466064453125\n","loss tensor(27.7227, grad_fn=<NegBackward>)\n","EPOCH: 206 loss: 27.722715377807617\n","loss tensor(27.7886, grad_fn=<NegBackward>)\n","EPOCH: 207 loss: 27.788572311401367\n","loss tensor(27.7566, grad_fn=<NegBackward>)\n","EPOCH: 208 loss: 27.756610870361328\n","loss tensor(27.9096, grad_fn=<NegBackward>)\n","EPOCH: 209 loss: 27.909555435180664\n","loss tensor(27.8975, grad_fn=<NegBackward>)\n","EPOCH: 210 loss: 27.897537231445312\n","loss tensor(27.9431, grad_fn=<NegBackward>)\n","EPOCH: 211 loss: 27.943140029907227\n","loss tensor(27.7214, grad_fn=<NegBackward>)\n","EPOCH: 212 loss: 27.721385955810547\n","loss tensor(27.5037, grad_fn=<NegBackward>)\n","EPOCH: 213 loss: 27.503711700439453\n","loss tensor(27.7099, grad_fn=<NegBackward>)\n","EPOCH: 214 loss: 27.709903717041016\n","loss tensor(27.8234, grad_fn=<NegBackward>)\n","EPOCH: 215 loss: 27.823408126831055\n","loss tensor(27.5385, grad_fn=<NegBackward>)\n","EPOCH: 216 loss: 27.538484573364258\n","loss tensor(27.4870, grad_fn=<NegBackward>)\n","EPOCH: 217 loss: 27.487003326416016\n","loss tensor(27.8760, grad_fn=<NegBackward>)\n","EPOCH: 218 loss: 27.876047134399414\n","loss tensor(27.3965, grad_fn=<NegBackward>)\n","EPOCH: 219 loss: 27.396488189697266\n","loss tensor(27.5430, grad_fn=<NegBackward>)\n","EPOCH: 220 loss: 27.543014526367188\n","loss tensor(27.5998, grad_fn=<NegBackward>)\n","EPOCH: 221 loss: 27.599834442138672\n","loss tensor(27.4318, grad_fn=<NegBackward>)\n","EPOCH: 222 loss: 27.431808471679688\n","loss tensor(27.4955, grad_fn=<NegBackward>)\n","EPOCH: 223 loss: 27.495498657226562\n","loss tensor(27.4060, grad_fn=<NegBackward>)\n","EPOCH: 224 loss: 27.406007766723633\n","loss tensor(27.4255, grad_fn=<NegBackward>)\n","EPOCH: 225 loss: 27.425512313842773\n","loss tensor(27.8256, grad_fn=<NegBackward>)\n","EPOCH: 226 loss: 27.825620651245117\n","loss tensor(27.6774, grad_fn=<NegBackward>)\n","EPOCH: 227 loss: 27.67739486694336\n","loss tensor(28.0668, grad_fn=<NegBackward>)\n","EPOCH: 228 loss: 28.066761016845703\n","loss tensor(27.6498, grad_fn=<NegBackward>)\n","EPOCH: 229 loss: 27.64976692199707\n","loss tensor(27.6753, grad_fn=<NegBackward>)\n","EPOCH: 230 loss: 27.675289154052734\n","loss tensor(27.9609, grad_fn=<NegBackward>)\n","EPOCH: 231 loss: 27.960948944091797\n","loss tensor(27.8186, grad_fn=<NegBackward>)\n","EPOCH: 232 loss: 27.818647384643555\n","loss tensor(27.4986, grad_fn=<NegBackward>)\n","EPOCH: 233 loss: 27.498634338378906\n","loss tensor(27.4972, grad_fn=<NegBackward>)\n","EPOCH: 234 loss: 27.497249603271484\n","loss tensor(27.6407, grad_fn=<NegBackward>)\n","EPOCH: 235 loss: 27.640676498413086\n","loss tensor(27.6591, grad_fn=<NegBackward>)\n","EPOCH: 236 loss: 27.65908432006836\n","loss tensor(27.4028, grad_fn=<NegBackward>)\n","EPOCH: 237 loss: 27.40281867980957\n","loss tensor(27.9374, grad_fn=<NegBackward>)\n","EPOCH: 238 loss: 27.937374114990234\n","loss tensor(27.4485, grad_fn=<NegBackward>)\n","EPOCH: 239 loss: 27.4484806060791\n","loss tensor(27.4066, grad_fn=<NegBackward>)\n","EPOCH: 240 loss: 27.406644821166992\n","loss tensor(27.9113, grad_fn=<NegBackward>)\n","EPOCH: 241 loss: 27.911273956298828\n","loss tensor(27.4590, grad_fn=<NegBackward>)\n","EPOCH: 242 loss: 27.45897674560547\n","loss tensor(27.8184, grad_fn=<NegBackward>)\n","EPOCH: 243 loss: 27.818405151367188\n","loss tensor(27.4874, grad_fn=<NegBackward>)\n","EPOCH: 244 loss: 27.487422943115234\n","loss tensor(27.2372, grad_fn=<NegBackward>)\n","EPOCH: 245 loss: 27.237224578857422\n","loss tensor(27.7190, grad_fn=<NegBackward>)\n","EPOCH: 246 loss: 27.719024658203125\n","loss tensor(27.9452, grad_fn=<NegBackward>)\n","EPOCH: 247 loss: 27.945232391357422\n","loss tensor(27.3580, grad_fn=<NegBackward>)\n","EPOCH: 248 loss: 27.357967376708984\n","loss tensor(27.8425, grad_fn=<NegBackward>)\n","EPOCH: 249 loss: 27.842546463012695\n","loss tensor(28.0203, grad_fn=<NegBackward>)\n","EPOCH: 250 loss: 28.020322799682617\n","loss tensor(27.5978, grad_fn=<NegBackward>)\n","EPOCH: 251 loss: 27.59781837463379\n","loss tensor(27.6204, grad_fn=<NegBackward>)\n","EPOCH: 252 loss: 27.620412826538086\n","loss tensor(27.4338, grad_fn=<NegBackward>)\n","EPOCH: 253 loss: 27.433813095092773\n","loss tensor(27.4196, grad_fn=<NegBackward>)\n","EPOCH: 254 loss: 27.419551849365234\n","loss tensor(27.4538, grad_fn=<NegBackward>)\n","EPOCH: 255 loss: 27.453838348388672\n","loss tensor(27.7999, grad_fn=<NegBackward>)\n","EPOCH: 256 loss: 27.79989242553711\n","loss tensor(27.4387, grad_fn=<NegBackward>)\n","EPOCH: 257 loss: 27.43866729736328\n","loss tensor(27.1475, grad_fn=<NegBackward>)\n","EPOCH: 258 loss: 27.14745330810547\n","loss tensor(27.3429, grad_fn=<NegBackward>)\n","EPOCH: 259 loss: 27.342866897583008\n","loss tensor(27.3414, grad_fn=<NegBackward>)\n","EPOCH: 260 loss: 27.34135627746582\n","loss tensor(27.2782, grad_fn=<NegBackward>)\n","EPOCH: 261 loss: 27.278167724609375\n","loss tensor(27.3673, grad_fn=<NegBackward>)\n","EPOCH: 262 loss: 27.36725616455078\n","loss tensor(27.2347, grad_fn=<NegBackward>)\n","EPOCH: 263 loss: 27.23473358154297\n","loss tensor(27.1913, grad_fn=<NegBackward>)\n","EPOCH: 264 loss: 27.191251754760742\n","loss tensor(27.2697, grad_fn=<NegBackward>)\n","EPOCH: 265 loss: 27.269716262817383\n","loss tensor(27.6145, grad_fn=<NegBackward>)\n","EPOCH: 266 loss: 27.614519119262695\n","loss tensor(27.4755, grad_fn=<NegBackward>)\n","EPOCH: 267 loss: 27.47551918029785\n","loss tensor(27.4259, grad_fn=<NegBackward>)\n","EPOCH: 268 loss: 27.42588233947754\n","loss tensor(27.2973, grad_fn=<NegBackward>)\n","EPOCH: 269 loss: 27.297300338745117\n","loss tensor(27.8367, grad_fn=<NegBackward>)\n","EPOCH: 270 loss: 27.836681365966797\n","loss tensor(27.5871, grad_fn=<NegBackward>)\n","EPOCH: 271 loss: 27.587135314941406\n","loss tensor(27.2486, grad_fn=<NegBackward>)\n","EPOCH: 272 loss: 27.248563766479492\n","loss tensor(27.6561, grad_fn=<NegBackward>)\n","EPOCH: 273 loss: 27.656137466430664\n","loss tensor(27.4778, grad_fn=<NegBackward>)\n","EPOCH: 274 loss: 27.47783851623535\n","loss tensor(27.5181, grad_fn=<NegBackward>)\n","EPOCH: 275 loss: 27.518102645874023\n","loss tensor(27.5087, grad_fn=<NegBackward>)\n","EPOCH: 276 loss: 27.508716583251953\n","loss tensor(27.2897, grad_fn=<NegBackward>)\n","EPOCH: 277 loss: 27.289737701416016\n","loss tensor(27.3756, grad_fn=<NegBackward>)\n","EPOCH: 278 loss: 27.375627517700195\n","loss tensor(27.4687, grad_fn=<NegBackward>)\n","EPOCH: 279 loss: 27.468725204467773\n","loss tensor(27.1954, grad_fn=<NegBackward>)\n","EPOCH: 280 loss: 27.195430755615234\n","loss tensor(27.6102, grad_fn=<NegBackward>)\n","EPOCH: 281 loss: 27.610231399536133\n","loss tensor(27.6845, grad_fn=<NegBackward>)\n","EPOCH: 282 loss: 27.68448829650879\n","loss tensor(27.1912, grad_fn=<NegBackward>)\n","EPOCH: 283 loss: 27.191165924072266\n","loss tensor(27.4336, grad_fn=<NegBackward>)\n","EPOCH: 284 loss: 27.4335880279541\n","loss tensor(27.3238, grad_fn=<NegBackward>)\n","EPOCH: 285 loss: 27.32379150390625\n","loss tensor(27.3591, grad_fn=<NegBackward>)\n","EPOCH: 286 loss: 27.359086990356445\n","loss tensor(27.5391, grad_fn=<NegBackward>)\n","EPOCH: 287 loss: 27.5390682220459\n","loss tensor(27.2178, grad_fn=<NegBackward>)\n","EPOCH: 288 loss: 27.217750549316406\n","loss tensor(27.0668, grad_fn=<NegBackward>)\n","EPOCH: 289 loss: 27.066789627075195\n","loss tensor(27.7684, grad_fn=<NegBackward>)\n","EPOCH: 290 loss: 27.768369674682617\n","loss tensor(27.2574, grad_fn=<NegBackward>)\n","EPOCH: 291 loss: 27.2574462890625\n","loss tensor(27.4701, grad_fn=<NegBackward>)\n","EPOCH: 292 loss: 27.470104217529297\n","loss tensor(27.3235, grad_fn=<NegBackward>)\n","EPOCH: 293 loss: 27.323495864868164\n","loss tensor(27.1083, grad_fn=<NegBackward>)\n","EPOCH: 294 loss: 27.10831642150879\n","loss tensor(27.1495, grad_fn=<NegBackward>)\n","EPOCH: 295 loss: 27.14948272705078\n","loss tensor(27.1756, grad_fn=<NegBackward>)\n","EPOCH: 296 loss: 27.175569534301758\n","loss tensor(27.1791, grad_fn=<NegBackward>)\n","EPOCH: 297 loss: 27.17909812927246\n","loss tensor(27.6681, grad_fn=<NegBackward>)\n","EPOCH: 298 loss: 27.668067932128906\n","loss tensor(27.2677, grad_fn=<NegBackward>)\n","EPOCH: 299 loss: 27.267745971679688\n","loss tensor(27.2752, grad_fn=<NegBackward>)\n","EPOCH: 300 loss: 27.27520179748535\n","loss tensor(27.1581, grad_fn=<NegBackward>)\n","EPOCH: 301 loss: 27.158071517944336\n","loss tensor(27.5501, grad_fn=<NegBackward>)\n","EPOCH: 302 loss: 27.550121307373047\n","loss tensor(27.2801, grad_fn=<NegBackward>)\n","EPOCH: 303 loss: 27.28014373779297\n","loss tensor(27.3605, grad_fn=<NegBackward>)\n","EPOCH: 304 loss: 27.360469818115234\n","loss tensor(27.1209, grad_fn=<NegBackward>)\n","EPOCH: 305 loss: 27.12092399597168\n","loss tensor(27.0672, grad_fn=<NegBackward>)\n","EPOCH: 306 loss: 27.067222595214844\n","loss tensor(27.4124, grad_fn=<NegBackward>)\n","EPOCH: 307 loss: 27.412425994873047\n","loss tensor(27.3064, grad_fn=<NegBackward>)\n","EPOCH: 308 loss: 27.306413650512695\n","loss tensor(27.0175, grad_fn=<NegBackward>)\n","EPOCH: 309 loss: 27.01754379272461\n","loss tensor(27.2268, grad_fn=<NegBackward>)\n","EPOCH: 310 loss: 27.226823806762695\n","loss tensor(27.5856, grad_fn=<NegBackward>)\n","EPOCH: 311 loss: 27.585634231567383\n","loss tensor(27.1897, grad_fn=<NegBackward>)\n","EPOCH: 312 loss: 27.18973731994629\n","loss tensor(27.3389, grad_fn=<NegBackward>)\n","EPOCH: 313 loss: 27.33887481689453\n","loss tensor(27.1770, grad_fn=<NegBackward>)\n","EPOCH: 314 loss: 27.177000045776367\n","loss tensor(27.3889, grad_fn=<NegBackward>)\n","EPOCH: 315 loss: 27.38885498046875\n","loss tensor(27.0731, grad_fn=<NegBackward>)\n","EPOCH: 316 loss: 27.073091506958008\n","loss tensor(27.0773, grad_fn=<NegBackward>)\n","EPOCH: 317 loss: 27.077260971069336\n","loss tensor(27.4719, grad_fn=<NegBackward>)\n","EPOCH: 318 loss: 27.471864700317383\n","loss tensor(27.3552, grad_fn=<NegBackward>)\n","EPOCH: 319 loss: 27.355247497558594\n","loss tensor(27.1976, grad_fn=<NegBackward>)\n","EPOCH: 320 loss: 27.1976261138916\n","loss tensor(27.7733, grad_fn=<NegBackward>)\n","EPOCH: 321 loss: 27.77334976196289\n","loss tensor(26.9770, grad_fn=<NegBackward>)\n","EPOCH: 322 loss: 26.976972579956055\n","loss tensor(27.7051, grad_fn=<NegBackward>)\n","EPOCH: 323 loss: 27.70513153076172\n","loss tensor(27.2133, grad_fn=<NegBackward>)\n","EPOCH: 324 loss: 27.213258743286133\n","loss tensor(27.0605, grad_fn=<NegBackward>)\n","EPOCH: 325 loss: 27.060504913330078\n","loss tensor(27.0453, grad_fn=<NegBackward>)\n","EPOCH: 326 loss: 27.04534339904785\n","loss tensor(27.2649, grad_fn=<NegBackward>)\n","EPOCH: 327 loss: 27.264896392822266\n","loss tensor(27.3760, grad_fn=<NegBackward>)\n","EPOCH: 328 loss: 27.376028060913086\n","loss tensor(27.2821, grad_fn=<NegBackward>)\n","EPOCH: 329 loss: 27.282123565673828\n","loss tensor(27.1521, grad_fn=<NegBackward>)\n","EPOCH: 330 loss: 27.15213966369629\n","loss tensor(27.3181, grad_fn=<NegBackward>)\n","EPOCH: 331 loss: 27.31812286376953\n","loss tensor(26.9772, grad_fn=<NegBackward>)\n","EPOCH: 332 loss: 26.9771728515625\n","loss tensor(27.0798, grad_fn=<NegBackward>)\n","EPOCH: 333 loss: 27.079832077026367\n","loss tensor(27.3686, grad_fn=<NegBackward>)\n","EPOCH: 334 loss: 27.368619918823242\n","loss tensor(27.2434, grad_fn=<NegBackward>)\n","EPOCH: 335 loss: 27.24335479736328\n","loss tensor(27.2396, grad_fn=<NegBackward>)\n","EPOCH: 336 loss: 27.239561080932617\n","loss tensor(27.2726, grad_fn=<NegBackward>)\n","EPOCH: 337 loss: 27.27264404296875\n","loss tensor(27.6167, grad_fn=<NegBackward>)\n","EPOCH: 338 loss: 27.616674423217773\n","loss tensor(27.2363, grad_fn=<NegBackward>)\n","EPOCH: 339 loss: 27.236303329467773\n","loss tensor(26.9783, grad_fn=<NegBackward>)\n","EPOCH: 340 loss: 26.978271484375\n","loss tensor(27.2521, grad_fn=<NegBackward>)\n","EPOCH: 341 loss: 27.252063751220703\n","loss tensor(27.2722, grad_fn=<NegBackward>)\n","EPOCH: 342 loss: 27.272214889526367\n","loss tensor(27.4003, grad_fn=<NegBackward>)\n","EPOCH: 343 loss: 27.400278091430664\n","loss tensor(27.0551, grad_fn=<NegBackward>)\n","EPOCH: 344 loss: 27.055089950561523\n","loss tensor(27.1116, grad_fn=<NegBackward>)\n","EPOCH: 345 loss: 27.111604690551758\n","loss tensor(27.3473, grad_fn=<NegBackward>)\n","EPOCH: 346 loss: 27.34727668762207\n","loss tensor(27.2791, grad_fn=<NegBackward>)\n","EPOCH: 347 loss: 27.27914810180664\n","loss tensor(26.9976, grad_fn=<NegBackward>)\n","EPOCH: 348 loss: 26.99760627746582\n","loss tensor(27.2709, grad_fn=<NegBackward>)\n","EPOCH: 349 loss: 27.270944595336914\n","loss tensor(27.2066, grad_fn=<NegBackward>)\n","EPOCH: 350 loss: 27.206584930419922\n","loss tensor(27.1374, grad_fn=<NegBackward>)\n","EPOCH: 351 loss: 27.137432098388672\n","loss tensor(27.6220, grad_fn=<NegBackward>)\n","EPOCH: 352 loss: 27.622011184692383\n","loss tensor(27.2306, grad_fn=<NegBackward>)\n","EPOCH: 353 loss: 27.230581283569336\n","loss tensor(27.2391, grad_fn=<NegBackward>)\n","EPOCH: 354 loss: 27.239065170288086\n","loss tensor(26.9314, grad_fn=<NegBackward>)\n","EPOCH: 355 loss: 26.931367874145508\n","loss tensor(26.9815, grad_fn=<NegBackward>)\n","EPOCH: 356 loss: 26.981489181518555\n","loss tensor(27.0877, grad_fn=<NegBackward>)\n","EPOCH: 357 loss: 27.087663650512695\n","loss tensor(27.2188, grad_fn=<NegBackward>)\n","EPOCH: 358 loss: 27.218793869018555\n","loss tensor(26.9901, grad_fn=<NegBackward>)\n","EPOCH: 359 loss: 26.990140914916992\n","loss tensor(27.1705, grad_fn=<NegBackward>)\n","EPOCH: 360 loss: 27.170507431030273\n","loss tensor(27.1685, grad_fn=<NegBackward>)\n","EPOCH: 361 loss: 27.16851043701172\n","loss tensor(27.1830, grad_fn=<NegBackward>)\n","EPOCH: 362 loss: 27.183002471923828\n","loss tensor(27.0129, grad_fn=<NegBackward>)\n","EPOCH: 363 loss: 27.012908935546875\n","loss tensor(27.1284, grad_fn=<NegBackward>)\n","EPOCH: 364 loss: 27.128376007080078\n","loss tensor(27.2749, grad_fn=<NegBackward>)\n","EPOCH: 365 loss: 27.274860382080078\n","loss tensor(27.1407, grad_fn=<NegBackward>)\n","EPOCH: 366 loss: 27.14072036743164\n","loss tensor(27.0381, grad_fn=<NegBackward>)\n","EPOCH: 367 loss: 27.038053512573242\n","loss tensor(27.4375, grad_fn=<NegBackward>)\n","EPOCH: 368 loss: 27.437543869018555\n","loss tensor(27.1376, grad_fn=<NegBackward>)\n","EPOCH: 369 loss: 27.137617111206055\n","loss tensor(26.9006, grad_fn=<NegBackward>)\n","EPOCH: 370 loss: 26.90058708190918\n","loss tensor(27.0580, grad_fn=<NegBackward>)\n","EPOCH: 371 loss: 27.058008193969727\n","loss tensor(26.9064, grad_fn=<NegBackward>)\n","EPOCH: 372 loss: 26.906402587890625\n","loss tensor(26.9038, grad_fn=<NegBackward>)\n","EPOCH: 373 loss: 26.903810501098633\n","loss tensor(27.5205, grad_fn=<NegBackward>)\n","EPOCH: 374 loss: 27.52045249938965\n","loss tensor(26.9042, grad_fn=<NegBackward>)\n","EPOCH: 375 loss: 26.90420913696289\n","loss tensor(27.2655, grad_fn=<NegBackward>)\n","EPOCH: 376 loss: 27.265460968017578\n","loss tensor(27.0925, grad_fn=<NegBackward>)\n","EPOCH: 377 loss: 27.092483520507812\n","loss tensor(27.3734, grad_fn=<NegBackward>)\n","EPOCH: 378 loss: 27.373395919799805\n","loss tensor(26.9136, grad_fn=<NegBackward>)\n","EPOCH: 379 loss: 26.913562774658203\n","loss tensor(27.1281, grad_fn=<NegBackward>)\n","EPOCH: 380 loss: 27.128067016601562\n","loss tensor(27.0533, grad_fn=<NegBackward>)\n","EPOCH: 381 loss: 27.053258895874023\n","loss tensor(26.9632, grad_fn=<NegBackward>)\n","EPOCH: 382 loss: 26.963228225708008\n","loss tensor(27.1880, grad_fn=<NegBackward>)\n","EPOCH: 383 loss: 27.18802833557129\n","loss tensor(26.9152, grad_fn=<NegBackward>)\n","EPOCH: 384 loss: 26.915231704711914\n","loss tensor(26.9914, grad_fn=<NegBackward>)\n","EPOCH: 385 loss: 26.991378784179688\n","loss tensor(27.1909, grad_fn=<NegBackward>)\n","EPOCH: 386 loss: 27.19093132019043\n","loss tensor(27.0917, grad_fn=<NegBackward>)\n","EPOCH: 387 loss: 27.091663360595703\n","loss tensor(26.8555, grad_fn=<NegBackward>)\n","EPOCH: 388 loss: 26.855514526367188\n","loss tensor(26.9290, grad_fn=<NegBackward>)\n","EPOCH: 389 loss: 26.92901039123535\n","loss tensor(27.0493, grad_fn=<NegBackward>)\n","EPOCH: 390 loss: 27.049314498901367\n","loss tensor(26.9460, grad_fn=<NegBackward>)\n","EPOCH: 391 loss: 26.945999145507812\n","loss tensor(26.8825, grad_fn=<NegBackward>)\n","EPOCH: 392 loss: 26.882511138916016\n","loss tensor(26.8619, grad_fn=<NegBackward>)\n","EPOCH: 393 loss: 26.861886978149414\n","loss tensor(26.9207, grad_fn=<NegBackward>)\n","EPOCH: 394 loss: 26.920732498168945\n","loss tensor(27.0554, grad_fn=<NegBackward>)\n","EPOCH: 395 loss: 27.055431365966797\n","loss tensor(26.8767, grad_fn=<NegBackward>)\n","EPOCH: 396 loss: 26.8767032623291\n","loss tensor(26.8303, grad_fn=<NegBackward>)\n","EPOCH: 397 loss: 26.830259323120117\n","loss tensor(27.2514, grad_fn=<NegBackward>)\n","EPOCH: 398 loss: 27.251379013061523\n","loss tensor(27.2242, grad_fn=<NegBackward>)\n","EPOCH: 399 loss: 27.22419548034668\n","loss tensor(26.7165, grad_fn=<NegBackward>)\n","EPOCH: 400 loss: 26.716459274291992\n","loss tensor(26.8846, grad_fn=<NegBackward>)\n","EPOCH: 401 loss: 26.88463020324707\n","loss tensor(26.9731, grad_fn=<NegBackward>)\n","EPOCH: 402 loss: 26.973119735717773\n","loss tensor(27.1323, grad_fn=<NegBackward>)\n","EPOCH: 403 loss: 27.132265090942383\n","loss tensor(26.8468, grad_fn=<NegBackward>)\n","EPOCH: 404 loss: 26.846790313720703\n","loss tensor(27.2005, grad_fn=<NegBackward>)\n","EPOCH: 405 loss: 27.200515747070312\n","loss tensor(27.0822, grad_fn=<NegBackward>)\n","EPOCH: 406 loss: 27.08222770690918\n","loss tensor(26.9312, grad_fn=<NegBackward>)\n","EPOCH: 407 loss: 26.931241989135742\n","loss tensor(26.9452, grad_fn=<NegBackward>)\n","EPOCH: 408 loss: 26.945240020751953\n","loss tensor(27.1041, grad_fn=<NegBackward>)\n","EPOCH: 409 loss: 27.1041316986084\n","loss tensor(27.2392, grad_fn=<NegBackward>)\n","EPOCH: 410 loss: 27.239179611206055\n","loss tensor(27.2529, grad_fn=<NegBackward>)\n","EPOCH: 411 loss: 27.252891540527344\n","loss tensor(26.7474, grad_fn=<NegBackward>)\n","EPOCH: 412 loss: 26.747392654418945\n","loss tensor(26.9234, grad_fn=<NegBackward>)\n","EPOCH: 413 loss: 26.92337989807129\n","loss tensor(26.9314, grad_fn=<NegBackward>)\n","EPOCH: 414 loss: 26.931381225585938\n","loss tensor(26.9754, grad_fn=<NegBackward>)\n","EPOCH: 415 loss: 26.975366592407227\n","loss tensor(26.7709, grad_fn=<NegBackward>)\n","EPOCH: 416 loss: 26.770906448364258\n","loss tensor(26.8296, grad_fn=<NegBackward>)\n","EPOCH: 417 loss: 26.829639434814453\n","loss tensor(27.1269, grad_fn=<NegBackward>)\n","EPOCH: 418 loss: 27.126863479614258\n","loss tensor(27.0200, grad_fn=<NegBackward>)\n","EPOCH: 419 loss: 27.020002365112305\n","loss tensor(26.9467, grad_fn=<NegBackward>)\n","EPOCH: 420 loss: 26.946712493896484\n","loss tensor(26.6493, grad_fn=<NegBackward>)\n","EPOCH: 421 loss: 26.649276733398438\n","loss tensor(26.8830, grad_fn=<NegBackward>)\n","EPOCH: 422 loss: 26.88304328918457\n","loss tensor(27.2447, grad_fn=<NegBackward>)\n","EPOCH: 423 loss: 27.24471092224121\n","loss tensor(26.9953, grad_fn=<NegBackward>)\n","EPOCH: 424 loss: 26.995319366455078\n","loss tensor(26.8809, grad_fn=<NegBackward>)\n","EPOCH: 425 loss: 26.880887985229492\n","loss tensor(26.8832, grad_fn=<NegBackward>)\n","EPOCH: 426 loss: 26.883180618286133\n","loss tensor(26.9448, grad_fn=<NegBackward>)\n","EPOCH: 427 loss: 26.944786071777344\n","loss tensor(26.9423, grad_fn=<NegBackward>)\n","EPOCH: 428 loss: 26.94234275817871\n","loss tensor(26.9221, grad_fn=<NegBackward>)\n","EPOCH: 429 loss: 26.92207145690918\n","loss tensor(27.2798, grad_fn=<NegBackward>)\n","EPOCH: 430 loss: 27.279817581176758\n","loss tensor(26.6106, grad_fn=<NegBackward>)\n","EPOCH: 431 loss: 26.61060333251953\n","loss tensor(26.8847, grad_fn=<NegBackward>)\n","EPOCH: 432 loss: 26.884723663330078\n","loss tensor(26.8405, grad_fn=<NegBackward>)\n","EPOCH: 433 loss: 26.840452194213867\n","loss tensor(26.8259, grad_fn=<NegBackward>)\n","EPOCH: 434 loss: 26.825885772705078\n","loss tensor(26.7858, grad_fn=<NegBackward>)\n","EPOCH: 435 loss: 26.785751342773438\n","loss tensor(26.7776, grad_fn=<NegBackward>)\n","EPOCH: 436 loss: 26.777591705322266\n","loss tensor(26.8675, grad_fn=<NegBackward>)\n","EPOCH: 437 loss: 26.86745834350586\n","loss tensor(26.9497, grad_fn=<NegBackward>)\n","EPOCH: 438 loss: 26.949689865112305\n","loss tensor(26.8669, grad_fn=<NegBackward>)\n","EPOCH: 439 loss: 26.866941452026367\n","loss tensor(26.7044, grad_fn=<NegBackward>)\n","EPOCH: 440 loss: 26.70436668395996\n","loss tensor(27.0553, grad_fn=<NegBackward>)\n","EPOCH: 441 loss: 27.055326461791992\n","loss tensor(26.5375, grad_fn=<NegBackward>)\n","EPOCH: 442 loss: 26.53751564025879\n","loss tensor(26.9961, grad_fn=<NegBackward>)\n","EPOCH: 443 loss: 26.99614143371582\n","loss tensor(27.0431, grad_fn=<NegBackward>)\n","EPOCH: 444 loss: 27.04306411743164\n","loss tensor(27.0423, grad_fn=<NegBackward>)\n","EPOCH: 445 loss: 27.042274475097656\n","loss tensor(26.6493, grad_fn=<NegBackward>)\n","EPOCH: 446 loss: 26.649276733398438\n","loss tensor(27.1537, grad_fn=<NegBackward>)\n","EPOCH: 447 loss: 27.15367317199707\n","loss tensor(27.0568, grad_fn=<NegBackward>)\n","EPOCH: 448 loss: 27.05683708190918\n","loss tensor(26.7170, grad_fn=<NegBackward>)\n","EPOCH: 449 loss: 26.717041015625\n","loss tensor(26.9065, grad_fn=<NegBackward>)\n","EPOCH: 450 loss: 26.906543731689453\n","loss tensor(26.7553, grad_fn=<NegBackward>)\n","EPOCH: 451 loss: 26.75533676147461\n","loss tensor(26.7559, grad_fn=<NegBackward>)\n","EPOCH: 452 loss: 26.755897521972656\n","loss tensor(26.9871, grad_fn=<NegBackward>)\n","EPOCH: 453 loss: 26.98705291748047\n","loss tensor(26.7865, grad_fn=<NegBackward>)\n","EPOCH: 454 loss: 26.78648567199707\n","loss tensor(26.6476, grad_fn=<NegBackward>)\n","EPOCH: 455 loss: 26.647611618041992\n","loss tensor(27.0428, grad_fn=<NegBackward>)\n","EPOCH: 456 loss: 27.042808532714844\n","loss tensor(26.9237, grad_fn=<NegBackward>)\n","EPOCH: 457 loss: 26.923717498779297\n","loss tensor(27.2681, grad_fn=<NegBackward>)\n","EPOCH: 458 loss: 27.268089294433594\n","loss tensor(26.5837, grad_fn=<NegBackward>)\n","EPOCH: 459 loss: 26.583683013916016\n","loss tensor(26.8233, grad_fn=<NegBackward>)\n","EPOCH: 460 loss: 26.82331085205078\n","loss tensor(26.6531, grad_fn=<NegBackward>)\n","EPOCH: 461 loss: 26.65310287475586\n","loss tensor(26.7468, grad_fn=<NegBackward>)\n","EPOCH: 462 loss: 26.746816635131836\n","loss tensor(26.7028, grad_fn=<NegBackward>)\n","EPOCH: 463 loss: 26.702831268310547\n","loss tensor(26.7496, grad_fn=<NegBackward>)\n","EPOCH: 464 loss: 26.749591827392578\n","loss tensor(26.6487, grad_fn=<NegBackward>)\n","EPOCH: 465 loss: 26.648651123046875\n","loss tensor(26.4576, grad_fn=<NegBackward>)\n","EPOCH: 466 loss: 26.4576416015625\n","loss tensor(26.7270, grad_fn=<NegBackward>)\n","EPOCH: 467 loss: 26.726951599121094\n","loss tensor(26.7081, grad_fn=<NegBackward>)\n","EPOCH: 468 loss: 26.708066940307617\n","loss tensor(26.7819, grad_fn=<NegBackward>)\n","EPOCH: 469 loss: 26.781936645507812\n","loss tensor(26.7157, grad_fn=<NegBackward>)\n","EPOCH: 470 loss: 26.715665817260742\n","loss tensor(26.7322, grad_fn=<NegBackward>)\n","EPOCH: 471 loss: 26.73224639892578\n","loss tensor(27.0590, grad_fn=<NegBackward>)\n","EPOCH: 472 loss: 27.05904769897461\n","loss tensor(26.6040, grad_fn=<NegBackward>)\n","EPOCH: 473 loss: 26.603952407836914\n","loss tensor(26.8843, grad_fn=<NegBackward>)\n","EPOCH: 474 loss: 26.884281158447266\n","loss tensor(27.0064, grad_fn=<NegBackward>)\n","EPOCH: 475 loss: 27.00640869140625\n","loss tensor(26.7123, grad_fn=<NegBackward>)\n","EPOCH: 476 loss: 26.71229362487793\n","loss tensor(26.5305, grad_fn=<NegBackward>)\n","EPOCH: 477 loss: 26.530467987060547\n","loss tensor(26.7445, grad_fn=<NegBackward>)\n","EPOCH: 478 loss: 26.744544982910156\n","loss tensor(26.7938, grad_fn=<NegBackward>)\n","EPOCH: 479 loss: 26.793825149536133\n","loss tensor(26.3603, grad_fn=<NegBackward>)\n","EPOCH: 480 loss: 26.36032485961914\n","loss tensor(26.5524, grad_fn=<NegBackward>)\n","EPOCH: 481 loss: 26.552364349365234\n","loss tensor(26.9047, grad_fn=<NegBackward>)\n","EPOCH: 482 loss: 26.904722213745117\n","loss tensor(26.7492, grad_fn=<NegBackward>)\n","EPOCH: 483 loss: 26.749191284179688\n","loss tensor(26.6323, grad_fn=<NegBackward>)\n","EPOCH: 484 loss: 26.63225746154785\n","loss tensor(26.5733, grad_fn=<NegBackward>)\n","EPOCH: 485 loss: 26.57328224182129\n","loss tensor(26.7297, grad_fn=<NegBackward>)\n","EPOCH: 486 loss: 26.72965431213379\n","loss tensor(26.7221, grad_fn=<NegBackward>)\n","EPOCH: 487 loss: 26.722082138061523\n","loss tensor(26.9414, grad_fn=<NegBackward>)\n","EPOCH: 488 loss: 26.941373825073242\n","loss tensor(26.8719, grad_fn=<NegBackward>)\n","EPOCH: 489 loss: 26.87190055847168\n","loss tensor(26.5423, grad_fn=<NegBackward>)\n","EPOCH: 490 loss: 26.542299270629883\n","loss tensor(26.9781, grad_fn=<NegBackward>)\n","EPOCH: 491 loss: 26.978111267089844\n","loss tensor(26.6254, grad_fn=<NegBackward>)\n","EPOCH: 492 loss: 26.625404357910156\n","loss tensor(26.5113, grad_fn=<NegBackward>)\n","EPOCH: 493 loss: 26.51127815246582\n","loss tensor(26.9636, grad_fn=<NegBackward>)\n","EPOCH: 494 loss: 26.96355438232422\n","loss tensor(26.8873, grad_fn=<NegBackward>)\n","EPOCH: 495 loss: 26.887277603149414\n","loss tensor(26.8489, grad_fn=<NegBackward>)\n","EPOCH: 496 loss: 26.84894371032715\n","loss tensor(26.4816, grad_fn=<NegBackward>)\n","EPOCH: 497 loss: 26.481586456298828\n","loss tensor(26.6834, grad_fn=<NegBackward>)\n","EPOCH: 498 loss: 26.68344497680664\n","loss tensor(26.6419, grad_fn=<NegBackward>)\n","EPOCH: 499 loss: 26.64191436767578\n","loss tensor(26.4681, grad_fn=<NegBackward>)\n","EPOCH: 500 loss: 26.468076705932617\n","loss tensor(26.4538, grad_fn=<NegBackward>)\n","EPOCH: 501 loss: 26.45376205444336\n","loss tensor(26.5968, grad_fn=<NegBackward>)\n","EPOCH: 502 loss: 26.596803665161133\n","loss tensor(26.9440, grad_fn=<NegBackward>)\n","EPOCH: 503 loss: 26.944007873535156\n","loss tensor(26.5737, grad_fn=<NegBackward>)\n","EPOCH: 504 loss: 26.573741912841797\n","loss tensor(26.6338, grad_fn=<NegBackward>)\n","EPOCH: 505 loss: 26.633785247802734\n","loss tensor(26.4223, grad_fn=<NegBackward>)\n","EPOCH: 506 loss: 26.422252655029297\n","loss tensor(26.5612, grad_fn=<NegBackward>)\n","EPOCH: 507 loss: 26.561159133911133\n","loss tensor(26.5020, grad_fn=<NegBackward>)\n","EPOCH: 508 loss: 26.501991271972656\n","loss tensor(26.6560, grad_fn=<NegBackward>)\n","EPOCH: 509 loss: 26.656023025512695\n","loss tensor(26.6193, grad_fn=<NegBackward>)\n","EPOCH: 510 loss: 26.619298934936523\n","loss tensor(26.4781, grad_fn=<NegBackward>)\n","EPOCH: 511 loss: 26.478132247924805\n","loss tensor(26.5942, grad_fn=<NegBackward>)\n","EPOCH: 512 loss: 26.594240188598633\n","loss tensor(26.6772, grad_fn=<NegBackward>)\n","EPOCH: 513 loss: 26.67717933654785\n","loss tensor(26.9020, grad_fn=<NegBackward>)\n","EPOCH: 514 loss: 26.902013778686523\n","loss tensor(26.3776, grad_fn=<NegBackward>)\n","EPOCH: 515 loss: 26.37761116027832\n","loss tensor(26.5894, grad_fn=<NegBackward>)\n","EPOCH: 516 loss: 26.589433670043945\n","loss tensor(26.5391, grad_fn=<NegBackward>)\n","EPOCH: 517 loss: 26.539133071899414\n","loss tensor(26.5292, grad_fn=<NegBackward>)\n","EPOCH: 518 loss: 26.529232025146484\n","loss tensor(26.4225, grad_fn=<NegBackward>)\n","EPOCH: 519 loss: 26.422462463378906\n","loss tensor(26.5058, grad_fn=<NegBackward>)\n","EPOCH: 520 loss: 26.505813598632812\n","loss tensor(26.4849, grad_fn=<NegBackward>)\n","EPOCH: 521 loss: 26.48494529724121\n","loss tensor(26.6784, grad_fn=<NegBackward>)\n","EPOCH: 522 loss: 26.678377151489258\n","loss tensor(26.5030, grad_fn=<NegBackward>)\n","EPOCH: 523 loss: 26.503009796142578\n","loss tensor(26.4155, grad_fn=<NegBackward>)\n","EPOCH: 524 loss: 26.415536880493164\n","loss tensor(26.5929, grad_fn=<NegBackward>)\n","EPOCH: 525 loss: 26.592931747436523\n","loss tensor(26.3537, grad_fn=<NegBackward>)\n","EPOCH: 526 loss: 26.35370445251465\n","loss tensor(26.8814, grad_fn=<NegBackward>)\n","EPOCH: 527 loss: 26.881399154663086\n","loss tensor(26.8328, grad_fn=<NegBackward>)\n","EPOCH: 528 loss: 26.8327579498291\n","loss tensor(26.3264, grad_fn=<NegBackward>)\n","EPOCH: 529 loss: 26.326425552368164\n","loss tensor(26.5424, grad_fn=<NegBackward>)\n","EPOCH: 530 loss: 26.54241371154785\n","loss tensor(26.6007, grad_fn=<NegBackward>)\n","EPOCH: 531 loss: 26.600666046142578\n","loss tensor(26.6161, grad_fn=<NegBackward>)\n","EPOCH: 532 loss: 26.61612892150879\n","loss tensor(26.5251, grad_fn=<NegBackward>)\n","EPOCH: 533 loss: 26.52510643005371\n","loss tensor(26.5031, grad_fn=<NegBackward>)\n","EPOCH: 534 loss: 26.503097534179688\n","loss tensor(26.5495, grad_fn=<NegBackward>)\n","EPOCH: 535 loss: 26.549480438232422\n","loss tensor(26.4852, grad_fn=<NegBackward>)\n","EPOCH: 536 loss: 26.485170364379883\n","loss tensor(26.5221, grad_fn=<NegBackward>)\n","EPOCH: 537 loss: 26.522092819213867\n","loss tensor(26.3678, grad_fn=<NegBackward>)\n","EPOCH: 538 loss: 26.36780548095703\n","loss tensor(26.1675, grad_fn=<NegBackward>)\n","EPOCH: 539 loss: 26.1674747467041\n","loss tensor(26.3844, grad_fn=<NegBackward>)\n","EPOCH: 540 loss: 26.38443374633789\n","loss tensor(26.8172, grad_fn=<NegBackward>)\n","EPOCH: 541 loss: 26.81724739074707\n","loss tensor(26.6127, grad_fn=<NegBackward>)\n","EPOCH: 542 loss: 26.61273765563965\n","loss tensor(26.4661, grad_fn=<NegBackward>)\n","EPOCH: 543 loss: 26.466108322143555\n","loss tensor(26.3004, grad_fn=<NegBackward>)\n","EPOCH: 544 loss: 26.30042266845703\n","loss tensor(26.5105, grad_fn=<NegBackward>)\n","EPOCH: 545 loss: 26.51048469543457\n","loss tensor(26.1048, grad_fn=<NegBackward>)\n","EPOCH: 546 loss: 26.104820251464844\n","loss tensor(26.3669, grad_fn=<NegBackward>)\n","EPOCH: 547 loss: 26.366945266723633\n","loss tensor(26.5715, grad_fn=<NegBackward>)\n","EPOCH: 548 loss: 26.571504592895508\n","loss tensor(26.3524, grad_fn=<NegBackward>)\n","EPOCH: 549 loss: 26.352428436279297\n","loss tensor(26.7525, grad_fn=<NegBackward>)\n","EPOCH: 550 loss: 26.752473831176758\n","loss tensor(26.4278, grad_fn=<NegBackward>)\n","EPOCH: 551 loss: 26.427845001220703\n","loss tensor(26.4906, grad_fn=<NegBackward>)\n","EPOCH: 552 loss: 26.490610122680664\n","loss tensor(26.6090, grad_fn=<NegBackward>)\n","EPOCH: 553 loss: 26.609012603759766\n","loss tensor(26.4435, grad_fn=<NegBackward>)\n","EPOCH: 554 loss: 26.443546295166016\n","loss tensor(26.1778, grad_fn=<NegBackward>)\n","EPOCH: 555 loss: 26.17781639099121\n","loss tensor(26.2199, grad_fn=<NegBackward>)\n","EPOCH: 556 loss: 26.21991729736328\n","loss tensor(26.3604, grad_fn=<NegBackward>)\n","EPOCH: 557 loss: 26.360403060913086\n","loss tensor(26.3690, grad_fn=<NegBackward>)\n","EPOCH: 558 loss: 26.368968963623047\n","loss tensor(26.0176, grad_fn=<NegBackward>)\n","EPOCH: 559 loss: 26.017602920532227\n","loss tensor(26.5456, grad_fn=<NegBackward>)\n","EPOCH: 560 loss: 26.545625686645508\n","loss tensor(26.6626, grad_fn=<NegBackward>)\n","EPOCH: 561 loss: 26.662622451782227\n","loss tensor(26.3353, grad_fn=<NegBackward>)\n","EPOCH: 562 loss: 26.335308074951172\n","loss tensor(26.1556, grad_fn=<NegBackward>)\n","EPOCH: 563 loss: 26.15557289123535\n","loss tensor(26.2545, grad_fn=<NegBackward>)\n","EPOCH: 564 loss: 26.254472732543945\n","loss tensor(26.3368, grad_fn=<NegBackward>)\n","EPOCH: 565 loss: 26.33682632446289\n","loss tensor(26.5492, grad_fn=<NegBackward>)\n","EPOCH: 566 loss: 26.549165725708008\n","loss tensor(26.4620, grad_fn=<NegBackward>)\n","EPOCH: 567 loss: 26.4620361328125\n","loss tensor(26.2930, grad_fn=<NegBackward>)\n","EPOCH: 568 loss: 26.293014526367188\n","loss tensor(26.4851, grad_fn=<NegBackward>)\n","EPOCH: 569 loss: 26.485109329223633\n","loss tensor(26.2824, grad_fn=<NegBackward>)\n","EPOCH: 570 loss: 26.28240394592285\n","loss tensor(26.4245, grad_fn=<NegBackward>)\n","EPOCH: 571 loss: 26.424495697021484\n","loss tensor(26.5084, grad_fn=<NegBackward>)\n","EPOCH: 572 loss: 26.50840950012207\n","loss tensor(26.5307, grad_fn=<NegBackward>)\n","EPOCH: 573 loss: 26.530670166015625\n","loss tensor(26.3732, grad_fn=<NegBackward>)\n","EPOCH: 574 loss: 26.373184204101562\n","loss tensor(26.2372, grad_fn=<NegBackward>)\n","EPOCH: 575 loss: 26.23717498779297\n","loss tensor(25.9941, grad_fn=<NegBackward>)\n","EPOCH: 576 loss: 25.99408531188965\n","loss tensor(26.2879, grad_fn=<NegBackward>)\n","EPOCH: 577 loss: 26.287883758544922\n","loss tensor(26.4037, grad_fn=<NegBackward>)\n","EPOCH: 578 loss: 26.403657913208008\n","loss tensor(26.3404, grad_fn=<NegBackward>)\n","EPOCH: 579 loss: 26.34039878845215\n","loss tensor(26.0853, grad_fn=<NegBackward>)\n","EPOCH: 580 loss: 26.08526039123535\n","loss tensor(26.4677, grad_fn=<NegBackward>)\n","EPOCH: 581 loss: 26.46773338317871\n","loss tensor(26.2726, grad_fn=<NegBackward>)\n","EPOCH: 582 loss: 26.272565841674805\n","loss tensor(26.3739, grad_fn=<NegBackward>)\n","EPOCH: 583 loss: 26.373851776123047\n","loss tensor(26.4360, grad_fn=<NegBackward>)\n","EPOCH: 584 loss: 26.43602180480957\n","loss tensor(26.1988, grad_fn=<NegBackward>)\n","EPOCH: 585 loss: 26.19878578186035\n","loss tensor(26.5272, grad_fn=<NegBackward>)\n","EPOCH: 586 loss: 26.527236938476562\n","loss tensor(26.4973, grad_fn=<NegBackward>)\n","EPOCH: 587 loss: 26.497299194335938\n","loss tensor(26.4291, grad_fn=<NegBackward>)\n","EPOCH: 588 loss: 26.42905044555664\n","loss tensor(26.1744, grad_fn=<NegBackward>)\n","EPOCH: 589 loss: 26.17441749572754\n","loss tensor(26.2188, grad_fn=<NegBackward>)\n","EPOCH: 590 loss: 26.218782424926758\n","loss tensor(26.1939, grad_fn=<NegBackward>)\n","EPOCH: 591 loss: 26.19386863708496\n","loss tensor(26.2025, grad_fn=<NegBackward>)\n","EPOCH: 592 loss: 26.20246124267578\n","loss tensor(26.6199, grad_fn=<NegBackward>)\n","EPOCH: 593 loss: 26.61986541748047\n","loss tensor(26.5359, grad_fn=<NegBackward>)\n","EPOCH: 594 loss: 26.53586769104004\n","loss tensor(26.3735, grad_fn=<NegBackward>)\n","EPOCH: 595 loss: 26.373510360717773\n","loss tensor(26.2921, grad_fn=<NegBackward>)\n","EPOCH: 596 loss: 26.292131423950195\n","loss tensor(26.3397, grad_fn=<NegBackward>)\n","EPOCH: 597 loss: 26.339706420898438\n","loss tensor(26.1512, grad_fn=<NegBackward>)\n","EPOCH: 598 loss: 26.151151657104492\n","loss tensor(26.3979, grad_fn=<NegBackward>)\n","EPOCH: 599 loss: 26.39794158935547\n","loss tensor(26.3188, grad_fn=<NegBackward>)\n","EPOCH: 600 loss: 26.318750381469727\n","loss tensor(26.3503, grad_fn=<NegBackward>)\n","EPOCH: 601 loss: 26.35034942626953\n","loss tensor(26.1510, grad_fn=<NegBackward>)\n","EPOCH: 602 loss: 26.15104866027832\n","loss tensor(26.2248, grad_fn=<NegBackward>)\n","EPOCH: 603 loss: 26.22479248046875\n","loss tensor(26.4752, grad_fn=<NegBackward>)\n","EPOCH: 604 loss: 26.475170135498047\n","loss tensor(26.3856, grad_fn=<NegBackward>)\n","EPOCH: 605 loss: 26.38555335998535\n","loss tensor(25.8660, grad_fn=<NegBackward>)\n","EPOCH: 606 loss: 25.865989685058594\n","loss tensor(26.4900, grad_fn=<NegBackward>)\n","EPOCH: 607 loss: 26.49000358581543\n","loss tensor(26.0737, grad_fn=<NegBackward>)\n","EPOCH: 608 loss: 26.073705673217773\n","loss tensor(26.2330, grad_fn=<NegBackward>)\n","EPOCH: 609 loss: 26.232961654663086\n","loss tensor(26.2924, grad_fn=<NegBackward>)\n","EPOCH: 610 loss: 26.292438507080078\n","loss tensor(26.2731, grad_fn=<NegBackward>)\n","EPOCH: 611 loss: 26.273143768310547\n","loss tensor(25.9647, grad_fn=<NegBackward>)\n","EPOCH: 612 loss: 25.96474266052246\n","loss tensor(26.2448, grad_fn=<NegBackward>)\n","EPOCH: 613 loss: 26.24481773376465\n","loss tensor(26.2451, grad_fn=<NegBackward>)\n","EPOCH: 614 loss: 26.245132446289062\n","loss tensor(26.0141, grad_fn=<NegBackward>)\n","EPOCH: 615 loss: 26.01406478881836\n","loss tensor(26.4194, grad_fn=<NegBackward>)\n","EPOCH: 616 loss: 26.419418334960938\n","loss tensor(26.2451, grad_fn=<NegBackward>)\n","EPOCH: 617 loss: 26.245098114013672\n","loss tensor(26.3176, grad_fn=<NegBackward>)\n","EPOCH: 618 loss: 26.31755256652832\n","loss tensor(26.3445, grad_fn=<NegBackward>)\n","EPOCH: 619 loss: 26.344459533691406\n","loss tensor(26.1398, grad_fn=<NegBackward>)\n","EPOCH: 620 loss: 26.139781951904297\n","loss tensor(25.8395, grad_fn=<NegBackward>)\n","EPOCH: 621 loss: 25.8394718170166\n","loss tensor(26.1040, grad_fn=<NegBackward>)\n","EPOCH: 622 loss: 26.103981018066406\n","loss tensor(26.1521, grad_fn=<NegBackward>)\n","EPOCH: 623 loss: 26.152053833007812\n","loss tensor(26.2110, grad_fn=<NegBackward>)\n","EPOCH: 624 loss: 26.210969924926758\n","loss tensor(25.8747, grad_fn=<NegBackward>)\n","EPOCH: 625 loss: 25.874691009521484\n","loss tensor(26.0002, grad_fn=<NegBackward>)\n","EPOCH: 626 loss: 26.00022315979004\n","loss tensor(26.1326, grad_fn=<NegBackward>)\n","EPOCH: 627 loss: 26.132577896118164\n","loss tensor(25.9960, grad_fn=<NegBackward>)\n","EPOCH: 628 loss: 25.995960235595703\n","loss tensor(26.1972, grad_fn=<NegBackward>)\n","EPOCH: 629 loss: 26.19715118408203\n","loss tensor(25.9468, grad_fn=<NegBackward>)\n","EPOCH: 630 loss: 25.946760177612305\n","loss tensor(26.3054, grad_fn=<NegBackward>)\n","EPOCH: 631 loss: 26.305381774902344\n","loss tensor(26.3891, grad_fn=<NegBackward>)\n","EPOCH: 632 loss: 26.389089584350586\n","loss tensor(26.4291, grad_fn=<NegBackward>)\n","EPOCH: 633 loss: 26.429086685180664\n","loss tensor(25.9363, grad_fn=<NegBackward>)\n","EPOCH: 634 loss: 25.93625259399414\n","loss tensor(25.8314, grad_fn=<NegBackward>)\n","EPOCH: 635 loss: 25.831384658813477\n","loss tensor(26.0081, grad_fn=<NegBackward>)\n","EPOCH: 636 loss: 26.008127212524414\n","loss tensor(25.8809, grad_fn=<NegBackward>)\n","EPOCH: 637 loss: 25.880897521972656\n","loss tensor(26.0998, grad_fn=<NegBackward>)\n","EPOCH: 638 loss: 26.099836349487305\n","loss tensor(26.1098, grad_fn=<NegBackward>)\n","EPOCH: 639 loss: 26.109766006469727\n","loss tensor(26.0103, grad_fn=<NegBackward>)\n","EPOCH: 640 loss: 26.01033592224121\n","loss tensor(26.1247, grad_fn=<NegBackward>)\n","EPOCH: 641 loss: 26.124713897705078\n","loss tensor(25.9314, grad_fn=<NegBackward>)\n","EPOCH: 642 loss: 25.931406021118164\n","loss tensor(26.3560, grad_fn=<NegBackward>)\n","EPOCH: 643 loss: 26.35602569580078\n","loss tensor(26.0808, grad_fn=<NegBackward>)\n","EPOCH: 644 loss: 26.080764770507812\n","loss tensor(26.2498, grad_fn=<NegBackward>)\n","EPOCH: 645 loss: 26.2497615814209\n","loss tensor(26.0528, grad_fn=<NegBackward>)\n","EPOCH: 646 loss: 26.052831649780273\n","loss tensor(26.0256, grad_fn=<NegBackward>)\n","EPOCH: 647 loss: 26.02558708190918\n","loss tensor(26.1511, grad_fn=<NegBackward>)\n","EPOCH: 648 loss: 26.151060104370117\n","loss tensor(25.9318, grad_fn=<NegBackward>)\n","EPOCH: 649 loss: 25.931764602661133\n","loss tensor(25.8830, grad_fn=<NegBackward>)\n","EPOCH: 650 loss: 25.883014678955078\n","loss tensor(25.9114, grad_fn=<NegBackward>)\n","EPOCH: 651 loss: 25.91143798828125\n","loss tensor(26.0611, grad_fn=<NegBackward>)\n","EPOCH: 652 loss: 26.061145782470703\n","loss tensor(25.9171, grad_fn=<NegBackward>)\n","EPOCH: 653 loss: 25.917123794555664\n","loss tensor(26.1037, grad_fn=<NegBackward>)\n","EPOCH: 654 loss: 26.103717803955078\n","loss tensor(26.1292, grad_fn=<NegBackward>)\n","EPOCH: 655 loss: 26.129236221313477\n","loss tensor(26.2749, grad_fn=<NegBackward>)\n","EPOCH: 656 loss: 26.2748966217041\n","loss tensor(26.1911, grad_fn=<NegBackward>)\n","EPOCH: 657 loss: 26.191137313842773\n","loss tensor(26.0846, grad_fn=<NegBackward>)\n","EPOCH: 658 loss: 26.084644317626953\n","loss tensor(26.0495, grad_fn=<NegBackward>)\n","EPOCH: 659 loss: 26.049535751342773\n","loss tensor(25.9427, grad_fn=<NegBackward>)\n","EPOCH: 660 loss: 25.94267463684082\n","loss tensor(25.9487, grad_fn=<NegBackward>)\n","EPOCH: 661 loss: 25.948654174804688\n","loss tensor(25.8831, grad_fn=<NegBackward>)\n","EPOCH: 662 loss: 25.883052825927734\n","loss tensor(26.0886, grad_fn=<NegBackward>)\n","EPOCH: 663 loss: 26.088584899902344\n","loss tensor(25.8806, grad_fn=<NegBackward>)\n","EPOCH: 664 loss: 25.880599975585938\n","loss tensor(25.9578, grad_fn=<NegBackward>)\n","EPOCH: 665 loss: 25.957843780517578\n","loss tensor(25.7674, grad_fn=<NegBackward>)\n","EPOCH: 666 loss: 25.76737403869629\n","loss tensor(25.7986, grad_fn=<NegBackward>)\n","EPOCH: 667 loss: 25.798622131347656\n","loss tensor(25.9207, grad_fn=<NegBackward>)\n","EPOCH: 668 loss: 25.920740127563477\n","loss tensor(26.0053, grad_fn=<NegBackward>)\n","EPOCH: 669 loss: 26.00534439086914\n","loss tensor(26.2544, grad_fn=<NegBackward>)\n","EPOCH: 670 loss: 26.25440788269043\n","loss tensor(25.9427, grad_fn=<NegBackward>)\n","EPOCH: 671 loss: 25.942699432373047\n","loss tensor(25.8335, grad_fn=<NegBackward>)\n","EPOCH: 672 loss: 25.8334903717041\n","loss tensor(25.8661, grad_fn=<NegBackward>)\n","EPOCH: 673 loss: 25.866146087646484\n","loss tensor(25.8067, grad_fn=<NegBackward>)\n","EPOCH: 674 loss: 25.806743621826172\n","loss tensor(26.0313, grad_fn=<NegBackward>)\n","EPOCH: 675 loss: 26.031314849853516\n","loss tensor(26.0156, grad_fn=<NegBackward>)\n","EPOCH: 676 loss: 26.015626907348633\n","loss tensor(25.7920, grad_fn=<NegBackward>)\n","EPOCH: 677 loss: 25.792016983032227\n","loss tensor(26.0466, grad_fn=<NegBackward>)\n","EPOCH: 678 loss: 26.046586990356445\n","loss tensor(25.9637, grad_fn=<NegBackward>)\n","EPOCH: 679 loss: 25.963651657104492\n","loss tensor(26.0053, grad_fn=<NegBackward>)\n","EPOCH: 680 loss: 26.005285263061523\n","loss tensor(25.9404, grad_fn=<NegBackward>)\n","EPOCH: 681 loss: 25.940397262573242\n","loss tensor(26.0735, grad_fn=<NegBackward>)\n","EPOCH: 682 loss: 26.073453903198242\n","loss tensor(25.9467, grad_fn=<NegBackward>)\n","EPOCH: 683 loss: 25.946735382080078\n","loss tensor(25.8139, grad_fn=<NegBackward>)\n","EPOCH: 684 loss: 25.81389808654785\n","loss tensor(25.8924, grad_fn=<NegBackward>)\n","EPOCH: 685 loss: 25.892440795898438\n","loss tensor(26.2967, grad_fn=<NegBackward>)\n","EPOCH: 686 loss: 26.296695709228516\n","loss tensor(25.9023, grad_fn=<NegBackward>)\n","EPOCH: 687 loss: 25.902265548706055\n","loss tensor(25.9282, grad_fn=<NegBackward>)\n","EPOCH: 688 loss: 25.928197860717773\n","loss tensor(25.7513, grad_fn=<NegBackward>)\n","EPOCH: 689 loss: 25.751323699951172\n","loss tensor(26.1404, grad_fn=<NegBackward>)\n","EPOCH: 690 loss: 26.14040184020996\n","loss tensor(26.0105, grad_fn=<NegBackward>)\n","EPOCH: 691 loss: 26.01046371459961\n","loss tensor(26.0288, grad_fn=<NegBackward>)\n","EPOCH: 692 loss: 26.028833389282227\n","loss tensor(25.7542, grad_fn=<NegBackward>)\n","EPOCH: 693 loss: 25.754213333129883\n","loss tensor(25.9710, grad_fn=<NegBackward>)\n","EPOCH: 694 loss: 25.970985412597656\n","loss tensor(25.8010, grad_fn=<NegBackward>)\n","EPOCH: 695 loss: 25.800962448120117\n","loss tensor(26.1032, grad_fn=<NegBackward>)\n","EPOCH: 696 loss: 26.103181838989258\n","loss tensor(26.0520, grad_fn=<NegBackward>)\n","EPOCH: 697 loss: 26.05200958251953\n","loss tensor(25.8716, grad_fn=<NegBackward>)\n","EPOCH: 698 loss: 25.871572494506836\n","loss tensor(26.0412, grad_fn=<NegBackward>)\n","EPOCH: 699 loss: 26.041202545166016\n","loss tensor(25.9128, grad_fn=<NegBackward>)\n","EPOCH: 700 loss: 25.9128360748291\n","loss tensor(25.9606, grad_fn=<NegBackward>)\n","EPOCH: 701 loss: 25.960594177246094\n","loss tensor(25.8006, grad_fn=<NegBackward>)\n","EPOCH: 702 loss: 25.80061912536621\n","loss tensor(25.9452, grad_fn=<NegBackward>)\n","EPOCH: 703 loss: 25.945247650146484\n","loss tensor(25.8016, grad_fn=<NegBackward>)\n","EPOCH: 704 loss: 25.80164337158203\n","loss tensor(25.6189, grad_fn=<NegBackward>)\n","EPOCH: 705 loss: 25.618919372558594\n","loss tensor(25.8512, grad_fn=<NegBackward>)\n","EPOCH: 706 loss: 25.851152420043945\n","loss tensor(25.8021, grad_fn=<NegBackward>)\n","EPOCH: 707 loss: 25.802122116088867\n","loss tensor(26.0874, grad_fn=<NegBackward>)\n","EPOCH: 708 loss: 26.087400436401367\n","loss tensor(26.1202, grad_fn=<NegBackward>)\n","EPOCH: 709 loss: 26.120208740234375\n","loss tensor(25.9804, grad_fn=<NegBackward>)\n","EPOCH: 710 loss: 25.980377197265625\n","loss tensor(25.9063, grad_fn=<NegBackward>)\n","EPOCH: 711 loss: 25.90630340576172\n","loss tensor(26.1989, grad_fn=<NegBackward>)\n","EPOCH: 712 loss: 26.19890785217285\n","loss tensor(25.7983, grad_fn=<NegBackward>)\n","EPOCH: 713 loss: 25.79832649230957\n","loss tensor(25.9393, grad_fn=<NegBackward>)\n","EPOCH: 714 loss: 25.939254760742188\n","loss tensor(25.7427, grad_fn=<NegBackward>)\n","EPOCH: 715 loss: 25.742698669433594\n","loss tensor(25.7506, grad_fn=<NegBackward>)\n","EPOCH: 716 loss: 25.75064468383789\n","loss tensor(26.1603, grad_fn=<NegBackward>)\n","EPOCH: 717 loss: 26.160295486450195\n","loss tensor(25.7252, grad_fn=<NegBackward>)\n","EPOCH: 718 loss: 25.725242614746094\n","loss tensor(25.7866, grad_fn=<NegBackward>)\n","EPOCH: 719 loss: 25.786643981933594\n","loss tensor(26.1642, grad_fn=<NegBackward>)\n","EPOCH: 720 loss: 26.1641902923584\n","loss tensor(25.9360, grad_fn=<NegBackward>)\n","EPOCH: 721 loss: 25.93596839904785\n","loss tensor(25.8832, grad_fn=<NegBackward>)\n","EPOCH: 722 loss: 25.88321304321289\n","loss tensor(26.1614, grad_fn=<NegBackward>)\n","EPOCH: 723 loss: 26.161409378051758\n","loss tensor(25.6825, grad_fn=<NegBackward>)\n","EPOCH: 724 loss: 25.682472229003906\n","loss tensor(25.9383, grad_fn=<NegBackward>)\n","EPOCH: 725 loss: 25.938337326049805\n","loss tensor(26.1095, grad_fn=<NegBackward>)\n","EPOCH: 726 loss: 26.10951042175293\n","loss tensor(25.9096, grad_fn=<NegBackward>)\n","EPOCH: 727 loss: 25.909624099731445\n","loss tensor(25.8068, grad_fn=<NegBackward>)\n","EPOCH: 728 loss: 25.806764602661133\n","loss tensor(25.5568, grad_fn=<NegBackward>)\n","EPOCH: 729 loss: 25.556833267211914\n","loss tensor(25.6601, grad_fn=<NegBackward>)\n","EPOCH: 730 loss: 25.660104751586914\n","loss tensor(25.8421, grad_fn=<NegBackward>)\n","EPOCH: 731 loss: 25.842090606689453\n","loss tensor(25.8240, grad_fn=<NegBackward>)\n","EPOCH: 732 loss: 25.823991775512695\n","loss tensor(25.7118, grad_fn=<NegBackward>)\n","EPOCH: 733 loss: 25.711780548095703\n","loss tensor(25.7731, grad_fn=<NegBackward>)\n","EPOCH: 734 loss: 25.773147583007812\n","loss tensor(25.8346, grad_fn=<NegBackward>)\n","EPOCH: 735 loss: 25.834640502929688\n","loss tensor(25.6093, grad_fn=<NegBackward>)\n","EPOCH: 736 loss: 25.609272003173828\n","loss tensor(25.8974, grad_fn=<NegBackward>)\n","EPOCH: 737 loss: 25.89743423461914\n","loss tensor(26.2270, grad_fn=<NegBackward>)\n","EPOCH: 738 loss: 26.227039337158203\n","loss tensor(25.7780, grad_fn=<NegBackward>)\n","EPOCH: 739 loss: 25.77804946899414\n","loss tensor(25.6013, grad_fn=<NegBackward>)\n","EPOCH: 740 loss: 25.601314544677734\n","loss tensor(25.7532, grad_fn=<NegBackward>)\n","EPOCH: 741 loss: 25.75318145751953\n","loss tensor(26.0914, grad_fn=<NegBackward>)\n","EPOCH: 742 loss: 26.091413497924805\n","loss tensor(25.8813, grad_fn=<NegBackward>)\n","EPOCH: 743 loss: 25.881290435791016\n","loss tensor(25.5499, grad_fn=<NegBackward>)\n","EPOCH: 744 loss: 25.549894332885742\n","loss tensor(25.7637, grad_fn=<NegBackward>)\n","EPOCH: 745 loss: 25.76369857788086\n","loss tensor(25.6787, grad_fn=<NegBackward>)\n","EPOCH: 746 loss: 25.678682327270508\n","loss tensor(25.6009, grad_fn=<NegBackward>)\n","EPOCH: 747 loss: 25.60088539123535\n","loss tensor(25.6066, grad_fn=<NegBackward>)\n","EPOCH: 748 loss: 25.606569290161133\n","loss tensor(25.5815, grad_fn=<NegBackward>)\n","EPOCH: 749 loss: 25.581546783447266\n","loss tensor(25.9962, grad_fn=<NegBackward>)\n","EPOCH: 750 loss: 25.996244430541992\n","loss tensor(26.0473, grad_fn=<NegBackward>)\n","EPOCH: 751 loss: 26.047313690185547\n","loss tensor(25.8416, grad_fn=<NegBackward>)\n","EPOCH: 752 loss: 25.84161949157715\n","loss tensor(25.7662, grad_fn=<NegBackward>)\n","EPOCH: 753 loss: 25.76622200012207\n","loss tensor(25.7094, grad_fn=<NegBackward>)\n","EPOCH: 754 loss: 25.709400177001953\n","loss tensor(25.6309, grad_fn=<NegBackward>)\n","EPOCH: 755 loss: 25.630868911743164\n","loss tensor(25.7453, grad_fn=<NegBackward>)\n","EPOCH: 756 loss: 25.74526596069336\n","loss tensor(25.9144, grad_fn=<NegBackward>)\n","EPOCH: 757 loss: 25.914426803588867\n","loss tensor(25.6202, grad_fn=<NegBackward>)\n","EPOCH: 758 loss: 25.620227813720703\n","loss tensor(25.9221, grad_fn=<NegBackward>)\n","EPOCH: 759 loss: 25.922122955322266\n","loss tensor(25.8011, grad_fn=<NegBackward>)\n","EPOCH: 760 loss: 25.801057815551758\n","loss tensor(25.7507, grad_fn=<NegBackward>)\n","EPOCH: 761 loss: 25.750713348388672\n","loss tensor(25.7191, grad_fn=<NegBackward>)\n","EPOCH: 762 loss: 25.719135284423828\n","loss tensor(25.8652, grad_fn=<NegBackward>)\n","EPOCH: 763 loss: 25.865234375\n","loss tensor(25.7833, grad_fn=<NegBackward>)\n","EPOCH: 764 loss: 25.783321380615234\n","loss tensor(25.7117, grad_fn=<NegBackward>)\n","EPOCH: 765 loss: 25.71169662475586\n","loss tensor(25.8314, grad_fn=<NegBackward>)\n","EPOCH: 766 loss: 25.831398010253906\n","loss tensor(25.8317, grad_fn=<NegBackward>)\n","EPOCH: 767 loss: 25.83171272277832\n","loss tensor(25.9228, grad_fn=<NegBackward>)\n","EPOCH: 768 loss: 25.922847747802734\n","loss tensor(25.9885, grad_fn=<NegBackward>)\n","EPOCH: 769 loss: 25.988510131835938\n","loss tensor(25.8585, grad_fn=<NegBackward>)\n","EPOCH: 770 loss: 25.858531951904297\n","loss tensor(25.7896, grad_fn=<NegBackward>)\n","EPOCH: 771 loss: 25.789630889892578\n","loss tensor(25.8469, grad_fn=<NegBackward>)\n","EPOCH: 772 loss: 25.846914291381836\n","loss tensor(25.5701, grad_fn=<NegBackward>)\n","EPOCH: 773 loss: 25.57010841369629\n","loss tensor(25.5694, grad_fn=<NegBackward>)\n","EPOCH: 774 loss: 25.56940460205078\n","loss tensor(25.5708, grad_fn=<NegBackward>)\n","EPOCH: 775 loss: 25.570755004882812\n","loss tensor(25.7747, grad_fn=<NegBackward>)\n","EPOCH: 776 loss: 25.774709701538086\n","loss tensor(25.7276, grad_fn=<NegBackward>)\n","EPOCH: 777 loss: 25.727630615234375\n","loss tensor(25.8828, grad_fn=<NegBackward>)\n","EPOCH: 778 loss: 25.882816314697266\n","loss tensor(25.7676, grad_fn=<NegBackward>)\n","EPOCH: 779 loss: 25.767562866210938\n","loss tensor(25.8897, grad_fn=<NegBackward>)\n","EPOCH: 780 loss: 25.88971519470215\n","loss tensor(25.3960, grad_fn=<NegBackward>)\n","EPOCH: 781 loss: 25.39600372314453\n","loss tensor(25.6698, grad_fn=<NegBackward>)\n","EPOCH: 782 loss: 25.66975975036621\n","loss tensor(25.5111, grad_fn=<NegBackward>)\n","EPOCH: 783 loss: 25.511133193969727\n","loss tensor(25.6761, grad_fn=<NegBackward>)\n","EPOCH: 784 loss: 25.676115036010742\n","loss tensor(25.6095, grad_fn=<NegBackward>)\n","EPOCH: 785 loss: 25.609500885009766\n","loss tensor(25.7913, grad_fn=<NegBackward>)\n","EPOCH: 786 loss: 25.791349411010742\n","loss tensor(25.4841, grad_fn=<NegBackward>)\n","EPOCH: 787 loss: 25.48412322998047\n","loss tensor(25.6894, grad_fn=<NegBackward>)\n","EPOCH: 788 loss: 25.689373016357422\n","loss tensor(25.9094, grad_fn=<NegBackward>)\n","EPOCH: 789 loss: 25.90944480895996\n","loss tensor(25.7319, grad_fn=<NegBackward>)\n","EPOCH: 790 loss: 25.7319278717041\n","loss tensor(25.4664, grad_fn=<NegBackward>)\n","EPOCH: 791 loss: 25.466386795043945\n","loss tensor(25.8763, grad_fn=<NegBackward>)\n","EPOCH: 792 loss: 25.876264572143555\n","loss tensor(25.6275, grad_fn=<NegBackward>)\n","EPOCH: 793 loss: 25.62746238708496\n","loss tensor(26.1666, grad_fn=<NegBackward>)\n","EPOCH: 794 loss: 26.166603088378906\n","loss tensor(25.3501, grad_fn=<NegBackward>)\n","EPOCH: 795 loss: 25.350122451782227\n","loss tensor(25.6195, grad_fn=<NegBackward>)\n","EPOCH: 796 loss: 25.619537353515625\n","loss tensor(25.8501, grad_fn=<NegBackward>)\n","EPOCH: 797 loss: 25.850059509277344\n","loss tensor(25.7537, grad_fn=<NegBackward>)\n","EPOCH: 798 loss: 25.75368309020996\n","loss tensor(25.4463, grad_fn=<NegBackward>)\n","EPOCH: 799 loss: 25.446271896362305\n","loss tensor(25.6126, grad_fn=<NegBackward>)\n","EPOCH: 800 loss: 25.61256217956543\n","loss tensor(25.5068, grad_fn=<NegBackward>)\n","EPOCH: 801 loss: 25.506847381591797\n","loss tensor(25.7416, grad_fn=<NegBackward>)\n","EPOCH: 802 loss: 25.7415771484375\n","loss tensor(25.7444, grad_fn=<NegBackward>)\n","EPOCH: 803 loss: 25.744382858276367\n","loss tensor(25.4430, grad_fn=<NegBackward>)\n","EPOCH: 804 loss: 25.442960739135742\n","loss tensor(25.6847, grad_fn=<NegBackward>)\n","EPOCH: 805 loss: 25.684736251831055\n","loss tensor(25.9305, grad_fn=<NegBackward>)\n","EPOCH: 806 loss: 25.9305419921875\n","loss tensor(25.5888, grad_fn=<NegBackward>)\n","EPOCH: 807 loss: 25.58880615234375\n","loss tensor(25.7110, grad_fn=<NegBackward>)\n","EPOCH: 808 loss: 25.710981369018555\n","loss tensor(26.0354, grad_fn=<NegBackward>)\n","EPOCH: 809 loss: 26.035358428955078\n","loss tensor(25.8960, grad_fn=<NegBackward>)\n","EPOCH: 810 loss: 25.895965576171875\n","loss tensor(25.6316, grad_fn=<NegBackward>)\n","EPOCH: 811 loss: 25.631610870361328\n","loss tensor(25.6551, grad_fn=<NegBackward>)\n","EPOCH: 812 loss: 25.65513038635254\n","loss tensor(26.1098, grad_fn=<NegBackward>)\n","EPOCH: 813 loss: 26.109764099121094\n","loss tensor(25.7199, grad_fn=<NegBackward>)\n","EPOCH: 814 loss: 25.71988868713379\n","loss tensor(25.8933, grad_fn=<NegBackward>)\n","EPOCH: 815 loss: 25.893272399902344\n","loss tensor(25.9030, grad_fn=<NegBackward>)\n","EPOCH: 816 loss: 25.90303611755371\n","loss tensor(25.7603, grad_fn=<NegBackward>)\n","EPOCH: 817 loss: 25.760334014892578\n","loss tensor(25.8201, grad_fn=<NegBackward>)\n","EPOCH: 818 loss: 25.820119857788086\n","loss tensor(25.8689, grad_fn=<NegBackward>)\n","EPOCH: 819 loss: 25.868940353393555\n","loss tensor(25.5323, grad_fn=<NegBackward>)\n","EPOCH: 820 loss: 25.532329559326172\n","loss tensor(25.7365, grad_fn=<NegBackward>)\n","EPOCH: 821 loss: 25.736492156982422\n","loss tensor(25.6297, grad_fn=<NegBackward>)\n","EPOCH: 822 loss: 25.62973976135254\n","loss tensor(25.8176, grad_fn=<NegBackward>)\n","EPOCH: 823 loss: 25.817626953125\n","loss tensor(25.5670, grad_fn=<NegBackward>)\n","EPOCH: 824 loss: 25.56699562072754\n","loss tensor(25.5672, grad_fn=<NegBackward>)\n","EPOCH: 825 loss: 25.567169189453125\n","loss tensor(25.7244, grad_fn=<NegBackward>)\n","EPOCH: 826 loss: 25.7243709564209\n","loss tensor(25.7178, grad_fn=<NegBackward>)\n","EPOCH: 827 loss: 25.717758178710938\n","loss tensor(25.5746, grad_fn=<NegBackward>)\n","EPOCH: 828 loss: 25.5745906829834\n","loss tensor(25.6802, grad_fn=<NegBackward>)\n","EPOCH: 829 loss: 25.68019676208496\n","loss tensor(25.3924, grad_fn=<NegBackward>)\n","EPOCH: 830 loss: 25.392362594604492\n","loss tensor(25.5919, grad_fn=<NegBackward>)\n","EPOCH: 831 loss: 25.591882705688477\n","loss tensor(25.3499, grad_fn=<NegBackward>)\n","EPOCH: 832 loss: 25.34990692138672\n","loss tensor(25.6613, grad_fn=<NegBackward>)\n","EPOCH: 833 loss: 25.661325454711914\n","loss tensor(25.6546, grad_fn=<NegBackward>)\n","EPOCH: 834 loss: 25.65459632873535\n","loss tensor(25.6038, grad_fn=<NegBackward>)\n","EPOCH: 835 loss: 25.603849411010742\n","loss tensor(25.7212, grad_fn=<NegBackward>)\n","EPOCH: 836 loss: 25.72121810913086\n","loss tensor(25.3979, grad_fn=<NegBackward>)\n","EPOCH: 837 loss: 25.39794158935547\n","loss tensor(25.5110, grad_fn=<NegBackward>)\n","EPOCH: 838 loss: 25.51101303100586\n","loss tensor(25.8539, grad_fn=<NegBackward>)\n","EPOCH: 839 loss: 25.85387420654297\n","loss tensor(25.5371, grad_fn=<NegBackward>)\n","EPOCH: 840 loss: 25.537076950073242\n","loss tensor(25.5473, grad_fn=<NegBackward>)\n","EPOCH: 841 loss: 25.54734230041504\n","loss tensor(25.8458, grad_fn=<NegBackward>)\n","EPOCH: 842 loss: 25.845766067504883\n","loss tensor(25.3787, grad_fn=<NegBackward>)\n","EPOCH: 843 loss: 25.37867546081543\n","loss tensor(25.4727, grad_fn=<NegBackward>)\n","EPOCH: 844 loss: 25.4726619720459\n","loss tensor(25.9165, grad_fn=<NegBackward>)\n","EPOCH: 845 loss: 25.91645050048828\n","loss tensor(25.6545, grad_fn=<NegBackward>)\n","EPOCH: 846 loss: 25.65447425842285\n","loss tensor(25.4838, grad_fn=<NegBackward>)\n","EPOCH: 847 loss: 25.48382568359375\n","loss tensor(25.8639, grad_fn=<NegBackward>)\n","EPOCH: 848 loss: 25.8638916015625\n","loss tensor(25.7490, grad_fn=<NegBackward>)\n","EPOCH: 849 loss: 25.749032974243164\n","loss tensor(25.6708, grad_fn=<NegBackward>)\n","EPOCH: 850 loss: 25.670766830444336\n","loss tensor(25.4625, grad_fn=<NegBackward>)\n","EPOCH: 851 loss: 25.46246910095215\n","loss tensor(25.6532, grad_fn=<NegBackward>)\n","EPOCH: 852 loss: 25.653247833251953\n","loss tensor(25.5161, grad_fn=<NegBackward>)\n","EPOCH: 853 loss: 25.516103744506836\n","loss tensor(25.5199, grad_fn=<NegBackward>)\n","EPOCH: 854 loss: 25.519939422607422\n","loss tensor(25.6644, grad_fn=<NegBackward>)\n","EPOCH: 855 loss: 25.66444969177246\n","loss tensor(25.3946, grad_fn=<NegBackward>)\n","EPOCH: 856 loss: 25.394550323486328\n","loss tensor(25.7200, grad_fn=<NegBackward>)\n","EPOCH: 857 loss: 25.720035552978516\n","loss tensor(25.4832, grad_fn=<NegBackward>)\n","EPOCH: 858 loss: 25.48320960998535\n","loss tensor(25.2276, grad_fn=<NegBackward>)\n","EPOCH: 859 loss: 25.227611541748047\n","loss tensor(25.4794, grad_fn=<NegBackward>)\n","EPOCH: 860 loss: 25.479419708251953\n","loss tensor(25.7053, grad_fn=<NegBackward>)\n","EPOCH: 861 loss: 25.705297470092773\n","loss tensor(25.6030, grad_fn=<NegBackward>)\n","EPOCH: 862 loss: 25.6030330657959\n","loss tensor(25.4505, grad_fn=<NegBackward>)\n","EPOCH: 863 loss: 25.450542449951172\n","loss tensor(25.4664, grad_fn=<NegBackward>)\n","EPOCH: 864 loss: 25.46643829345703\n","loss tensor(25.8008, grad_fn=<NegBackward>)\n","EPOCH: 865 loss: 25.80079460144043\n","loss tensor(25.6156, grad_fn=<NegBackward>)\n","EPOCH: 866 loss: 25.61558723449707\n","loss tensor(25.3703, grad_fn=<NegBackward>)\n","EPOCH: 867 loss: 25.370319366455078\n","loss tensor(25.4766, grad_fn=<NegBackward>)\n","EPOCH: 868 loss: 25.476633071899414\n","loss tensor(25.9169, grad_fn=<NegBackward>)\n","EPOCH: 869 loss: 25.916889190673828\n","loss tensor(25.7254, grad_fn=<NegBackward>)\n","EPOCH: 870 loss: 25.725379943847656\n","loss tensor(25.3104, grad_fn=<NegBackward>)\n","EPOCH: 871 loss: 25.31043815612793\n","loss tensor(25.4289, grad_fn=<NegBackward>)\n","EPOCH: 872 loss: 25.4289493560791\n","loss tensor(25.7147, grad_fn=<NegBackward>)\n","EPOCH: 873 loss: 25.71466064453125\n","loss tensor(25.5652, grad_fn=<NegBackward>)\n","EPOCH: 874 loss: 25.5651912689209\n","loss tensor(25.6975, grad_fn=<NegBackward>)\n","EPOCH: 875 loss: 25.69746971130371\n","loss tensor(25.2625, grad_fn=<NegBackward>)\n","EPOCH: 876 loss: 25.262454986572266\n","loss tensor(25.2723, grad_fn=<NegBackward>)\n","EPOCH: 877 loss: 25.272294998168945\n","loss tensor(25.4646, grad_fn=<NegBackward>)\n","EPOCH: 878 loss: 25.464570999145508\n","loss tensor(25.5596, grad_fn=<NegBackward>)\n","EPOCH: 879 loss: 25.55962371826172\n","loss tensor(25.5981, grad_fn=<NegBackward>)\n","EPOCH: 880 loss: 25.598087310791016\n","loss tensor(25.4195, grad_fn=<NegBackward>)\n","EPOCH: 881 loss: 25.419498443603516\n","loss tensor(25.2935, grad_fn=<NegBackward>)\n","EPOCH: 882 loss: 25.293542861938477\n","loss tensor(25.6803, grad_fn=<NegBackward>)\n","EPOCH: 883 loss: 25.680322647094727\n","loss tensor(25.6113, grad_fn=<NegBackward>)\n","EPOCH: 884 loss: 25.611255645751953\n","loss tensor(25.7441, grad_fn=<NegBackward>)\n","EPOCH: 885 loss: 25.744104385375977\n","loss tensor(25.6732, grad_fn=<NegBackward>)\n","EPOCH: 886 loss: 25.673206329345703\n","loss tensor(25.5006, grad_fn=<NegBackward>)\n","EPOCH: 887 loss: 25.50062370300293\n","loss tensor(25.3123, grad_fn=<NegBackward>)\n","EPOCH: 888 loss: 25.312320709228516\n","loss tensor(25.8172, grad_fn=<NegBackward>)\n","EPOCH: 889 loss: 25.817190170288086\n","loss tensor(25.4089, grad_fn=<NegBackward>)\n","EPOCH: 890 loss: 25.408855438232422\n","loss tensor(25.5003, grad_fn=<NegBackward>)\n","EPOCH: 891 loss: 25.500314712524414\n","loss tensor(25.4825, grad_fn=<NegBackward>)\n","EPOCH: 892 loss: 25.48247528076172\n","loss tensor(25.3118, grad_fn=<NegBackward>)\n","EPOCH: 893 loss: 25.311813354492188\n","loss tensor(25.4723, grad_fn=<NegBackward>)\n","EPOCH: 894 loss: 25.47226333618164\n","loss tensor(25.3506, grad_fn=<NegBackward>)\n","EPOCH: 895 loss: 25.350618362426758\n","loss tensor(25.9869, grad_fn=<NegBackward>)\n","EPOCH: 896 loss: 25.986900329589844\n","loss tensor(26.0221, grad_fn=<NegBackward>)\n","EPOCH: 897 loss: 26.022119522094727\n","loss tensor(25.4847, grad_fn=<NegBackward>)\n","EPOCH: 898 loss: 25.4847412109375\n","loss tensor(25.9195, grad_fn=<NegBackward>)\n","EPOCH: 899 loss: 25.919469833374023\n","loss tensor(25.4170, grad_fn=<NegBackward>)\n","EPOCH: 900 loss: 25.417022705078125\n","loss tensor(25.2813, grad_fn=<NegBackward>)\n","EPOCH: 901 loss: 25.28130531311035\n","loss tensor(25.3453, grad_fn=<NegBackward>)\n","EPOCH: 902 loss: 25.345338821411133\n","loss tensor(25.5732, grad_fn=<NegBackward>)\n","EPOCH: 903 loss: 25.573226928710938\n","loss tensor(25.4837, grad_fn=<NegBackward>)\n","EPOCH: 904 loss: 25.48369026184082\n","loss tensor(25.2354, grad_fn=<NegBackward>)\n","EPOCH: 905 loss: 25.235370635986328\n","loss tensor(25.3747, grad_fn=<NegBackward>)\n","EPOCH: 906 loss: 25.374679565429688\n","loss tensor(25.4502, grad_fn=<NegBackward>)\n","EPOCH: 907 loss: 25.450185775756836\n","loss tensor(25.4844, grad_fn=<NegBackward>)\n","EPOCH: 908 loss: 25.484376907348633\n","loss tensor(25.8005, grad_fn=<NegBackward>)\n","EPOCH: 909 loss: 25.8005428314209\n","loss tensor(25.5538, grad_fn=<NegBackward>)\n","EPOCH: 910 loss: 25.55381965637207\n","loss tensor(25.4324, grad_fn=<NegBackward>)\n","EPOCH: 911 loss: 25.43244171142578\n","loss tensor(25.6606, grad_fn=<NegBackward>)\n","EPOCH: 912 loss: 25.660572052001953\n","loss tensor(25.4660, grad_fn=<NegBackward>)\n","EPOCH: 913 loss: 25.465991973876953\n","loss tensor(25.3741, grad_fn=<NegBackward>)\n","EPOCH: 914 loss: 25.37411880493164\n","loss tensor(25.8049, grad_fn=<NegBackward>)\n","EPOCH: 915 loss: 25.80487823486328\n","loss tensor(25.7243, grad_fn=<NegBackward>)\n","EPOCH: 916 loss: 25.724260330200195\n","loss tensor(25.5479, grad_fn=<NegBackward>)\n","EPOCH: 917 loss: 25.54792594909668\n","loss tensor(25.5478, grad_fn=<NegBackward>)\n","EPOCH: 918 loss: 25.547809600830078\n","loss tensor(25.3420, grad_fn=<NegBackward>)\n","EPOCH: 919 loss: 25.342002868652344\n","loss tensor(25.7288, grad_fn=<NegBackward>)\n","EPOCH: 920 loss: 25.72880744934082\n","loss tensor(25.3336, grad_fn=<NegBackward>)\n","EPOCH: 921 loss: 25.33361053466797\n","loss tensor(25.5896, grad_fn=<NegBackward>)\n","EPOCH: 922 loss: 25.589553833007812\n","loss tensor(25.6222, grad_fn=<NegBackward>)\n","EPOCH: 923 loss: 25.62215805053711\n","loss tensor(25.8338, grad_fn=<NegBackward>)\n","EPOCH: 924 loss: 25.83376693725586\n","loss tensor(25.2635, grad_fn=<NegBackward>)\n","EPOCH: 925 loss: 25.263519287109375\n","loss tensor(25.6710, grad_fn=<NegBackward>)\n","EPOCH: 926 loss: 25.671039581298828\n","loss tensor(25.5882, grad_fn=<NegBackward>)\n","EPOCH: 927 loss: 25.588218688964844\n","loss tensor(25.1396, grad_fn=<NegBackward>)\n","EPOCH: 928 loss: 25.139583587646484\n","loss tensor(25.3257, grad_fn=<NegBackward>)\n","EPOCH: 929 loss: 25.325742721557617\n","loss tensor(25.2353, grad_fn=<NegBackward>)\n","EPOCH: 930 loss: 25.235334396362305\n","loss tensor(25.3882, grad_fn=<NegBackward>)\n","EPOCH: 931 loss: 25.38816261291504\n","loss tensor(25.7614, grad_fn=<NegBackward>)\n","EPOCH: 932 loss: 25.761417388916016\n","loss tensor(25.3920, grad_fn=<NegBackward>)\n","EPOCH: 933 loss: 25.392013549804688\n","loss tensor(25.4522, grad_fn=<NegBackward>)\n","EPOCH: 934 loss: 25.4521541595459\n","loss tensor(25.6152, grad_fn=<NegBackward>)\n","EPOCH: 935 loss: 25.61519432067871\n","loss tensor(25.5110, grad_fn=<NegBackward>)\n","EPOCH: 936 loss: 25.511030197143555\n","loss tensor(25.5713, grad_fn=<NegBackward>)\n","EPOCH: 937 loss: 25.57126235961914\n","loss tensor(25.3952, grad_fn=<NegBackward>)\n","EPOCH: 938 loss: 25.395160675048828\n","loss tensor(25.6077, grad_fn=<NegBackward>)\n","EPOCH: 939 loss: 25.607696533203125\n","loss tensor(25.4723, grad_fn=<NegBackward>)\n","EPOCH: 940 loss: 25.47226333618164\n","loss tensor(25.3674, grad_fn=<NegBackward>)\n","EPOCH: 941 loss: 25.367353439331055\n","loss tensor(25.5173, grad_fn=<NegBackward>)\n","EPOCH: 942 loss: 25.517271041870117\n","loss tensor(25.2892, grad_fn=<NegBackward>)\n","EPOCH: 943 loss: 25.289226531982422\n","loss tensor(25.5967, grad_fn=<NegBackward>)\n","EPOCH: 944 loss: 25.596683502197266\n","loss tensor(25.2309, grad_fn=<NegBackward>)\n","EPOCH: 945 loss: 25.23087501525879\n","loss tensor(25.3587, grad_fn=<NegBackward>)\n","EPOCH: 946 loss: 25.358667373657227\n","loss tensor(25.3182, grad_fn=<NegBackward>)\n","EPOCH: 947 loss: 25.318161010742188\n","loss tensor(25.1328, grad_fn=<NegBackward>)\n","EPOCH: 948 loss: 25.132761001586914\n","loss tensor(25.4344, grad_fn=<NegBackward>)\n","EPOCH: 949 loss: 25.434438705444336\n","loss tensor(25.1509, grad_fn=<NegBackward>)\n","EPOCH: 950 loss: 25.150924682617188\n","loss tensor(25.5325, grad_fn=<NegBackward>)\n","EPOCH: 951 loss: 25.532485961914062\n","loss tensor(25.7216, grad_fn=<NegBackward>)\n","EPOCH: 952 loss: 25.721561431884766\n","loss tensor(25.6822, grad_fn=<NegBackward>)\n","EPOCH: 953 loss: 25.682180404663086\n","loss tensor(25.3384, grad_fn=<NegBackward>)\n","EPOCH: 954 loss: 25.33835220336914\n","loss tensor(25.5022, grad_fn=<NegBackward>)\n","EPOCH: 955 loss: 25.502214431762695\n","loss tensor(25.5461, grad_fn=<NegBackward>)\n","EPOCH: 956 loss: 25.546064376831055\n","loss tensor(25.4937, grad_fn=<NegBackward>)\n","EPOCH: 957 loss: 25.49374771118164\n","loss tensor(25.2900, grad_fn=<NegBackward>)\n","EPOCH: 958 loss: 25.289974212646484\n","loss tensor(25.2942, grad_fn=<NegBackward>)\n","EPOCH: 959 loss: 25.294174194335938\n","loss tensor(25.4283, grad_fn=<NegBackward>)\n","EPOCH: 960 loss: 25.428258895874023\n","loss tensor(25.4238, grad_fn=<NegBackward>)\n","EPOCH: 961 loss: 25.423839569091797\n","loss tensor(25.1817, grad_fn=<NegBackward>)\n","EPOCH: 962 loss: 25.18172836303711\n","loss tensor(25.3605, grad_fn=<NegBackward>)\n","EPOCH: 963 loss: 25.360450744628906\n","loss tensor(25.5975, grad_fn=<NegBackward>)\n","EPOCH: 964 loss: 25.5975399017334\n","loss tensor(25.6844, grad_fn=<NegBackward>)\n","EPOCH: 965 loss: 25.684354782104492\n","loss tensor(25.6502, grad_fn=<NegBackward>)\n","EPOCH: 966 loss: 25.650163650512695\n","loss tensor(25.3099, grad_fn=<NegBackward>)\n","EPOCH: 967 loss: 25.30988311767578\n","loss tensor(25.5660, grad_fn=<NegBackward>)\n","EPOCH: 968 loss: 25.565982818603516\n","loss tensor(25.3603, grad_fn=<NegBackward>)\n","EPOCH: 969 loss: 25.36027717590332\n","loss tensor(25.1600, grad_fn=<NegBackward>)\n","EPOCH: 970 loss: 25.159988403320312\n","loss tensor(25.2987, grad_fn=<NegBackward>)\n","EPOCH: 971 loss: 25.298717498779297\n","loss tensor(25.2833, grad_fn=<NegBackward>)\n","EPOCH: 972 loss: 25.28327178955078\n","loss tensor(25.5279, grad_fn=<NegBackward>)\n","EPOCH: 973 loss: 25.527931213378906\n","loss tensor(25.4567, grad_fn=<NegBackward>)\n","EPOCH: 974 loss: 25.45668601989746\n","loss tensor(25.4454, grad_fn=<NegBackward>)\n","EPOCH: 975 loss: 25.445363998413086\n","loss tensor(25.2093, grad_fn=<NegBackward>)\n","EPOCH: 976 loss: 25.209314346313477\n","loss tensor(25.3499, grad_fn=<NegBackward>)\n","EPOCH: 977 loss: 25.349925994873047\n","loss tensor(25.3801, grad_fn=<NegBackward>)\n","EPOCH: 978 loss: 25.380050659179688\n","loss tensor(25.4451, grad_fn=<NegBackward>)\n","EPOCH: 979 loss: 25.445070266723633\n","loss tensor(25.3243, grad_fn=<NegBackward>)\n","EPOCH: 980 loss: 25.324344635009766\n","loss tensor(25.5881, grad_fn=<NegBackward>)\n","EPOCH: 981 loss: 25.58810043334961\n","loss tensor(25.4752, grad_fn=<NegBackward>)\n","EPOCH: 982 loss: 25.475196838378906\n","loss tensor(25.3197, grad_fn=<NegBackward>)\n","EPOCH: 983 loss: 25.31967544555664\n","loss tensor(24.9674, grad_fn=<NegBackward>)\n","EPOCH: 984 loss: 24.967424392700195\n","loss tensor(25.2153, grad_fn=<NegBackward>)\n","EPOCH: 985 loss: 25.215259552001953\n","loss tensor(25.4209, grad_fn=<NegBackward>)\n","EPOCH: 986 loss: 25.420902252197266\n","loss tensor(25.6868, grad_fn=<NegBackward>)\n","EPOCH: 987 loss: 25.68683433532715\n","loss tensor(25.4160, grad_fn=<NegBackward>)\n","EPOCH: 988 loss: 25.415950775146484\n","loss tensor(25.0303, grad_fn=<NegBackward>)\n","EPOCH: 989 loss: 25.030282974243164\n","loss tensor(25.4277, grad_fn=<NegBackward>)\n","EPOCH: 990 loss: 25.427711486816406\n","loss tensor(25.1632, grad_fn=<NegBackward>)\n","EPOCH: 991 loss: 25.16317367553711\n","loss tensor(25.2745, grad_fn=<NegBackward>)\n","EPOCH: 992 loss: 25.274539947509766\n","loss tensor(25.3378, grad_fn=<NegBackward>)\n","EPOCH: 993 loss: 25.337785720825195\n","loss tensor(25.1945, grad_fn=<NegBackward>)\n","EPOCH: 994 loss: 25.194467544555664\n","loss tensor(25.0182, grad_fn=<NegBackward>)\n","EPOCH: 995 loss: 25.018184661865234\n","loss tensor(25.2920, grad_fn=<NegBackward>)\n","EPOCH: 996 loss: 25.292043685913086\n","loss tensor(25.2177, grad_fn=<NegBackward>)\n","EPOCH: 997 loss: 25.217731475830078\n","loss tensor(25.4584, grad_fn=<NegBackward>)\n","EPOCH: 998 loss: 25.458402633666992\n","loss tensor(25.2537, grad_fn=<NegBackward>)\n","EPOCH: 999 loss: 25.253658294677734\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DqGP6sdqKP1U"},"source":["学習の収束の図化 (python)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"b3gAwXwTKVzE","executionInfo":{"status":"ok","timestamp":1629782870897,"user_tz":-540,"elapsed":259,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"78902a5c-eedf-45ee-f35f-25611ae10f9b"},"source":["fig = plt.figure()\n","ax = fig.add_subplot()\n","ax.plot(list(range(len(epoch_loss))), epoch_loss)\n","ax.set_xlabel('#epoch')\n","ax.set_ylabel('loss')\n","fig.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"dXOkxBZC8JBu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DH9fSkhA8KtF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DKnu0MzWPeYa"},"source":["import torch.distributions as tdist\n","\n","n = tdist.Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n","\n","e = n.sample((n_mid,))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MY5J27SJKBt4","executionInfo":{"status":"ok","timestamp":1627728127811,"user_tz":-540,"elapsed":3,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"c76573d1-5c6a-4217-8ffa-cea790c5af5a"},"source":["e"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.5516],\n","        [-1.4746],\n","        [-1.2032],\n","        [-0.4709],\n","        [ 1.0430],\n","        [-0.9447],\n","        [ 0.7663],\n","        [-0.8108],\n","        [-0.7571],\n","        [ 0.6683],\n","        [-0.9177],\n","        [-0.1356],\n","        [-0.8786],\n","        [ 3.5638],\n","        [-1.4244],\n","        [-0.5050]])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5IAeM4x1KCV4","executionInfo":{"status":"ok","timestamp":1627728172837,"user_tz":-540,"elapsed":195,"user":{"displayName":"Mari Hiroshi","photoUrl":"","userId":"18424111801791476769"}},"outputId":"f6e72e9a-5a65-496d-901b-f051a864230b"},"source":["e2= torch.randn(n_mid)\n","print(e2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([ 1.4625,  0.0852,  0.2403, -0.7425, -1.1566, -1.9472, -0.0739,  1.4408,\n","        -1.5811,  0.1749,  1.1583,  0.6945, -0.4813, -0.2892,  1.3323, -0.9172])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vw8ZVvCVKNUG"},"source":[""],"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20211004_minictorch_cross_entropy_loss.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOSa2YBiPmRePVSJBTjs3oZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Sml8FMQqbgEj"},"source":["cross entropy loss　分類問題"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ULQtL8IeXTg2","executionInfo":{"status":"ok","timestamp":1633997189485,"user_tz":-540,"elapsed":18473,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"41acfccc-94d1-4b39-ff82-e509c588cb68"},"source":["#　colaboraory用: Google drive をマウントする\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NujIexoFXUmL","executionInfo":{"status":"ok","timestamp":1633997191738,"user_tz":-540,"elapsed":246,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"af175159-94ab-4f5a-c698-0932c31ef8c6"},"source":["# colaboratory用: フォルダを移る\n","%cd \"drive/My Drive/Colab Notebooks/\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks\n"]}]},{"cell_type":"markdown","metadata":{"id":"0DNl9nIDfY0l"},"source":["フォルダは自分の指定のものに変更して下さい。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gR9cSFPNfVA4","executionInfo":{"status":"ok","timestamp":1633997213026,"user_tz":-540,"elapsed":637,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"ee00f8bb-bbe1-4d22-f900-3ca421513562"},"source":["%cd \"ctorch210929/MiniCTorch_Prototype/notebook\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks/ctorch210929/MiniCTorch_Prototype/notebook\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPPGcVEQ7fwE","executionInfo":{"status":"ok","timestamp":1633997219585,"user_tz":-540,"elapsed":4443,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"deb310f6-d31c-4122-f2b7-96eb3f9bb7de"},"source":["! pip install lark-parser"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lark-parser\n","  Downloading lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n","\u001b[?25l\r\u001b[K     |███▏                            | 10 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 20 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 30 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 40 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 81 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 102 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 103 kB 6.5 MB/s \n","\u001b[?25hInstalling collected packages: lark-parser\n","Successfully installed lark-parser-0.12.0\n"]}]},{"cell_type":"code","metadata":{"id":"vuIJaurj7brd"},"source":["import sys\n","sys.path.append(\"../\")\n","\n","import json\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import minictorch.generator as GN\n","import minictorch.converter as CV"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UWUPb5h0Blqx"},"source":["from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing   import StandardScaler\n","\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fuAfJop8BpYI"},"source":["データ読み込み"]},{"cell_type":"code","metadata":{"id":"5FYRis-rBr_Y"},"source":["# データ読み込み\n","iris = datasets.load_iris()\n","data   = iris['data']\n","target = iris['target']\n","\n","# 学習データと検証データに分割\n","x_train, x_valid, y_train, y_valid = train_test_split( data, target, shuffle=True )\n","\n","# 特徴量の標準化\n","scaler = StandardScaler()\n","scaler.fit( x_train )\n","\n","x_train = scaler.transform(x_train)\n","x_valid = scaler.transform(x_valid)\n","\n","# Tensor型に変換\n","# 学習に入れるときはfloat型 or long型になっている必要があるのここで変換してしまう\n","x_train = torch.from_numpy(x_train).float()\n","y_train = torch.from_numpy(y_train).long()\n","x_valid = torch.from_numpy(x_valid).float()\n","y_valid = torch.from_numpy(y_valid).long()\n","\n","#print('x_train : ', x_train.shape)\n","#print('y_train : ', y_train.shape)\n","#print('x_valid : ', x_valid.shape)\n","#print('y_valid : ', y_valid.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cDivgOEAB-wC"},"source":["DataSetとDataLoaderの生成"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-AznF7ZCEEJ","executionInfo":{"status":"ok","timestamp":1633997302997,"user_tz":-540,"elapsed":248,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"3d5ef7c9-a578-483c-f06e-cd2d32aace4c"},"source":["train_dataset = TensorDataset(x_train, y_train)\n","valid_dataset = TensorDataset(x_valid, y_valid)\n","\n","# indexを指定すればデータを取り出すことができます。\n","index = 0\n","print( train_dataset.__getitem__(index)[0].size() )\n","print( train_dataset.__getitem__(index)[1] )\n","\n","\n","batch_size = 112\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","\n","# 動作確認\n","# こんな感じでバッチ単位で取り出す子ができます。\n","# イテレータに変換\n","batch_iterator = iter(train_dataloader)\n","\n","# 1番目の要素を取り出す\n","inputs, labels = next(batch_iterator)\n","print(inputs.size())\n","print(labels.size())\n","print(inputs)\n","print(labels)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4])\n","tensor(2)\n","torch.Size([112, 4])\n","torch.Size([112])\n","tensor([[ 7.5603e-01, -1.2699e-01,  9.7357e-01,  7.8536e-01],\n","        [-1.2800e+00,  9.8768e-02, -1.2219e+00, -1.3027e+00],\n","        [ 9.9556e-01, -1.2699e-01,  8.0469e-01,  1.4379e+00],\n","        [ 3.7427e-02, -1.2699e-01,  2.4176e-01,  3.9385e-01],\n","        [ 1.7142e+00, -3.5274e-01,  1.4239e+00,  7.8536e-01],\n","        [-2.0211e-01, -5.7850e-01,  1.8547e-01,  1.3284e-01],\n","        [ 9.9556e-01,  9.8768e-02,  1.0299e+00,  1.5684e+00],\n","        [ 2.7696e-01, -1.2699e-01,  6.3581e-01,  7.8536e-01],\n","        [ 3.7427e-02,  3.2452e-01,  5.7952e-01,  7.8536e-01],\n","        [ 7.5603e-01, -1.2699e-01,  1.1424e+00,  1.3074e+00],\n","        [-4.4164e-01,  2.5821e+00, -1.3344e+00, -1.3027e+00],\n","        [ 2.1932e+00, -1.0300e+00,  1.7617e+00,  1.4379e+00],\n","        [ 2.7696e-01, -1.2699e-01,  4.6693e-01,  2.6334e-01],\n","        [ 9.9556e-01,  5.5028e-01,  1.0862e+00,  1.6989e+00],\n","        [-3.2187e-01, -5.7850e-01,  6.3581e-01,  1.0464e+00],\n","        [-9.2071e-01,  1.6791e+00, -1.0530e+00, -1.0417e+00],\n","        [ 5.1650e-01,  5.5028e-01,  5.2322e-01,  5.2435e-01],\n","        [-1.0405e+00,  7.7603e-01, -1.2782e+00, -1.3027e+00],\n","        [ 1.5944e+00,  3.2452e-01,  1.2550e+00,  7.8536e-01],\n","        [ 6.3626e-01, -5.7850e-01,  1.0299e+00,  1.1769e+00],\n","        [-8.2340e-02, -8.0425e-01,  7.4840e-01,  9.1587e-01],\n","        [ 2.7696e-01, -3.5274e-01,  5.2322e-01,  2.6334e-01],\n","        [-8.0094e-01, -8.0425e-01,  7.2880e-02,  2.6334e-01],\n","        [ 1.5944e+00,  1.2275e+00,  1.3113e+00,  1.6989e+00],\n","        [ 1.5719e-01, -8.0425e-01,  7.4840e-01,  5.2435e-01],\n","        [ 9.9556e-01,  9.8768e-02,  3.5435e-01,  2.6334e-01],\n","        [ 5.1650e-01, -8.0425e-01,  6.3581e-01,  7.8536e-01],\n","        [ 1.5719e-01, -1.9330e+00,  1.2917e-01, -2.5868e-01],\n","        [ 6.3626e-01,  9.8768e-02,  9.7357e-01,  7.8536e-01],\n","        [-5.6141e-01,  7.7603e-01, -1.2782e+00, -1.0417e+00],\n","        [-9.2071e-01,  1.4533e+00, -1.2782e+00, -1.0417e+00],\n","        [ 1.8339e+00, -5.7850e-01,  1.3113e+00,  9.1587e-01],\n","        [-8.2340e-02, -5.7850e-01,  7.4840e-01,  1.5684e+00],\n","        [-8.2340e-02, -8.0425e-01,  1.8547e-01, -2.5868e-01],\n","        [-8.0094e-01,  2.3563e+00, -1.2782e+00, -1.4332e+00],\n","        [ 1.1153e+00, -5.7850e-01,  5.7952e-01,  2.6334e-01],\n","        [-1.0405e+00,  3.2452e-01, -1.4470e+00, -1.3027e+00],\n","        [-9.2071e-01,  7.7603e-01, -1.2782e+00, -1.3027e+00],\n","        [-1.2800e+00,  7.7603e-01, -1.0530e+00, -1.3027e+00],\n","        [-1.5195e+00,  7.7603e-01, -1.3344e+00, -1.1722e+00],\n","        [ 1.2351e+00,  9.8768e-02,  6.3581e-01,  3.9385e-01],\n","        [-9.2071e-01, -1.2558e+00, -4.3376e-01, -1.2818e-01],\n","        [-1.7591e+00, -1.2699e-01, -1.3907e+00, -1.3027e+00],\n","        [-1.1602e+00,  9.8768e-02, -1.2782e+00, -1.3027e+00],\n","        [-5.6141e-01,  7.7603e-01, -1.1656e+00, -1.3027e+00],\n","        [-1.0405e+00, -1.2699e-01, -1.2219e+00, -1.3027e+00],\n","        [ 1.2351e+00,  9.8768e-02,  9.1728e-01,  1.1769e+00],\n","        [ 1.2351e+00,  3.2452e-01,  1.0862e+00,  1.4379e+00],\n","        [-4.4164e-01, -1.4815e+00,  1.6586e-02, -1.2818e-01],\n","        [ 1.5719e-01, -3.5274e-01,  4.1064e-01,  3.9385e-01],\n","        [ 6.3626e-01, -3.5274e-01,  2.9805e-01,  1.3284e-01],\n","        [ 2.1932e+00, -1.2699e-01,  1.3113e+00,  1.4379e+00],\n","        [ 1.1153e+00, -1.2699e-01,  9.7357e-01,  1.1769e+00],\n","        [-2.0211e-01, -1.2699e-01,  2.4176e-01,  2.3305e-03],\n","        [ 2.1932e+00,  1.6791e+00,  1.6491e+00,  1.3074e+00],\n","        [-4.4164e-01, -1.0300e+00,  3.5435e-01,  2.3305e-03],\n","        [-1.8788e+00, -1.2699e-01, -1.5033e+00, -1.4332e+00],\n","        [-4.4164e-01, -1.4815e+00, -3.9707e-02, -2.5868e-01],\n","        [-1.1602e+00,  1.2275e+00, -1.3344e+00, -1.4332e+00],\n","        [-5.6141e-01,  1.9048e+00, -1.1656e+00, -1.0417e+00],\n","        [-3.2187e-01, -3.5274e-01, -9.6000e-02,  1.3284e-01],\n","        [ 2.7696e-01, -5.7850e-01,  5.2322e-01,  2.3305e-03],\n","        [-1.1602e+00, -1.2699e-01, -1.3344e+00, -1.3027e+00],\n","        [-1.3998e+00,  3.2452e-01, -1.3907e+00, -1.3027e+00],\n","        [ 1.3549e+00,  3.2452e-01,  5.2322e-01,  2.6334e-01],\n","        [-4.4164e-01,  1.0018e+00, -1.3907e+00, -1.3027e+00],\n","        [-6.8118e-01,  1.4533e+00, -1.2782e+00, -1.3027e+00],\n","        [-8.2340e-02,  2.1306e+00, -1.4470e+00, -1.3027e+00],\n","        [-5.6141e-01, -1.2699e-01,  4.1064e-01,  3.9385e-01],\n","        [ 1.5719e-01, -1.2699e-01,  5.7952e-01,  7.8536e-01],\n","        [ 1.5719e-01, -1.9330e+00,  6.9210e-01,  3.9385e-01],\n","        [-1.0405e+00,  5.5028e-01, -1.3344e+00, -1.3027e+00],\n","        [ 3.9673e-01, -1.9330e+00,  4.1064e-01,  3.9385e-01],\n","        [ 9.9556e-01,  9.8768e-02,  5.2322e-01,  3.9385e-01],\n","        [ 1.1153e+00,  3.2452e-01,  1.1987e+00,  1.4379e+00],\n","        [-9.2071e-01,  1.6791e+00, -1.2219e+00, -1.3027e+00],\n","        [ 8.7580e-01, -3.5274e-01,  4.6693e-01,  1.3284e-01],\n","        [ 9.9556e-01,  5.5028e-01,  1.0862e+00,  1.1769e+00],\n","        [-1.0405e+00,  1.0018e+00, -1.3907e+00, -1.1722e+00],\n","        [ 6.3626e-01,  3.2452e-01,  4.1064e-01,  3.9385e-01],\n","        [ 5.1650e-01, -1.7073e+00,  3.5435e-01,  1.3284e-01],\n","        [-1.2800e+00, -1.2699e-01, -1.3344e+00, -1.4332e+00],\n","        [-2.0211e-01, -3.5274e-01,  2.4176e-01,  1.3284e-01],\n","        [-1.2800e+00, -1.2699e-01, -1.3344e+00, -1.1722e+00],\n","        [-2.0211e-01, -1.0300e+00, -1.5229e-01, -2.5868e-01],\n","        [ 6.3626e-01,  3.2452e-01,  8.6098e-01,  1.4379e+00],\n","        [-5.6141e-01,  1.9048e+00, -1.3907e+00, -1.0417e+00],\n","        [-5.6141e-01,  1.4533e+00, -1.2782e+00, -1.3027e+00],\n","        [-1.1602e+00, -1.2558e+00,  4.1064e-01,  6.5486e-01],\n","        [-2.0211e-01, -5.7850e-01,  4.1064e-01,  1.3284e-01],\n","        [-2.0211e-01,  1.6791e+00, -1.1656e+00, -1.1722e+00],\n","        [ 6.3626e-01, -5.7850e-01,  1.0299e+00,  1.3074e+00],\n","        [-1.6393e+00, -1.7073e+00, -1.3907e+00, -1.1722e+00],\n","        [-4.4164e-01, -1.2558e+00,  1.2917e-01,  1.3284e-01],\n","        [ 5.1650e-01, -3.5274e-01,  1.0299e+00,  7.8536e-01],\n","        [-9.2071e-01,  1.0018e+00, -1.3344e+00, -1.3027e+00],\n","        [-3.2187e-01, -8.0425e-01,  2.4176e-01,  1.3284e-01],\n","        [-1.0405e+00,  1.2275e+00, -1.3344e+00, -1.3027e+00],\n","        [ 7.5603e-01, -1.2699e-01,  8.0469e-01,  1.0464e+00],\n","        [ 5.1650e-01, -1.2558e+00,  6.9210e-01,  9.1587e-01],\n","        [-3.2187e-01, -1.2558e+00,  7.2880e-02, -1.2818e-01],\n","        [-1.5195e+00,  1.2275e+00, -1.5596e+00, -1.3027e+00],\n","        [ 6.3626e-01, -8.0425e-01,  8.6098e-01,  9.1587e-01],\n","        [-1.0405e+00, -2.3845e+00, -1.5229e-01, -2.5868e-01],\n","        [-1.7591e+00,  3.2452e-01, -1.3907e+00, -1.3027e+00],\n","        [ 5.1650e-01, -5.7850e-01,  7.4840e-01,  3.9385e-01],\n","        [ 2.4328e+00,  1.6791e+00,  1.4802e+00,  1.0464e+00],\n","        [-1.0405e+00, -1.7073e+00, -2.6488e-01, -2.5868e-01],\n","        [ 2.0735e+00, -1.2699e-01,  1.5928e+00,  1.1769e+00],\n","        [-8.0094e-01,  7.7603e-01, -1.3344e+00, -1.3027e+00],\n","        [ 7.5603e-01,  3.2452e-01,  7.4840e-01,  1.0464e+00],\n","        [ 9.9556e-01, -1.2558e+00,  1.1424e+00,  7.8536e-01]])\n","tensor([2, 0, 2, 1, 2, 1, 2, 2, 1, 2, 0, 2, 1, 2, 2, 0, 1, 0, 2, 2, 2, 1, 1, 2,\n","        1, 1, 2, 1, 2, 0, 0, 2, 2, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 2,\n","        1, 1, 1, 2, 2, 1, 2, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 2, 2, 0,\n","        1, 1, 2, 0, 1, 2, 0, 1, 1, 0, 1, 0, 1, 2, 0, 0, 2, 1, 0, 2, 0, 1, 2, 0,\n","        1, 0, 2, 2, 1, 0, 2, 1, 0, 2, 2, 1, 2, 0, 2, 2])\n"]}]},{"cell_type":"markdown","metadata":{"id":"8Io-p4ogJysT"},"source":["ニューラルネットワークの定義"]},{"cell_type":"code","metadata":{"id":"FMLT7mbxauCN"},"source":["class Net(nn.Module):    \n","    def __init__(self,t):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(4, 64)\n","        self.fc2 = nn.Linear(64, 3)\n","        self.target = t\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        #x = F.log_softmax(x, dim=1)\n","        #return x\n","        \n","        #print(x);\n","        #print(self.target);\n","        self.out = x\n","\n","        loss = nn.CrossEntropyLoss()\n","        #loss = nn.NLLLoss()\n","        output = loss(x,self.target)\n","        return output\n","        \n","\n","class Net2(nn.Module):    \n","    def __init__(self):\n","        super(Net2, self).__init__()\n","        self.fc1 = nn.Linear(4, 64)\n","        self.fc2 = nn.Linear(64, 3)\n","        #self.fc1 = nn.Linear(4, 128)\n","        #self.fc2 = nn.Linear(128, 3)\n","    \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        #x = F.relu(self.fc2(x))\n","        x = self.fc2(x)\n","        #x = F.softmax(x, dim=1)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_yu6J3uzDVsp"},"source":["def generate_json( json_path, input, target ):\n","\n","    model = Net( target )\n","    model.eval()\n","    with torch.no_grad():\n","        print(\"[SAVE]\", json_path )\n","        GN.generate_minictorch_file( model, input, json_path )\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gavYhJ2Z6tft","executionInfo":{"status":"ok","timestamp":1633997346570,"user_tz":-540,"elapsed":851,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"7b87fc5d-79c0-4d9f-f667-3bab21c12b34"},"source":["torch.manual_seed( 1 )\n","\n","print(\"inputs\",inputs)\n","print(\"target\",labels)\n","inputs.requires_grad = True\n","\n","project = 'cse1'\n","json_path = '../network/' + project +'.json'\n","\n","model = generate_json( json_path, inputs, labels )\n","\n","with torch.set_grad_enabled(True):\n","\n","  output = model( inputs )\n","  print(\"output\",output)\n","\n","  model.zero_grad()\n","  output.backward()\n","  print(\"output grad\",output.grad)\n","  print(\"input grad\",inputs.grad)\n","\n","  # ラベルを予測\n","  #print(\"output\", model.out, inputs.size(0))\n","  _, preds = torch.max( model.out, 1 )\n","\n","  # イテレーション結果の計算\n","  epoch_loss = output * inputs.size(0)\n","\n","  # 正解数の合計を更新\n","  epoch_corrects = torch.sum( preds == labels.data )\n","\n","  epoch_loss = epoch_loss / float(inputs.size(0))\n","  epoch_acc  = epoch_corrects.double() / float(inputs.size(0))\n","\n","  epoch=1\n","  print('Train Loss {}: {:.4f} Acc: {:.4f}'.format( epoch, epoch_loss, epoch_acc ))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs tensor([[ 7.5603e-01, -1.2699e-01,  9.7357e-01,  7.8536e-01],\n","        [-1.2800e+00,  9.8768e-02, -1.2219e+00, -1.3027e+00],\n","        [ 9.9556e-01, -1.2699e-01,  8.0469e-01,  1.4379e+00],\n","        [ 3.7427e-02, -1.2699e-01,  2.4176e-01,  3.9385e-01],\n","        [ 1.7142e+00, -3.5274e-01,  1.4239e+00,  7.8536e-01],\n","        [-2.0211e-01, -5.7850e-01,  1.8547e-01,  1.3284e-01],\n","        [ 9.9556e-01,  9.8768e-02,  1.0299e+00,  1.5684e+00],\n","        [ 2.7696e-01, -1.2699e-01,  6.3581e-01,  7.8536e-01],\n","        [ 3.7427e-02,  3.2452e-01,  5.7952e-01,  7.8536e-01],\n","        [ 7.5603e-01, -1.2699e-01,  1.1424e+00,  1.3074e+00],\n","        [-4.4164e-01,  2.5821e+00, -1.3344e+00, -1.3027e+00],\n","        [ 2.1932e+00, -1.0300e+00,  1.7617e+00,  1.4379e+00],\n","        [ 2.7696e-01, -1.2699e-01,  4.6693e-01,  2.6334e-01],\n","        [ 9.9556e-01,  5.5028e-01,  1.0862e+00,  1.6989e+00],\n","        [-3.2187e-01, -5.7850e-01,  6.3581e-01,  1.0464e+00],\n","        [-9.2071e-01,  1.6791e+00, -1.0530e+00, -1.0417e+00],\n","        [ 5.1650e-01,  5.5028e-01,  5.2322e-01,  5.2435e-01],\n","        [-1.0405e+00,  7.7603e-01, -1.2782e+00, -1.3027e+00],\n","        [ 1.5944e+00,  3.2452e-01,  1.2550e+00,  7.8536e-01],\n","        [ 6.3626e-01, -5.7850e-01,  1.0299e+00,  1.1769e+00],\n","        [-8.2340e-02, -8.0425e-01,  7.4840e-01,  9.1587e-01],\n","        [ 2.7696e-01, -3.5274e-01,  5.2322e-01,  2.6334e-01],\n","        [-8.0094e-01, -8.0425e-01,  7.2880e-02,  2.6334e-01],\n","        [ 1.5944e+00,  1.2275e+00,  1.3113e+00,  1.6989e+00],\n","        [ 1.5719e-01, -8.0425e-01,  7.4840e-01,  5.2435e-01],\n","        [ 9.9556e-01,  9.8768e-02,  3.5435e-01,  2.6334e-01],\n","        [ 5.1650e-01, -8.0425e-01,  6.3581e-01,  7.8536e-01],\n","        [ 1.5719e-01, -1.9330e+00,  1.2917e-01, -2.5868e-01],\n","        [ 6.3626e-01,  9.8768e-02,  9.7357e-01,  7.8536e-01],\n","        [-5.6141e-01,  7.7603e-01, -1.2782e+00, -1.0417e+00],\n","        [-9.2071e-01,  1.4533e+00, -1.2782e+00, -1.0417e+00],\n","        [ 1.8339e+00, -5.7850e-01,  1.3113e+00,  9.1587e-01],\n","        [-8.2340e-02, -5.7850e-01,  7.4840e-01,  1.5684e+00],\n","        [-8.2340e-02, -8.0425e-01,  1.8547e-01, -2.5868e-01],\n","        [-8.0094e-01,  2.3563e+00, -1.2782e+00, -1.4332e+00],\n","        [ 1.1153e+00, -5.7850e-01,  5.7952e-01,  2.6334e-01],\n","        [-1.0405e+00,  3.2452e-01, -1.4470e+00, -1.3027e+00],\n","        [-9.2071e-01,  7.7603e-01, -1.2782e+00, -1.3027e+00],\n","        [-1.2800e+00,  7.7603e-01, -1.0530e+00, -1.3027e+00],\n","        [-1.5195e+00,  7.7603e-01, -1.3344e+00, -1.1722e+00],\n","        [ 1.2351e+00,  9.8768e-02,  6.3581e-01,  3.9385e-01],\n","        [-9.2071e-01, -1.2558e+00, -4.3376e-01, -1.2818e-01],\n","        [-1.7591e+00, -1.2699e-01, -1.3907e+00, -1.3027e+00],\n","        [-1.1602e+00,  9.8768e-02, -1.2782e+00, -1.3027e+00],\n","        [-5.6141e-01,  7.7603e-01, -1.1656e+00, -1.3027e+00],\n","        [-1.0405e+00, -1.2699e-01, -1.2219e+00, -1.3027e+00],\n","        [ 1.2351e+00,  9.8768e-02,  9.1728e-01,  1.1769e+00],\n","        [ 1.2351e+00,  3.2452e-01,  1.0862e+00,  1.4379e+00],\n","        [-4.4164e-01, -1.4815e+00,  1.6586e-02, -1.2818e-01],\n","        [ 1.5719e-01, -3.5274e-01,  4.1064e-01,  3.9385e-01],\n","        [ 6.3626e-01, -3.5274e-01,  2.9805e-01,  1.3284e-01],\n","        [ 2.1932e+00, -1.2699e-01,  1.3113e+00,  1.4379e+00],\n","        [ 1.1153e+00, -1.2699e-01,  9.7357e-01,  1.1769e+00],\n","        [-2.0211e-01, -1.2699e-01,  2.4176e-01,  2.3305e-03],\n","        [ 2.1932e+00,  1.6791e+00,  1.6491e+00,  1.3074e+00],\n","        [-4.4164e-01, -1.0300e+00,  3.5435e-01,  2.3305e-03],\n","        [-1.8788e+00, -1.2699e-01, -1.5033e+00, -1.4332e+00],\n","        [-4.4164e-01, -1.4815e+00, -3.9707e-02, -2.5868e-01],\n","        [-1.1602e+00,  1.2275e+00, -1.3344e+00, -1.4332e+00],\n","        [-5.6141e-01,  1.9048e+00, -1.1656e+00, -1.0417e+00],\n","        [-3.2187e-01, -3.5274e-01, -9.6000e-02,  1.3284e-01],\n","        [ 2.7696e-01, -5.7850e-01,  5.2322e-01,  2.3305e-03],\n","        [-1.1602e+00, -1.2699e-01, -1.3344e+00, -1.3027e+00],\n","        [-1.3998e+00,  3.2452e-01, -1.3907e+00, -1.3027e+00],\n","        [ 1.3549e+00,  3.2452e-01,  5.2322e-01,  2.6334e-01],\n","        [-4.4164e-01,  1.0018e+00, -1.3907e+00, -1.3027e+00],\n","        [-6.8118e-01,  1.4533e+00, -1.2782e+00, -1.3027e+00],\n","        [-8.2340e-02,  2.1306e+00, -1.4470e+00, -1.3027e+00],\n","        [-5.6141e-01, -1.2699e-01,  4.1064e-01,  3.9385e-01],\n","        [ 1.5719e-01, -1.2699e-01,  5.7952e-01,  7.8536e-01],\n","        [ 1.5719e-01, -1.9330e+00,  6.9210e-01,  3.9385e-01],\n","        [-1.0405e+00,  5.5028e-01, -1.3344e+00, -1.3027e+00],\n","        [ 3.9673e-01, -1.9330e+00,  4.1064e-01,  3.9385e-01],\n","        [ 9.9556e-01,  9.8768e-02,  5.2322e-01,  3.9385e-01],\n","        [ 1.1153e+00,  3.2452e-01,  1.1987e+00,  1.4379e+00],\n","        [-9.2071e-01,  1.6791e+00, -1.2219e+00, -1.3027e+00],\n","        [ 8.7580e-01, -3.5274e-01,  4.6693e-01,  1.3284e-01],\n","        [ 9.9556e-01,  5.5028e-01,  1.0862e+00,  1.1769e+00],\n","        [-1.0405e+00,  1.0018e+00, -1.3907e+00, -1.1722e+00],\n","        [ 6.3626e-01,  3.2452e-01,  4.1064e-01,  3.9385e-01],\n","        [ 5.1650e-01, -1.7073e+00,  3.5435e-01,  1.3284e-01],\n","        [-1.2800e+00, -1.2699e-01, -1.3344e+00, -1.4332e+00],\n","        [-2.0211e-01, -3.5274e-01,  2.4176e-01,  1.3284e-01],\n","        [-1.2800e+00, -1.2699e-01, -1.3344e+00, -1.1722e+00],\n","        [-2.0211e-01, -1.0300e+00, -1.5229e-01, -2.5868e-01],\n","        [ 6.3626e-01,  3.2452e-01,  8.6098e-01,  1.4379e+00],\n","        [-5.6141e-01,  1.9048e+00, -1.3907e+00, -1.0417e+00],\n","        [-5.6141e-01,  1.4533e+00, -1.2782e+00, -1.3027e+00],\n","        [-1.1602e+00, -1.2558e+00,  4.1064e-01,  6.5486e-01],\n","        [-2.0211e-01, -5.7850e-01,  4.1064e-01,  1.3284e-01],\n","        [-2.0211e-01,  1.6791e+00, -1.1656e+00, -1.1722e+00],\n","        [ 6.3626e-01, -5.7850e-01,  1.0299e+00,  1.3074e+00],\n","        [-1.6393e+00, -1.7073e+00, -1.3907e+00, -1.1722e+00],\n","        [-4.4164e-01, -1.2558e+00,  1.2917e-01,  1.3284e-01],\n","        [ 5.1650e-01, -3.5274e-01,  1.0299e+00,  7.8536e-01],\n","        [-9.2071e-01,  1.0018e+00, -1.3344e+00, -1.3027e+00],\n","        [-3.2187e-01, -8.0425e-01,  2.4176e-01,  1.3284e-01],\n","        [-1.0405e+00,  1.2275e+00, -1.3344e+00, -1.3027e+00],\n","        [ 7.5603e-01, -1.2699e-01,  8.0469e-01,  1.0464e+00],\n","        [ 5.1650e-01, -1.2558e+00,  6.9210e-01,  9.1587e-01],\n","        [-3.2187e-01, -1.2558e+00,  7.2880e-02, -1.2818e-01],\n","        [-1.5195e+00,  1.2275e+00, -1.5596e+00, -1.3027e+00],\n","        [ 6.3626e-01, -8.0425e-01,  8.6098e-01,  9.1587e-01],\n","        [-1.0405e+00, -2.3845e+00, -1.5229e-01, -2.5868e-01],\n","        [-1.7591e+00,  3.2452e-01, -1.3907e+00, -1.3027e+00],\n","        [ 5.1650e-01, -5.7850e-01,  7.4840e-01,  3.9385e-01],\n","        [ 2.4328e+00,  1.6791e+00,  1.4802e+00,  1.0464e+00],\n","        [-1.0405e+00, -1.7073e+00, -2.6488e-01, -2.5868e-01],\n","        [ 2.0735e+00, -1.2699e-01,  1.5928e+00,  1.1769e+00],\n","        [-8.0094e-01,  7.7603e-01, -1.3344e+00, -1.3027e+00],\n","        [ 7.5603e-01,  3.2452e-01,  7.4840e-01,  1.0464e+00],\n","        [ 9.9556e-01, -1.2558e+00,  1.1424e+00,  7.8536e-01]])\n","target tensor([2, 0, 2, 1, 2, 1, 2, 2, 1, 2, 0, 2, 1, 2, 2, 0, 1, 0, 2, 2, 2, 1, 1, 2,\n","        1, 1, 2, 1, 2, 0, 0, 2, 2, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 2,\n","        1, 1, 1, 2, 2, 1, 2, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 2, 2, 0,\n","        1, 1, 2, 0, 1, 2, 0, 1, 1, 0, 1, 0, 1, 2, 0, 0, 2, 1, 0, 2, 0, 1, 2, 0,\n","        1, 0, 2, 2, 1, 0, 2, 1, 0, 2, 2, 1, 2, 0, 2, 2])\n","[SAVE] ../network/cse1.json\n","skip: Net/Linear[fc1]/weight/26\n","skip: Net/Linear[fc1]/weight/26\n","skip: Net/Linear[fc2]/weight/29\n","skip: Net/Linear[fc2]/weight/29\n","output tensor(1.1529, grad_fn=<NllLossBackward>)\n","output grad None\n","input grad tensor([[ 7.5204e-04,  1.2240e-03, -6.8898e-04,  8.2862e-04],\n","        [-1.2364e-03, -5.0431e-04,  3.7808e-04, -4.9678e-04],\n","        [ 1.9556e-04,  8.5828e-04, -7.0654e-04,  9.0441e-04],\n","        [ 3.8523e-04, -6.8969e-04,  5.6433e-04, -1.0087e-03],\n","        [ 1.1719e-03,  1.0938e-03, -4.9961e-04,  5.5832e-04],\n","        [ 1.0005e-03, -4.8740e-04,  1.2632e-03, -4.6770e-04],\n","        [ 5.2906e-04,  8.5324e-04, -5.3784e-04,  6.1835e-04],\n","        [ 8.4336e-04,  9.7026e-04, -1.0496e-03,  8.4999e-04],\n","        [ 7.6622e-05, -6.6827e-04, -2.8178e-04, -1.5851e-03],\n","        [ 5.4083e-04,  1.5011e-03, -3.7460e-04,  7.5169e-04],\n","        [-1.4596e-03,  3.8086e-04,  3.5870e-04,  3.3955e-04],\n","        [ 1.3971e-03,  7.9271e-04, -7.1302e-04,  6.3249e-04],\n","        [-1.3394e-04, -5.6120e-04,  7.3256e-05, -1.7355e-03],\n","        [ 2.1525e-04,  1.0139e-03,  1.3270e-05,  6.8002e-04],\n","        [ 1.0059e-03,  9.8630e-04, -1.1935e-03,  8.6262e-04],\n","        [-1.9062e-03, -8.3865e-05,  5.3355e-04,  3.1503e-04],\n","        [-2.1988e-04, -6.5004e-04, -4.8273e-04, -1.1523e-03],\n","        [-1.3805e-03, -5.9792e-04,  3.3659e-04, -3.1947e-04],\n","        [ 1.4010e-03,  1.0948e-03, -4.6536e-04,  4.8778e-05],\n","        [ 5.5356e-04,  1.0657e-03, -6.9685e-04,  8.6542e-04],\n","        [ 9.4411e-04,  1.3735e-03, -1.3459e-03,  9.0613e-04],\n","        [-2.7723e-04, -4.9538e-04,  3.3562e-04, -1.6274e-03],\n","        [ 8.3908e-04, -2.4979e-04,  1.1431e-03, -1.6188e-04],\n","        [ 9.5230e-04,  1.5161e-03, -6.9743e-05,  2.9453e-04],\n","        [-3.7281e-04, -1.2693e-04, -6.4146e-06, -1.5190e-03],\n","        [-5.0815e-04, -1.1872e-04, -6.1904e-04, -1.0525e-03],\n","        [ 1.1848e-03,  7.1278e-04, -1.0521e-03,  7.2215e-04],\n","        [-1.1562e-04,  7.1675e-04,  9.1066e-04,  7.8610e-04],\n","        [ 7.5672e-04,  1.1332e-03, -8.5013e-04,  9.5839e-04],\n","        [-1.8836e-03, -5.1337e-04,  3.2186e-04, -2.9365e-04],\n","        [-1.5327e-03, -3.0506e-04,  4.9752e-04,  4.5505e-05],\n","        [ 1.4012e-03,  8.0719e-04, -8.0896e-04,  5.7886e-04],\n","        [ 9.9653e-04,  1.1537e-03, -9.7020e-04,  7.9681e-04],\n","        [ 1.1822e-03, -3.6295e-05,  1.5206e-03,  1.5605e-04],\n","        [-1.5347e-03,  3.9050e-04,  3.6293e-04,  3.5553e-04],\n","        [-8.3362e-04, -1.3966e-04, -5.9801e-04, -5.3709e-04],\n","        [-1.3346e-03, -6.4495e-04,  2.9515e-04, -3.6247e-04],\n","        [-1.3536e-03, -6.0020e-04,  3.3835e-04, -3.1060e-04],\n","        [-1.4601e-03, -4.6180e-04,  1.2655e-04, -1.8228e-04],\n","        [-1.3681e-03, -4.6516e-04,  5.0286e-04, -4.8770e-04],\n","        [-5.1054e-04, -1.2355e-04, -6.4736e-04, -1.0444e-03],\n","        [ 8.8931e-04, -3.6507e-04,  1.3177e-03,  3.3364e-04],\n","        [-1.1340e-03, -4.3987e-04,  3.7451e-04, -4.8115e-04],\n","        [-1.3809e-03, -6.8368e-04,  4.1535e-04, -3.4379e-04],\n","        [-1.9005e-03, -5.6437e-04,  3.1190e-04, -3.1234e-04],\n","        [-1.2264e-03, -5.0112e-04,  4.1725e-04, -5.0457e-04],\n","        [ 8.9642e-04,  7.0169e-04, -3.4279e-04,  4.0693e-04],\n","        [ 3.9597e-04,  1.0177e-03, -6.3328e-05,  6.1001e-04],\n","        [ 3.7513e-04,  3.5626e-04,  9.7749e-04,  2.8456e-06],\n","        [-2.6399e-04, -3.7373e-04,  2.5027e-04, -1.5288e-03],\n","        [-5.3999e-04,  7.5919e-05, -3.0558e-04, -1.5647e-03],\n","        [ 1.0628e-03,  3.2470e-04, -2.5524e-04,  4.3486e-04],\n","        [ 1.1044e-03,  1.0502e-03, -5.6901e-04,  4.9496e-04],\n","        [ 1.0380e-03, -1.4086e-03,  7.9540e-04, -6.9957e-04],\n","        [ 7.9928e-04,  6.0877e-04,  7.5445e-04,  5.6879e-04],\n","        [ 7.8388e-04,  9.9168e-05,  1.0066e-03, -3.7167e-05],\n","        [-1.0962e-03, -4.7417e-04,  3.0621e-04, -5.3846e-04],\n","        [ 1.8012e-04,  5.6320e-04,  1.3306e-03,  2.5396e-04],\n","        [-1.4258e-03, -5.8025e-04,  3.2539e-04, -3.3697e-04],\n","        [-1.4410e-03,  3.1098e-04,  3.6424e-04,  3.4784e-04],\n","        [ 1.1918e-03, -4.7905e-04,  1.4269e-03, -1.5959e-04],\n","        [-1.0044e-03,  7.0583e-05, -4.9294e-05, -1.6893e-03],\n","        [-1.2393e-03, -4.9643e-04,  3.9652e-04, -5.0680e-04],\n","        [-1.4586e-03, -6.0333e-04,  4.2405e-04, -3.2287e-04],\n","        [-5.6047e-04, -1.5505e-04, -6.2682e-04, -1.2156e-03],\n","        [-1.8466e-03, -5.1973e-04,  2.8606e-04, -2.6409e-04],\n","        [-1.0816e-03, -2.2850e-04, -1.7227e-04,  6.4106e-06],\n","        [-1.4625e-03, -3.9448e-05,  1.4848e-04, -2.0536e-04],\n","        [ 8.8761e-04, -1.2836e-03,  9.6684e-04, -5.4208e-04],\n","        [ 8.4070e-04,  9.6655e-04, -1.0466e-03,  8.5012e-04],\n","        [ 5.5478e-04,  1.8485e-04, -1.1545e-03,  1.6280e-04],\n","        [-1.3296e-03, -6.4221e-04,  2.9377e-04, -3.6123e-04],\n","        [-4.5004e-04,  8.0510e-04,  2.2957e-06,  2.2784e-04],\n","        [-5.3059e-04, -1.3805e-04, -6.4677e-04, -1.0541e-03],\n","        [ 4.4183e-04,  1.0677e-03, -8.8223e-05,  6.4179e-04],\n","        [-1.5250e-03,  2.6193e-04,  2.6548e-04,  4.2702e-04],\n","        [-6.1460e-04, -1.3120e-04, -2.7424e-04, -1.2701e-03],\n","        [ 4.8448e-04,  8.9314e-04, -1.1239e-04,  7.0974e-04],\n","        [-1.3766e-03, -5.9067e-04,  3.3233e-04, -3.1962e-04],\n","        [-1.5525e-04, -7.1298e-04, -5.2538e-04, -1.0864e-03],\n","        [-2.5022e-04,  9.6017e-04,  2.7504e-05,  1.3186e-04],\n","        [-1.2433e-03, -5.0563e-04,  3.7817e-04, -4.9958e-04],\n","        [ 6.7987e-04, -1.0639e-03,  8.9263e-04, -6.9835e-04],\n","        [-1.2452e-03, -4.9606e-04,  3.9156e-04, -5.0835e-04],\n","        [ 1.2106e-03, -7.5358e-05,  1.6918e-03,  5.9759e-04],\n","        [ 3.7618e-04,  8.3467e-04, -4.2745e-04,  9.6670e-04],\n","        [-2.0159e-03,  3.6079e-04,  4.9813e-04,  2.7179e-04],\n","        [-1.5777e-03, -1.2538e-04,  4.8991e-05, -1.2137e-04],\n","        [ 4.1944e-04,  8.3073e-04, -1.7418e-03,  1.0607e-04],\n","        [ 3.9655e-04, -8.7851e-05,  1.4353e-03, -1.0251e-03],\n","        [-1.4317e-03, -7.2004e-05,  1.4340e-04, -1.9002e-04],\n","        [ 7.0745e-04,  9.9054e-04, -7.5537e-04,  7.4943e-04],\n","        [-2.1087e-03, -8.6518e-04,  1.6844e-03, -3.6541e-04],\n","        [ 4.6621e-04,  6.1950e-05,  1.1428e-03, -2.6774e-04],\n","        [ 7.3182e-04,  1.0907e-03, -5.5455e-04,  7.8701e-04],\n","        [-1.3586e-03, -5.9236e-04,  3.3360e-04, -3.1364e-04],\n","        [ 4.7313e-04,  9.5258e-05,  1.1243e-03, -5.1411e-04],\n","        [-1.3894e-03, -5.8361e-04,  3.2793e-04, -3.2493e-04],\n","        [ 4.5250e-04,  1.2447e-03, -9.3778e-04,  9.6682e-04],\n","        [ 1.1577e-03,  6.9058e-04, -1.0026e-03,  7.2512e-04],\n","        [ 3.9425e-04,  3.7490e-04,  9.5764e-04,  1.0936e-05],\n","        [-1.3892e-03, -4.4495e-04,  4.8973e-04, -5.0086e-04],\n","        [ 1.1805e-03,  7.0370e-04, -1.0203e-03,  7.4098e-04],\n","        [-9.1598e-05,  3.1957e-04,  1.1749e-03,  3.8047e-04],\n","        [-1.1512e-03, -4.1282e-04,  3.4700e-04, -4.8241e-04],\n","        [ 1.1807e-03,  7.7688e-04, -7.1017e-04,  9.5758e-04],\n","        [ 7.0608e-04,  5.6654e-04,  8.0901e-04,  6.4887e-04],\n","        [ 6.4547e-04, -2.9697e-04,  1.2898e-03,  2.5538e-04],\n","        [ 9.1267e-04,  4.9852e-04, -5.0678e-05,  3.5982e-04],\n","        [-1.4505e-03, -6.7761e-04,  7.6888e-05, -2.1035e-04],\n","        [ 6.4060e-04,  7.2888e-04, -6.3766e-04,  6.5938e-04],\n","        [ 1.1981e-03,  6.4050e-04, -9.2417e-04,  7.0554e-04]])\n","Train Loss 1: 1.1529 Acc: 0.1518\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n"]}]},{"cell_type":"code","metadata":{"id":"MN3gKf3D8iut"},"source":["\"\"\"\n","def convert_json( project, folder, model, input_x, json_path, rand_flag=0 ):\n","\n","    #folder = \"src\"\n","    cpp_fname   = project + \".cpp\"\n","    param_fname = project + \"_param.cpp\"\n","    cpp_path    = folder + \"/\" + cpp_fname\n","    param_path  = folder + \"/\" + param_fname\n","    make_path   = folder + \"/\" + \"Makefile\"\n","\n","    # load json file\n","    print( \"[JSON]\", json_path )\n","    fp = open( json_path )\n","    obj = json.load( fp )\n","\n","    # save parameter file\n","    code1 = CV.c_param_generator( obj, model, input_x )\n","    if len( code1 ) > 0:\n","       print( \"[PARAM]\", param_path )\n","       ofparam = open( param_path, \"w\" )\n","       ofparam.write( code1 )\n","\n","    # save cpp file\n","    print( \"[CPP]  \", cpp_path )\n","    code2 = CV.c_code_generator( obj, model, rand_flag )\n","\n","    #ofp=open(args.path+\"/\"+args.output,\"w\")\n","    ofp = open( cpp_path, \"w\" )\n","    ofp.write( code2 )\n","\n","    # save make file\n","    print( \"[MAKE] \", make_path )\n","    make_code = CV.makefile_generator( cpp_fname )\n","\n","    #makefp=open(args.path+\"/\"+\"Makefile\",\"w\")\n","    makefp = open( make_path, \"w\" )\n","    makefp.write( make_code )\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5EtFEsQ0sLW1","executionInfo":{"status":"ok","timestamp":1633997358099,"user_tz":-540,"elapsed":611,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"43b7f849-3df4-4997-ea21-47499dda962a"},"source":["CV.convert_json( project, \"../src\", model, inputs, json_path )"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[JSON] ../network/cse1.json\n","{'name': 'Net/Linear[fc1]/weight/35', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [3], 'sorted_id': 1}\n","{'name': 'Net/Linear[fc1]/bias/34', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [3], 'sorted_id': 2}\n","{'name': 'Net/Linear[fc2]/weight/38', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [7], 'sorted_id': 5}\n","{'name': 'Net/Linear[fc2]/bias/37', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [7], 'sorted_id': 6}\n","[PARAM] ../src/cse1_param.cpp\n","{'name': 'input/x', 'op': 'IO Node', 'in': [], 'output_id': 0, 'shape': [112, 4], 'out': [3], 'sorted_id': 0}\n","{'name': 'Net/Linear[fc1]/weight/35', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [3], 'sorted_id': 1}\n","Net/Linear[fc1]/weight/35  ->  fc1_weight\n","{'name': 'Net/Linear[fc1]/bias/34', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [3], 'sorted_id': 2}\n","Net/Linear[fc1]/bias/34  ->  fc1_bias\n","{'name': 'Net/Linear[fc1]/input.1', 'op': 'aten::linear', 'in': [0, 1, 2], 'output_id': 0, 'shape': [112, 64], 'out': [4], 'sorted_id': 3}\n","{'name': 'Net/input.3', 'op': 'aten::relu', 'in': [3], 'output_id': 0, 'shape': [112, 64], 'out': [7], 'sorted_id': 4}\n","{'name': 'Net/Linear[fc2]/weight/38', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [7], 'sorted_id': 5}\n","Net/Linear[fc2]/weight/38  ->  fc2_weight\n","{'name': 'Net/Linear[fc2]/bias/37', 'op': 'prim::GetAttr', 'in': [], 'output_id': 0, 'shape': [], 'out': [7], 'sorted_id': 6}\n","Net/Linear[fc2]/bias/37  ->  fc2_bias\n","{'name': 'Net/Linear[fc2]/input', 'op': 'aten::linear', 'in': [4, 5, 6], 'output_id': 0, 'shape': [112, 3], 'out': [12], 'sorted_id': 7}\n","{'name': 'Net/17', 'op': 'prim::Constant', 'in': [], 'output_id': 0, 'shape': [112], 'constant_value': [2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 2.0, 0.0, 1.0, 0.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 0.0, 0.0, 2.0, 2.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 2.0, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 2.0, 1.0, 2.0, 0.0, 2.0, 2.0], 'out': [12], 'sorted_id': 8}\n","{'name': 'Net/18', 'op': 'prim::Constant', 'in': [], 'output_id': 0, 'shape': [], 'out': [12], 'sorted_id': 9}\n","{'name': 'Net/19', 'op': 'prim::Constant', 'in': [], 'output_id': 0, 'shape': [], 'constant_value': 1.0, 'out': [12], 'sorted_id': 10}\n","{'name': 'Net/20', 'op': 'prim::Constant', 'in': [], 'output_id': 0, 'shape': [], 'constant_value': -100.0, 'out': [12], 'sorted_id': 11}\n","{'name': 'Net/21', 'op': 'aten::cross_entropy_loss', 'in': [7, 8, 9, 10, 11], 'output_id': 0, 'shape': [], 'out': [13], 'sorted_id': 12}\n","{'name': 'output/output.1', 'op': 'IO Node', 'in': [12], 'output_id': 0, 'shape': [], 'out': [], 'sorted_id': 13}\n","[CPP]  ../src/cse1.cpp\n","[MAKE] ../src/Makefile\n"]}]},{"cell_type":"code","metadata":{"id":"zf1OiQzc9u5t"},"source":["!g++ -std=c++14 ../src/cse1.cpp ../src/cse1_param.cpp -I ../../../xtensor -lcblas -o ../bin/cse1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4bIvl823mZVt"},"source":["(注意) xtensorフォルダにxtensor関連のincludeを置いています。各自の環境に合わせて変更して下さい。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I31lNv_hh4s2","executionInfo":{"status":"ok","timestamp":1633997750296,"user_tz":-540,"elapsed":640,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"61c6bc3f-c201-4d1a-aaf6-09a98ab9c391"},"source":["!../bin/cse1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["### forward computation ...\n"," 1.152917\n","### backward computation ...\n","input_grad{{ 0.000752,  0.001224, -0.000689,  0.000829},\n"," {-0.001236, -0.000504,  0.000378, -0.000497},\n"," { 0.000196,  0.000858, -0.000707,  0.000904},\n"," { 0.000385, -0.00069 ,  0.000564, -0.001009},\n"," { 0.001172,  0.001094, -0.0005  ,  0.000558},\n"," { 0.001   , -0.000487,  0.001263, -0.000468},\n"," { 0.000529,  0.000853, -0.000538,  0.000618},\n"," { 0.000843,  0.00097 , -0.00105 ,  0.00085 },\n"," { 0.000077, -0.000668, -0.000282, -0.001585},\n"," { 0.000541,  0.001501, -0.000375,  0.000752},\n"," {-0.00146 ,  0.000381,  0.000359,  0.00034 },\n"," { 0.001397,  0.000793, -0.000713,  0.000632},\n"," {-0.000134, -0.000561,  0.000073, -0.001736},\n"," { 0.000215,  0.001014,  0.000013,  0.00068 },\n"," { 0.001006,  0.000986, -0.001194,  0.000863},\n"," {-0.001906, -0.000084,  0.000534,  0.000315},\n"," {-0.00022 , -0.00065 , -0.000483, -0.001152},\n"," {-0.001381, -0.000598,  0.000337, -0.000319},\n"," { 0.001401,  0.001095, -0.000465,  0.000049},\n"," { 0.000554,  0.001066, -0.000697,  0.000865},\n"," { 0.000944,  0.001373, -0.001346,  0.000906},\n"," {-0.000277, -0.000495,  0.000336, -0.001627},\n"," { 0.000839, -0.00025 ,  0.001143, -0.000162},\n"," { 0.000952,  0.001516, -0.00007 ,  0.000295},\n"," {-0.000373, -0.000127, -0.000006, -0.001519},\n"," {-0.000508, -0.000119, -0.000619, -0.001052},\n"," { 0.001185,  0.000713, -0.001052,  0.000722},\n"," {-0.000116,  0.000717,  0.000911,  0.000786},\n"," { 0.000757,  0.001133, -0.00085 ,  0.000958},\n"," {-0.001884, -0.000513,  0.000322, -0.000294},\n"," {-0.001533, -0.000305,  0.000498,  0.000046},\n"," { 0.001401,  0.000807, -0.000809,  0.000579},\n"," { 0.000997,  0.001154, -0.00097 ,  0.000797},\n"," { 0.001182, -0.000036,  0.001521,  0.000156},\n"," {-0.001535,  0.00039 ,  0.000363,  0.000356},\n"," {-0.000834, -0.00014 , -0.000598, -0.000537},\n"," {-0.001335, -0.000645,  0.000295, -0.000362},\n"," {-0.001354, -0.0006  ,  0.000338, -0.000311},\n"," {-0.00146 , -0.000462,  0.000127, -0.000182},\n"," {-0.001368, -0.000465,  0.000503, -0.000488},\n"," {-0.000511, -0.000124, -0.000647, -0.001044},\n"," { 0.000889, -0.000365,  0.001318,  0.000334},\n"," {-0.001134, -0.00044 ,  0.000375, -0.000481},\n"," {-0.001381, -0.000684,  0.000415, -0.000344},\n"," {-0.0019  , -0.000564,  0.000312, -0.000312},\n"," {-0.001226, -0.000501,  0.000417, -0.000505},\n"," { 0.000896,  0.000702, -0.000343,  0.000407},\n"," { 0.000396,  0.001018, -0.000063,  0.00061 },\n"," { 0.000375,  0.000356,  0.000977,  0.000003},\n"," {-0.000264, -0.000374,  0.00025 , -0.001529},\n"," {-0.00054 ,  0.000076, -0.000306, -0.001565},\n"," { 0.001063,  0.000325, -0.000255,  0.000435},\n"," { 0.001104,  0.00105 , -0.000569,  0.000495},\n"," { 0.001038, -0.001409,  0.000795, -0.0007  },\n"," { 0.000799,  0.000609,  0.000754,  0.000569},\n"," { 0.000784,  0.000099,  0.001007, -0.000037},\n"," {-0.001096, -0.000474,  0.000306, -0.000538},\n"," { 0.00018 ,  0.000563,  0.001331,  0.000254},\n"," {-0.001426, -0.00058 ,  0.000325, -0.000337},\n"," {-0.001441,  0.000311,  0.000364,  0.000348},\n"," { 0.001192, -0.000479,  0.001427, -0.00016 },\n"," {-0.001004,  0.000071, -0.000049, -0.001689},\n"," {-0.001239, -0.000496,  0.000397, -0.000507},\n"," {-0.001459, -0.000603,  0.000424, -0.000323},\n"," {-0.00056 , -0.000155, -0.000627, -0.001216},\n"," {-0.001847, -0.00052 ,  0.000286, -0.000264},\n"," {-0.001082, -0.000228, -0.000172,  0.000006},\n"," {-0.001462, -0.000039,  0.000148, -0.000205},\n"," { 0.000888, -0.001284,  0.000967, -0.000542},\n"," { 0.000841,  0.000967, -0.001047,  0.00085 },\n"," { 0.000555,  0.000185, -0.001154,  0.000163},\n"," {-0.00133 , -0.000642,  0.000294, -0.000361},\n"," {-0.00045 ,  0.000805,  0.000002,  0.000228},\n"," {-0.000531, -0.000138, -0.000647, -0.001054},\n"," { 0.000442,  0.001068, -0.000088,  0.000642},\n"," {-0.001525,  0.000262,  0.000265,  0.000427},\n"," {-0.000615, -0.000131, -0.000274, -0.00127 },\n"," { 0.000484,  0.000893, -0.000112,  0.00071 },\n"," {-0.001377, -0.000591,  0.000332, -0.00032 },\n"," {-0.000155, -0.000713, -0.000525, -0.001086},\n"," {-0.00025 ,  0.00096 ,  0.000028,  0.000132},\n"," {-0.001243, -0.000506,  0.000378, -0.0005  },\n"," { 0.00068 , -0.001064,  0.000893, -0.000698},\n"," {-0.001245, -0.000496,  0.000392, -0.000508},\n"," { 0.001211, -0.000075,  0.001692,  0.000598},\n"," { 0.000376,  0.000835, -0.000427,  0.000967},\n"," {-0.002016,  0.000361,  0.000498,  0.000272},\n"," {-0.001578, -0.000125,  0.000049, -0.000121},\n"," { 0.000419,  0.000831, -0.001742,  0.000106},\n"," { 0.000397, -0.000088,  0.001435, -0.001025},\n"," {-0.001432, -0.000072,  0.000143, -0.00019 },\n"," { 0.000707,  0.000991, -0.000755,  0.000749},\n"," {-0.002109, -0.000865,  0.001684, -0.000365},\n"," { 0.000466,  0.000062,  0.001143, -0.000268},\n"," { 0.000732,  0.001091, -0.000555,  0.000787},\n"," {-0.001359, -0.000592,  0.000334, -0.000314},\n"," { 0.000473,  0.000095,  0.001124, -0.000514},\n"," {-0.001389, -0.000584,  0.000328, -0.000325},\n"," { 0.000453,  0.001245, -0.000938,  0.000967},\n"," { 0.001158,  0.000691, -0.001003,  0.000725},\n"," { 0.000394,  0.000375,  0.000958,  0.000011},\n"," {-0.001389, -0.000445,  0.00049 , -0.000501},\n"," { 0.00118 ,  0.000704, -0.00102 ,  0.000741},\n"," {-0.000092,  0.00032 ,  0.001175,  0.00038 },\n"," {-0.001151, -0.000413,  0.000347, -0.000482},\n"," { 0.001181,  0.000777, -0.00071 ,  0.000958},\n"," { 0.000706,  0.000567,  0.000809,  0.000649},\n"," { 0.000645, -0.000297,  0.00129 ,  0.000255},\n"," { 0.000913,  0.000499, -0.000051,  0.00036 },\n"," {-0.001451, -0.000678,  0.000077, -0.00021 },\n"," { 0.000641,  0.000729, -0.000638,  0.000659},\n"," { 0.001198,  0.000641, -0.000924,  0.000706}}\n"]}]},{"cell_type":"code","metadata":{"id":"-AaUeeTX6UmM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633997927957,"user_tz":-540,"elapsed":1287,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"0819b377-3a4a-45b4-b4fd-8ea2dd765903"},"source":["torch.manual_seed( 1 )\n","\n","#print(\"target\",target)\n","inputs.requires_grad = True\n","\n","#model = Net( labels )\n","model = Net2()\n","\n","num = inputs.size(0)\n","\n","#project = 'test5'\n","#json_path = 'network/' + project +'.json'\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD( model.parameters(), lr=0.01 )\n","\n","num_epochs = 300\n","\n","acc = []\n","\n","for epoch in range(num_epochs):\n","  with torch.set_grad_enabled(True):\n","\n","    model.train()   # モデルを訓練モードに設定\n","\n","    outputs = model( inputs )\n","    #print(outputs)\n","    #print(labels)\n","    #print(\"input grad\",inputs.grad)\n","\n","    loss = criterion( outputs, labels )\n","    print(\"loss \",epoch, \" - \",loss)\n","\n","    # ラベルを予測\n","    #print(\"output\", outputs, num )\n","    _, preds = torch.max( outputs, 1 )\n","    #print(labels)\n","    #print(preds)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # イテレーション結果の計算\n","    epoch_loss = loss.item() * float(num)\n","\n","    # 正解数の合計を更新\n","    epoch_corrects = torch.sum( preds == labels )\n","\n","    epoch_loss = epoch_loss / float(num)\n","    epoch_acc  = epoch_corrects.double() / float(num)\n","    print('Train Loss {}: {:.4f} Acc: {:.4f} {}'.format( epoch, epoch_loss, epoch_acc, epoch_corrects ))\n","\n","    acc.append( epoch_acc )"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loss  0  -  tensor(1.1529, grad_fn=<NllLossBackward>)\n","Train Loss 0: 1.1529 Acc: 0.1518 17\n","loss  1  -  tensor(1.1285, grad_fn=<NllLossBackward>)\n","Train Loss 1: 1.1285 Acc: 0.1696 19\n","loss  2  -  tensor(1.1051, grad_fn=<NllLossBackward>)\n","Train Loss 2: 1.1051 Acc: 0.1786 20\n","loss  3  -  tensor(1.0826, grad_fn=<NllLossBackward>)\n","Train Loss 3: 1.0826 Acc: 0.1696 19\n","loss  4  -  tensor(1.0610, grad_fn=<NllLossBackward>)\n","Train Loss 4: 1.0610 Acc: 0.1786 20\n","loss  5  -  tensor(1.0403, grad_fn=<NllLossBackward>)\n","Train Loss 5: 1.0403 Acc: 0.1964 22\n","loss  6  -  tensor(1.0205, grad_fn=<NllLossBackward>)\n","Train Loss 6: 1.0205 Acc: 0.2143 24\n","loss  7  -  tensor(1.0014, grad_fn=<NllLossBackward>)\n","Train Loss 7: 1.0014 Acc: 0.2589 29\n","loss  8  -  tensor(0.9832, grad_fn=<NllLossBackward>)\n","Train Loss 8: 0.9832 Acc: 0.3125 35\n","loss  9  -  tensor(0.9658, grad_fn=<NllLossBackward>)\n","Train Loss 9: 0.9658 Acc: 0.4107 46\n","loss  10  -  tensor(0.9490, grad_fn=<NllLossBackward>)\n","Train Loss 10: 0.9490 Acc: 0.5000 56\n","loss  11  -  tensor(0.9330, grad_fn=<NllLossBackward>)\n","Train Loss 11: 0.9330 Acc: 0.5357 60\n","loss  12  -  tensor(0.9177, grad_fn=<NllLossBackward>)\n","Train Loss 12: 0.9177 Acc: 0.5446 61\n","loss  13  -  tensor(0.9030, grad_fn=<NllLossBackward>)\n","Train Loss 13: 0.9030 Acc: 0.5804 65\n","loss  14  -  tensor(0.8889, grad_fn=<NllLossBackward>)\n","Train Loss 14: 0.8889 Acc: 0.6161 69\n","loss  15  -  tensor(0.8754, grad_fn=<NllLossBackward>)\n","Train Loss 15: 0.8754 Acc: 0.6696 75\n","loss  16  -  tensor(0.8624, grad_fn=<NllLossBackward>)\n","Train Loss 16: 0.8624 Acc: 0.6786 76\n","loss  17  -  tensor(0.8500, grad_fn=<NllLossBackward>)\n","Train Loss 17: 0.8500 Acc: 0.6786 76\n","loss  18  -  tensor(0.8381, grad_fn=<NllLossBackward>)\n","Train Loss 18: 0.8381 Acc: 0.6786 76\n","loss  19  -  tensor(0.8267, grad_fn=<NllLossBackward>)\n","Train Loss 19: 0.8267 Acc: 0.6786 76\n","loss  20  -  tensor(0.8158, grad_fn=<NllLossBackward>)\n","Train Loss 20: 0.8158 Acc: 0.6786 76\n","loss  21  -  tensor(0.8053, grad_fn=<NllLossBackward>)\n","Train Loss 21: 0.8053 Acc: 0.6786 76\n","loss  22  -  tensor(0.7952, grad_fn=<NllLossBackward>)\n","Train Loss 22: 0.7952 Acc: 0.6875 77\n","loss  23  -  tensor(0.7855, grad_fn=<NllLossBackward>)\n","Train Loss 23: 0.7855 Acc: 0.6875 77\n","loss  24  -  tensor(0.7761, grad_fn=<NllLossBackward>)\n","Train Loss 24: 0.7761 Acc: 0.6875 77\n","loss  25  -  tensor(0.7672, grad_fn=<NllLossBackward>)\n","Train Loss 25: 0.7672 Acc: 0.6875 77\n","loss  26  -  tensor(0.7585, grad_fn=<NllLossBackward>)\n","Train Loss 26: 0.7585 Acc: 0.6875 77\n","loss  27  -  tensor(0.7502, grad_fn=<NllLossBackward>)\n","Train Loss 27: 0.7502 Acc: 0.6964 78\n","loss  28  -  tensor(0.7422, grad_fn=<NllLossBackward>)\n","Train Loss 28: 0.7422 Acc: 0.6964 78\n","loss  29  -  tensor(0.7345, grad_fn=<NllLossBackward>)\n","Train Loss 29: 0.7345 Acc: 0.7054 79\n","loss  30  -  tensor(0.7270, grad_fn=<NllLossBackward>)\n","Train Loss 30: 0.7270 Acc: 0.7054 79\n","loss  31  -  tensor(0.7198, grad_fn=<NllLossBackward>)\n","Train Loss 31: 0.7198 Acc: 0.7054 79\n","loss  32  -  tensor(0.7128, grad_fn=<NllLossBackward>)\n","Train Loss 32: 0.7128 Acc: 0.7054 79\n","loss  33  -  tensor(0.7061, grad_fn=<NllLossBackward>)\n","Train Loss 33: 0.7061 Acc: 0.7143 80\n","loss  34  -  tensor(0.6996, grad_fn=<NllLossBackward>)\n","Train Loss 34: 0.6996 Acc: 0.7232 81\n","loss  35  -  tensor(0.6933, grad_fn=<NllLossBackward>)\n","Train Loss 35: 0.6933 Acc: 0.7232 81\n","loss  36  -  tensor(0.6873, grad_fn=<NllLossBackward>)\n","Train Loss 36: 0.6873 Acc: 0.7232 81\n","loss  37  -  tensor(0.6814, grad_fn=<NllLossBackward>)\n","Train Loss 37: 0.6814 Acc: 0.7232 81\n","loss  38  -  tensor(0.6757, grad_fn=<NllLossBackward>)\n","Train Loss 38: 0.6757 Acc: 0.7232 81\n","loss  39  -  tensor(0.6701, grad_fn=<NllLossBackward>)\n","Train Loss 39: 0.6701 Acc: 0.7232 81\n","loss  40  -  tensor(0.6648, grad_fn=<NllLossBackward>)\n","Train Loss 40: 0.6648 Acc: 0.7321 82\n","loss  41  -  tensor(0.6595, grad_fn=<NllLossBackward>)\n","Train Loss 41: 0.6595 Acc: 0.7321 82\n","loss  42  -  tensor(0.6545, grad_fn=<NllLossBackward>)\n","Train Loss 42: 0.6545 Acc: 0.7321 82\n","loss  43  -  tensor(0.6495, grad_fn=<NllLossBackward>)\n","Train Loss 43: 0.6495 Acc: 0.7411 83\n","loss  44  -  tensor(0.6448, grad_fn=<NllLossBackward>)\n","Train Loss 44: 0.6448 Acc: 0.7411 83\n","loss  45  -  tensor(0.6401, grad_fn=<NllLossBackward>)\n","Train Loss 45: 0.6401 Acc: 0.7500 84\n","loss  46  -  tensor(0.6356, grad_fn=<NllLossBackward>)\n","Train Loss 46: 0.6356 Acc: 0.7500 84\n","loss  47  -  tensor(0.6312, grad_fn=<NllLossBackward>)\n","Train Loss 47: 0.6312 Acc: 0.7500 84\n","loss  48  -  tensor(0.6269, grad_fn=<NllLossBackward>)\n","Train Loss 48: 0.6269 Acc: 0.7589 85\n","loss  49  -  tensor(0.6227, grad_fn=<NllLossBackward>)\n","Train Loss 49: 0.6227 Acc: 0.7589 85\n","loss  50  -  tensor(0.6186, grad_fn=<NllLossBackward>)\n","Train Loss 50: 0.6186 Acc: 0.7589 85\n","loss  51  -  tensor(0.6146, grad_fn=<NllLossBackward>)\n","Train Loss 51: 0.6146 Acc: 0.7589 85\n","loss  52  -  tensor(0.6107, grad_fn=<NllLossBackward>)\n","Train Loss 52: 0.6107 Acc: 0.7589 85\n","loss  53  -  tensor(0.6069, grad_fn=<NllLossBackward>)\n","Train Loss 53: 0.6069 Acc: 0.7589 85\n","loss  54  -  tensor(0.6032, grad_fn=<NllLossBackward>)\n","Train Loss 54: 0.6032 Acc: 0.7589 85\n","loss  55  -  tensor(0.5995, grad_fn=<NllLossBackward>)\n","Train Loss 55: 0.5995 Acc: 0.7679 86\n","loss  56  -  tensor(0.5960, grad_fn=<NllLossBackward>)\n","Train Loss 56: 0.5960 Acc: 0.7679 86\n","loss  57  -  tensor(0.5925, grad_fn=<NllLossBackward>)\n","Train Loss 57: 0.5925 Acc: 0.7679 86\n","loss  58  -  tensor(0.5891, grad_fn=<NllLossBackward>)\n","Train Loss 58: 0.5891 Acc: 0.7679 86\n","loss  59  -  tensor(0.5858, grad_fn=<NllLossBackward>)\n","Train Loss 59: 0.5858 Acc: 0.7679 86\n","loss  60  -  tensor(0.5825, grad_fn=<NllLossBackward>)\n","Train Loss 60: 0.5825 Acc: 0.7679 86\n","loss  61  -  tensor(0.5793, grad_fn=<NllLossBackward>)\n","Train Loss 61: 0.5793 Acc: 0.7679 86\n","loss  62  -  tensor(0.5762, grad_fn=<NllLossBackward>)\n","Train Loss 62: 0.5762 Acc: 0.7679 86\n","loss  63  -  tensor(0.5731, grad_fn=<NllLossBackward>)\n","Train Loss 63: 0.5731 Acc: 0.7679 86\n","loss  64  -  tensor(0.5701, grad_fn=<NllLossBackward>)\n","Train Loss 64: 0.5701 Acc: 0.7679 86\n","loss  65  -  tensor(0.5672, grad_fn=<NllLossBackward>)\n","Train Loss 65: 0.5672 Acc: 0.7679 86\n","loss  66  -  tensor(0.5643, grad_fn=<NllLossBackward>)\n","Train Loss 66: 0.5643 Acc: 0.7768 87\n","loss  67  -  tensor(0.5614, grad_fn=<NllLossBackward>)\n","Train Loss 67: 0.5614 Acc: 0.7768 87\n","loss  68  -  tensor(0.5586, grad_fn=<NllLossBackward>)\n","Train Loss 68: 0.5586 Acc: 0.7768 87\n","loss  69  -  tensor(0.5559, grad_fn=<NllLossBackward>)\n","Train Loss 69: 0.5559 Acc: 0.7768 87\n","loss  70  -  tensor(0.5532, grad_fn=<NllLossBackward>)\n","Train Loss 70: 0.5532 Acc: 0.7857 88\n","loss  71  -  tensor(0.5506, grad_fn=<NllLossBackward>)\n","Train Loss 71: 0.5506 Acc: 0.7857 88\n","loss  72  -  tensor(0.5479, grad_fn=<NllLossBackward>)\n","Train Loss 72: 0.5479 Acc: 0.7946 89\n","loss  73  -  tensor(0.5454, grad_fn=<NllLossBackward>)\n","Train Loss 73: 0.5454 Acc: 0.7946 89\n","loss  74  -  tensor(0.5429, grad_fn=<NllLossBackward>)\n","Train Loss 74: 0.5429 Acc: 0.7946 89\n","loss  75  -  tensor(0.5404, grad_fn=<NllLossBackward>)\n","Train Loss 75: 0.5404 Acc: 0.7946 89\n","loss  76  -  tensor(0.5380, grad_fn=<NllLossBackward>)\n","Train Loss 76: 0.5380 Acc: 0.7946 89\n","loss  77  -  tensor(0.5356, grad_fn=<NllLossBackward>)\n","Train Loss 77: 0.5356 Acc: 0.7946 89\n","loss  78  -  tensor(0.5332, grad_fn=<NllLossBackward>)\n","Train Loss 78: 0.5332 Acc: 0.7946 89\n","loss  79  -  tensor(0.5309, grad_fn=<NllLossBackward>)\n","Train Loss 79: 0.5309 Acc: 0.7946 89\n","loss  80  -  tensor(0.5286, grad_fn=<NllLossBackward>)\n","Train Loss 80: 0.5286 Acc: 0.7946 89\n","loss  81  -  tensor(0.5264, grad_fn=<NllLossBackward>)\n","Train Loss 81: 0.5264 Acc: 0.8036 90\n","loss  82  -  tensor(0.5241, grad_fn=<NllLossBackward>)\n","Train Loss 82: 0.5241 Acc: 0.8036 90\n","loss  83  -  tensor(0.5220, grad_fn=<NllLossBackward>)\n","Train Loss 83: 0.5220 Acc: 0.8036 90\n","loss  84  -  tensor(0.5198, grad_fn=<NllLossBackward>)\n","Train Loss 84: 0.5198 Acc: 0.8036 90\n","loss  85  -  tensor(0.5177, grad_fn=<NllLossBackward>)\n","Train Loss 85: 0.5177 Acc: 0.8036 90\n","loss  86  -  tensor(0.5156, grad_fn=<NllLossBackward>)\n","Train Loss 86: 0.5156 Acc: 0.8036 90\n","loss  87  -  tensor(0.5135, grad_fn=<NllLossBackward>)\n","Train Loss 87: 0.5135 Acc: 0.8125 91\n","loss  88  -  tensor(0.5115, grad_fn=<NllLossBackward>)\n","Train Loss 88: 0.5115 Acc: 0.8125 91\n","loss  89  -  tensor(0.5095, grad_fn=<NllLossBackward>)\n","Train Loss 89: 0.5095 Acc: 0.8125 91\n","loss  90  -  tensor(0.5075, grad_fn=<NllLossBackward>)\n","Train Loss 90: 0.5075 Acc: 0.8125 91\n","loss  91  -  tensor(0.5055, grad_fn=<NllLossBackward>)\n","Train Loss 91: 0.5055 Acc: 0.8125 91\n","loss  92  -  tensor(0.5036, grad_fn=<NllLossBackward>)\n","Train Loss 92: 0.5036 Acc: 0.8125 91\n","loss  93  -  tensor(0.5017, grad_fn=<NllLossBackward>)\n","Train Loss 93: 0.5017 Acc: 0.8125 91\n","loss  94  -  tensor(0.4998, grad_fn=<NllLossBackward>)\n","Train Loss 94: 0.4998 Acc: 0.8125 91\n","loss  95  -  tensor(0.4980, grad_fn=<NllLossBackward>)\n","Train Loss 95: 0.4980 Acc: 0.8214 92\n","loss  96  -  tensor(0.4962, grad_fn=<NllLossBackward>)\n","Train Loss 96: 0.4962 Acc: 0.8214 92\n","loss  97  -  tensor(0.4943, grad_fn=<NllLossBackward>)\n","Train Loss 97: 0.4943 Acc: 0.8214 92\n","loss  98  -  tensor(0.4926, grad_fn=<NllLossBackward>)\n","Train Loss 98: 0.4926 Acc: 0.8214 92\n","loss  99  -  tensor(0.4908, grad_fn=<NllLossBackward>)\n","Train Loss 99: 0.4908 Acc: 0.8214 92\n","loss  100  -  tensor(0.4891, grad_fn=<NllLossBackward>)\n","Train Loss 100: 0.4891 Acc: 0.8214 92\n","loss  101  -  tensor(0.4873, grad_fn=<NllLossBackward>)\n","Train Loss 101: 0.4873 Acc: 0.8304 93\n","loss  102  -  tensor(0.4856, grad_fn=<NllLossBackward>)\n","Train Loss 102: 0.4856 Acc: 0.8393 94\n","loss  103  -  tensor(0.4840, grad_fn=<NllLossBackward>)\n","Train Loss 103: 0.4840 Acc: 0.8393 94\n","loss  104  -  tensor(0.4823, grad_fn=<NllLossBackward>)\n","Train Loss 104: 0.4823 Acc: 0.8482 95\n","loss  105  -  tensor(0.4807, grad_fn=<NllLossBackward>)\n","Train Loss 105: 0.4807 Acc: 0.8482 95\n","loss  106  -  tensor(0.4790, grad_fn=<NllLossBackward>)\n","Train Loss 106: 0.4790 Acc: 0.8482 95\n","loss  107  -  tensor(0.4774, grad_fn=<NllLossBackward>)\n","Train Loss 107: 0.4774 Acc: 0.8482 95\n","loss  108  -  tensor(0.4758, grad_fn=<NllLossBackward>)\n","Train Loss 108: 0.4758 Acc: 0.8482 95\n","loss  109  -  tensor(0.4743, grad_fn=<NllLossBackward>)\n","Train Loss 109: 0.4743 Acc: 0.8482 95\n","loss  110  -  tensor(0.4727, grad_fn=<NllLossBackward>)\n","Train Loss 110: 0.4727 Acc: 0.8482 95\n","loss  111  -  tensor(0.4712, grad_fn=<NllLossBackward>)\n","Train Loss 111: 0.4712 Acc: 0.8482 95\n","loss  112  -  tensor(0.4697, grad_fn=<NllLossBackward>)\n","Train Loss 112: 0.4697 Acc: 0.8393 94\n","loss  113  -  tensor(0.4682, grad_fn=<NllLossBackward>)\n","Train Loss 113: 0.4682 Acc: 0.8393 94\n","loss  114  -  tensor(0.4667, grad_fn=<NllLossBackward>)\n","Train Loss 114: 0.4667 Acc: 0.8393 94\n","loss  115  -  tensor(0.4652, grad_fn=<NllLossBackward>)\n","Train Loss 115: 0.4652 Acc: 0.8393 94\n","loss  116  -  tensor(0.4638, grad_fn=<NllLossBackward>)\n","Train Loss 116: 0.4638 Acc: 0.8393 94\n","loss  117  -  tensor(0.4623, grad_fn=<NllLossBackward>)\n","Train Loss 117: 0.4623 Acc: 0.8393 94\n","loss  118  -  tensor(0.4609, grad_fn=<NllLossBackward>)\n","Train Loss 118: 0.4609 Acc: 0.8393 94\n","loss  119  -  tensor(0.4595, grad_fn=<NllLossBackward>)\n","Train Loss 119: 0.4595 Acc: 0.8393 94\n","loss  120  -  tensor(0.4581, grad_fn=<NllLossBackward>)\n","Train Loss 120: 0.4581 Acc: 0.8393 94\n","loss  121  -  tensor(0.4567, grad_fn=<NllLossBackward>)\n","Train Loss 121: 0.4567 Acc: 0.8393 94\n","loss  122  -  tensor(0.4554, grad_fn=<NllLossBackward>)\n","Train Loss 122: 0.4554 Acc: 0.8393 94\n","loss  123  -  tensor(0.4540, grad_fn=<NllLossBackward>)\n","Train Loss 123: 0.4540 Acc: 0.8393 94\n","loss  124  -  tensor(0.4527, grad_fn=<NllLossBackward>)\n","Train Loss 124: 0.4527 Acc: 0.8393 94\n","loss  125  -  tensor(0.4514, grad_fn=<NllLossBackward>)\n","Train Loss 125: 0.4514 Acc: 0.8393 94\n","loss  126  -  tensor(0.4500, grad_fn=<NllLossBackward>)\n","Train Loss 126: 0.4500 Acc: 0.8393 94\n","loss  127  -  tensor(0.4487, grad_fn=<NllLossBackward>)\n","Train Loss 127: 0.4487 Acc: 0.8393 94\n","loss  128  -  tensor(0.4475, grad_fn=<NllLossBackward>)\n","Train Loss 128: 0.4475 Acc: 0.8393 94\n","loss  129  -  tensor(0.4462, grad_fn=<NllLossBackward>)\n","Train Loss 129: 0.4462 Acc: 0.8393 94\n","loss  130  -  tensor(0.4449, grad_fn=<NllLossBackward>)\n","Train Loss 130: 0.4449 Acc: 0.8393 94\n","loss  131  -  tensor(0.4437, grad_fn=<NllLossBackward>)\n","Train Loss 131: 0.4437 Acc: 0.8393 94\n","loss  132  -  tensor(0.4424, grad_fn=<NllLossBackward>)\n","Train Loss 132: 0.4424 Acc: 0.8393 94\n","loss  133  -  tensor(0.4412, grad_fn=<NllLossBackward>)\n","Train Loss 133: 0.4412 Acc: 0.8393 94\n","loss  134  -  tensor(0.4400, grad_fn=<NllLossBackward>)\n","Train Loss 134: 0.4400 Acc: 0.8393 94\n","loss  135  -  tensor(0.4388, grad_fn=<NllLossBackward>)\n","Train Loss 135: 0.4388 Acc: 0.8393 94\n","loss  136  -  tensor(0.4376, grad_fn=<NllLossBackward>)\n","Train Loss 136: 0.4376 Acc: 0.8393 94\n","loss  137  -  tensor(0.4364, grad_fn=<NllLossBackward>)\n","Train Loss 137: 0.4364 Acc: 0.8393 94\n","loss  138  -  tensor(0.4353, grad_fn=<NllLossBackward>)\n","Train Loss 138: 0.4353 Acc: 0.8393 94\n","loss  139  -  tensor(0.4341, grad_fn=<NllLossBackward>)\n","Train Loss 139: 0.4341 Acc: 0.8393 94\n","loss  140  -  tensor(0.4330, grad_fn=<NllLossBackward>)\n","Train Loss 140: 0.4330 Acc: 0.8393 94\n","loss  141  -  tensor(0.4318, grad_fn=<NllLossBackward>)\n","Train Loss 141: 0.4318 Acc: 0.8393 94\n","loss  142  -  tensor(0.4307, grad_fn=<NllLossBackward>)\n","Train Loss 142: 0.4307 Acc: 0.8393 94\n","loss  143  -  tensor(0.4296, grad_fn=<NllLossBackward>)\n","Train Loss 143: 0.4296 Acc: 0.8393 94\n","loss  144  -  tensor(0.4285, grad_fn=<NllLossBackward>)\n","Train Loss 144: 0.4285 Acc: 0.8393 94\n","loss  145  -  tensor(0.4274, grad_fn=<NllLossBackward>)\n","Train Loss 145: 0.4274 Acc: 0.8393 94\n","loss  146  -  tensor(0.4263, grad_fn=<NllLossBackward>)\n","Train Loss 146: 0.4263 Acc: 0.8393 94\n","loss  147  -  tensor(0.4252, grad_fn=<NllLossBackward>)\n","Train Loss 147: 0.4252 Acc: 0.8482 95\n","loss  148  -  tensor(0.4241, grad_fn=<NllLossBackward>)\n","Train Loss 148: 0.4241 Acc: 0.8482 95\n","loss  149  -  tensor(0.4231, grad_fn=<NllLossBackward>)\n","Train Loss 149: 0.4231 Acc: 0.8482 95\n","loss  150  -  tensor(0.4220, grad_fn=<NllLossBackward>)\n","Train Loss 150: 0.4220 Acc: 0.8482 95\n","loss  151  -  tensor(0.4210, grad_fn=<NllLossBackward>)\n","Train Loss 151: 0.4210 Acc: 0.8482 95\n","loss  152  -  tensor(0.4199, grad_fn=<NllLossBackward>)\n","Train Loss 152: 0.4199 Acc: 0.8571 96\n","loss  153  -  tensor(0.4189, grad_fn=<NllLossBackward>)\n","Train Loss 153: 0.4189 Acc: 0.8571 96\n","loss  154  -  tensor(0.4179, grad_fn=<NllLossBackward>)\n","Train Loss 154: 0.4179 Acc: 0.8571 96\n","loss  155  -  tensor(0.4169, grad_fn=<NllLossBackward>)\n","Train Loss 155: 0.4169 Acc: 0.8571 96\n","loss  156  -  tensor(0.4159, grad_fn=<NllLossBackward>)\n","Train Loss 156: 0.4159 Acc: 0.8571 96\n","loss  157  -  tensor(0.4149, grad_fn=<NllLossBackward>)\n","Train Loss 157: 0.4149 Acc: 0.8571 96\n","loss  158  -  tensor(0.4139, grad_fn=<NllLossBackward>)\n","Train Loss 158: 0.4139 Acc: 0.8571 96\n","loss  159  -  tensor(0.4129, grad_fn=<NllLossBackward>)\n","Train Loss 159: 0.4129 Acc: 0.8571 96\n","loss  160  -  tensor(0.4120, grad_fn=<NllLossBackward>)\n","Train Loss 160: 0.4120 Acc: 0.8571 96\n","loss  161  -  tensor(0.4110, grad_fn=<NllLossBackward>)\n","Train Loss 161: 0.4110 Acc: 0.8571 96\n","loss  162  -  tensor(0.4101, grad_fn=<NllLossBackward>)\n","Train Loss 162: 0.4101 Acc: 0.8571 96\n","loss  163  -  tensor(0.4091, grad_fn=<NllLossBackward>)\n","Train Loss 163: 0.4091 Acc: 0.8571 96\n","loss  164  -  tensor(0.4082, grad_fn=<NllLossBackward>)\n","Train Loss 164: 0.4082 Acc: 0.8571 96\n","loss  165  -  tensor(0.4072, grad_fn=<NllLossBackward>)\n","Train Loss 165: 0.4072 Acc: 0.8571 96\n","loss  166  -  tensor(0.4063, grad_fn=<NllLossBackward>)\n","Train Loss 166: 0.4063 Acc: 0.8571 96\n","loss  167  -  tensor(0.4054, grad_fn=<NllLossBackward>)\n","Train Loss 167: 0.4054 Acc: 0.8571 96\n","loss  168  -  tensor(0.4045, grad_fn=<NllLossBackward>)\n","Train Loss 168: 0.4045 Acc: 0.8571 96\n","loss  169  -  tensor(0.4036, grad_fn=<NllLossBackward>)\n","Train Loss 169: 0.4036 Acc: 0.8571 96\n","loss  170  -  tensor(0.4027, grad_fn=<NllLossBackward>)\n","Train Loss 170: 0.4027 Acc: 0.8571 96\n","loss  171  -  tensor(0.4018, grad_fn=<NllLossBackward>)\n","Train Loss 171: 0.4018 Acc: 0.8571 96\n","loss  172  -  tensor(0.4009, grad_fn=<NllLossBackward>)\n","Train Loss 172: 0.4009 Acc: 0.8571 96\n","loss  173  -  tensor(0.4000, grad_fn=<NllLossBackward>)\n","Train Loss 173: 0.4000 Acc: 0.8571 96\n","loss  174  -  tensor(0.3992, grad_fn=<NllLossBackward>)\n","Train Loss 174: 0.3992 Acc: 0.8571 96\n","loss  175  -  tensor(0.3983, grad_fn=<NllLossBackward>)\n","Train Loss 175: 0.3983 Acc: 0.8571 96\n","loss  176  -  tensor(0.3975, grad_fn=<NllLossBackward>)\n","Train Loss 176: 0.3975 Acc: 0.8571 96\n","loss  177  -  tensor(0.3966, grad_fn=<NllLossBackward>)\n","Train Loss 177: 0.3966 Acc: 0.8571 96\n","loss  178  -  tensor(0.3958, grad_fn=<NllLossBackward>)\n","Train Loss 178: 0.3958 Acc: 0.8571 96\n","loss  179  -  tensor(0.3949, grad_fn=<NllLossBackward>)\n","Train Loss 179: 0.3949 Acc: 0.8571 96\n","loss  180  -  tensor(0.3941, grad_fn=<NllLossBackward>)\n","Train Loss 180: 0.3941 Acc: 0.8571 96\n","loss  181  -  tensor(0.3933, grad_fn=<NllLossBackward>)\n","Train Loss 181: 0.3933 Acc: 0.8571 96\n","loss  182  -  tensor(0.3925, grad_fn=<NllLossBackward>)\n","Train Loss 182: 0.3925 Acc: 0.8571 96\n","loss  183  -  tensor(0.3916, grad_fn=<NllLossBackward>)\n","Train Loss 183: 0.3916 Acc: 0.8571 96\n","loss  184  -  tensor(0.3908, grad_fn=<NllLossBackward>)\n","Train Loss 184: 0.3908 Acc: 0.8571 96\n","loss  185  -  tensor(0.3900, grad_fn=<NllLossBackward>)\n","Train Loss 185: 0.3900 Acc: 0.8571 96\n","loss  186  -  tensor(0.3892, grad_fn=<NllLossBackward>)\n","Train Loss 186: 0.3892 Acc: 0.8661 97\n","loss  187  -  tensor(0.3884, grad_fn=<NllLossBackward>)\n","Train Loss 187: 0.3884 Acc: 0.8661 97\n","loss  188  -  tensor(0.3877, grad_fn=<NllLossBackward>)\n","Train Loss 188: 0.3877 Acc: 0.8661 97\n","loss  189  -  tensor(0.3869, grad_fn=<NllLossBackward>)\n","Train Loss 189: 0.3869 Acc: 0.8661 97\n","loss  190  -  tensor(0.3861, grad_fn=<NllLossBackward>)\n","Train Loss 190: 0.3861 Acc: 0.8661 97\n","loss  191  -  tensor(0.3853, grad_fn=<NllLossBackward>)\n","Train Loss 191: 0.3853 Acc: 0.8661 97\n","loss  192  -  tensor(0.3846, grad_fn=<NllLossBackward>)\n","Train Loss 192: 0.3846 Acc: 0.8661 97\n","loss  193  -  tensor(0.3838, grad_fn=<NllLossBackward>)\n","Train Loss 193: 0.3838 Acc: 0.8661 97\n","loss  194  -  tensor(0.3831, grad_fn=<NllLossBackward>)\n","Train Loss 194: 0.3831 Acc: 0.8661 97\n","loss  195  -  tensor(0.3823, grad_fn=<NllLossBackward>)\n","Train Loss 195: 0.3823 Acc: 0.8661 97\n","loss  196  -  tensor(0.3816, grad_fn=<NllLossBackward>)\n","Train Loss 196: 0.3816 Acc: 0.8661 97\n","loss  197  -  tensor(0.3808, grad_fn=<NllLossBackward>)\n","Train Loss 197: 0.3808 Acc: 0.8661 97\n","loss  198  -  tensor(0.3801, grad_fn=<NllLossBackward>)\n","Train Loss 198: 0.3801 Acc: 0.8661 97\n","loss  199  -  tensor(0.3794, grad_fn=<NllLossBackward>)\n","Train Loss 199: 0.3794 Acc: 0.8661 97\n","loss  200  -  tensor(0.3787, grad_fn=<NllLossBackward>)\n","Train Loss 200: 0.3787 Acc: 0.8750 98\n","loss  201  -  tensor(0.3779, grad_fn=<NllLossBackward>)\n","Train Loss 201: 0.3779 Acc: 0.8750 98\n","loss  202  -  tensor(0.3772, grad_fn=<NllLossBackward>)\n","Train Loss 202: 0.3772 Acc: 0.8750 98\n","loss  203  -  tensor(0.3765, grad_fn=<NllLossBackward>)\n","Train Loss 203: 0.3765 Acc: 0.8750 98\n","loss  204  -  tensor(0.3758, grad_fn=<NllLossBackward>)\n","Train Loss 204: 0.3758 Acc: 0.8750 98\n","loss  205  -  tensor(0.3751, grad_fn=<NllLossBackward>)\n","Train Loss 205: 0.3751 Acc: 0.8750 98\n","loss  206  -  tensor(0.3744, grad_fn=<NllLossBackward>)\n","Train Loss 206: 0.3744 Acc: 0.8750 98\n","loss  207  -  tensor(0.3737, grad_fn=<NllLossBackward>)\n","Train Loss 207: 0.3737 Acc: 0.8750 98\n","loss  208  -  tensor(0.3730, grad_fn=<NllLossBackward>)\n","Train Loss 208: 0.3730 Acc: 0.8750 98\n","loss  209  -  tensor(0.3724, grad_fn=<NllLossBackward>)\n","Train Loss 209: 0.3724 Acc: 0.8750 98\n","loss  210  -  tensor(0.3717, grad_fn=<NllLossBackward>)\n","Train Loss 210: 0.3717 Acc: 0.8750 98\n","loss  211  -  tensor(0.3710, grad_fn=<NllLossBackward>)\n","Train Loss 211: 0.3710 Acc: 0.8750 98\n","loss  212  -  tensor(0.3703, grad_fn=<NllLossBackward>)\n","Train Loss 212: 0.3703 Acc: 0.8750 98\n","loss  213  -  tensor(0.3697, grad_fn=<NllLossBackward>)\n","Train Loss 213: 0.3697 Acc: 0.8750 98\n","loss  214  -  tensor(0.3690, grad_fn=<NllLossBackward>)\n","Train Loss 214: 0.3690 Acc: 0.8750 98\n","loss  215  -  tensor(0.3684, grad_fn=<NllLossBackward>)\n","Train Loss 215: 0.3684 Acc: 0.8750 98\n","loss  216  -  tensor(0.3677, grad_fn=<NllLossBackward>)\n","Train Loss 216: 0.3677 Acc: 0.8750 98\n","loss  217  -  tensor(0.3671, grad_fn=<NllLossBackward>)\n","Train Loss 217: 0.3671 Acc: 0.8750 98\n","loss  218  -  tensor(0.3664, grad_fn=<NllLossBackward>)\n","Train Loss 218: 0.3664 Acc: 0.8750 98\n","loss  219  -  tensor(0.3658, grad_fn=<NllLossBackward>)\n","Train Loss 219: 0.3658 Acc: 0.8750 98\n","loss  220  -  tensor(0.3651, grad_fn=<NllLossBackward>)\n","Train Loss 220: 0.3651 Acc: 0.8750 98\n","loss  221  -  tensor(0.3645, grad_fn=<NllLossBackward>)\n","Train Loss 221: 0.3645 Acc: 0.8750 98\n","loss  222  -  tensor(0.3639, grad_fn=<NllLossBackward>)\n","Train Loss 222: 0.3639 Acc: 0.8750 98\n","loss  223  -  tensor(0.3632, grad_fn=<NllLossBackward>)\n","Train Loss 223: 0.3632 Acc: 0.8750 98\n","loss  224  -  tensor(0.3626, grad_fn=<NllLossBackward>)\n","Train Loss 224: 0.3626 Acc: 0.8750 98\n","loss  225  -  tensor(0.3620, grad_fn=<NllLossBackward>)\n","Train Loss 225: 0.3620 Acc: 0.8750 98\n","loss  226  -  tensor(0.3614, grad_fn=<NllLossBackward>)\n","Train Loss 226: 0.3614 Acc: 0.8750 98\n","loss  227  -  tensor(0.3608, grad_fn=<NllLossBackward>)\n","Train Loss 227: 0.3608 Acc: 0.8750 98\n","loss  228  -  tensor(0.3602, grad_fn=<NllLossBackward>)\n","Train Loss 228: 0.3602 Acc: 0.8750 98\n","loss  229  -  tensor(0.3596, grad_fn=<NllLossBackward>)\n","Train Loss 229: 0.3596 Acc: 0.8839 99\n","loss  230  -  tensor(0.3590, grad_fn=<NllLossBackward>)\n","Train Loss 230: 0.3590 Acc: 0.8839 99\n","loss  231  -  tensor(0.3584, grad_fn=<NllLossBackward>)\n","Train Loss 231: 0.3584 Acc: 0.8839 99\n","loss  232  -  tensor(0.3578, grad_fn=<NllLossBackward>)\n","Train Loss 232: 0.3578 Acc: 0.8839 99\n","loss  233  -  tensor(0.3572, grad_fn=<NllLossBackward>)\n","Train Loss 233: 0.3572 Acc: 0.8839 99\n","loss  234  -  tensor(0.3566, grad_fn=<NllLossBackward>)\n","Train Loss 234: 0.3566 Acc: 0.8839 99\n","loss  235  -  tensor(0.3560, grad_fn=<NllLossBackward>)\n","Train Loss 235: 0.3560 Acc: 0.8839 99\n","loss  236  -  tensor(0.3554, grad_fn=<NllLossBackward>)\n","Train Loss 236: 0.3554 Acc: 0.8839 99\n","loss  237  -  tensor(0.3549, grad_fn=<NllLossBackward>)\n","Train Loss 237: 0.3549 Acc: 0.8839 99\n","loss  238  -  tensor(0.3543, grad_fn=<NllLossBackward>)\n","Train Loss 238: 0.3543 Acc: 0.8839 99\n","loss  239  -  tensor(0.3537, grad_fn=<NllLossBackward>)\n","Train Loss 239: 0.3537 Acc: 0.8839 99\n","loss  240  -  tensor(0.3531, grad_fn=<NllLossBackward>)\n","Train Loss 240: 0.3531 Acc: 0.8839 99\n","loss  241  -  tensor(0.3526, grad_fn=<NllLossBackward>)\n","Train Loss 241: 0.3526 Acc: 0.8839 99\n","loss  242  -  tensor(0.3520, grad_fn=<NllLossBackward>)\n","Train Loss 242: 0.3520 Acc: 0.8839 99\n","loss  243  -  tensor(0.3515, grad_fn=<NllLossBackward>)\n","Train Loss 243: 0.3515 Acc: 0.8839 99\n","loss  244  -  tensor(0.3509, grad_fn=<NllLossBackward>)\n","Train Loss 244: 0.3509 Acc: 0.8839 99\n","loss  245  -  tensor(0.3504, grad_fn=<NllLossBackward>)\n","Train Loss 245: 0.3504 Acc: 0.8839 99\n","loss  246  -  tensor(0.3498, grad_fn=<NllLossBackward>)\n","Train Loss 246: 0.3498 Acc: 0.8839 99\n","loss  247  -  tensor(0.3493, grad_fn=<NllLossBackward>)\n","Train Loss 247: 0.3493 Acc: 0.8839 99\n","loss  248  -  tensor(0.3487, grad_fn=<NllLossBackward>)\n","Train Loss 248: 0.3487 Acc: 0.8839 99\n","loss  249  -  tensor(0.3482, grad_fn=<NllLossBackward>)\n","Train Loss 249: 0.3482 Acc: 0.8839 99\n","loss  250  -  tensor(0.3476, grad_fn=<NllLossBackward>)\n","Train Loss 250: 0.3476 Acc: 0.8839 99\n","loss  251  -  tensor(0.3471, grad_fn=<NllLossBackward>)\n","Train Loss 251: 0.3471 Acc: 0.8839 99\n","loss  252  -  tensor(0.3466, grad_fn=<NllLossBackward>)\n","Train Loss 252: 0.3466 Acc: 0.8839 99\n","loss  253  -  tensor(0.3460, grad_fn=<NllLossBackward>)\n","Train Loss 253: 0.3460 Acc: 0.8929 100\n","loss  254  -  tensor(0.3455, grad_fn=<NllLossBackward>)\n","Train Loss 254: 0.3455 Acc: 0.8929 100\n","loss  255  -  tensor(0.3450, grad_fn=<NllLossBackward>)\n","Train Loss 255: 0.3450 Acc: 0.8929 100\n","loss  256  -  tensor(0.3445, grad_fn=<NllLossBackward>)\n","Train Loss 256: 0.3445 Acc: 0.8929 100\n","loss  257  -  tensor(0.3439, grad_fn=<NllLossBackward>)\n","Train Loss 257: 0.3439 Acc: 0.8929 100\n","loss  258  -  tensor(0.3434, grad_fn=<NllLossBackward>)\n","Train Loss 258: 0.3434 Acc: 0.8929 100\n","loss  259  -  tensor(0.3429, grad_fn=<NllLossBackward>)\n","Train Loss 259: 0.3429 Acc: 0.8929 100\n","loss  260  -  tensor(0.3424, grad_fn=<NllLossBackward>)\n","Train Loss 260: 0.3424 Acc: 0.8929 100\n","loss  261  -  tensor(0.3419, grad_fn=<NllLossBackward>)\n","Train Loss 261: 0.3419 Acc: 0.8929 100\n","loss  262  -  tensor(0.3414, grad_fn=<NllLossBackward>)\n","Train Loss 262: 0.3414 Acc: 0.8929 100\n","loss  263  -  tensor(0.3409, grad_fn=<NllLossBackward>)\n","Train Loss 263: 0.3409 Acc: 0.8929 100\n","loss  264  -  tensor(0.3404, grad_fn=<NllLossBackward>)\n","Train Loss 264: 0.3404 Acc: 0.8929 100\n","loss  265  -  tensor(0.3399, grad_fn=<NllLossBackward>)\n","Train Loss 265: 0.3399 Acc: 0.8929 100\n","loss  266  -  tensor(0.3394, grad_fn=<NllLossBackward>)\n","Train Loss 266: 0.3394 Acc: 0.8929 100\n","loss  267  -  tensor(0.3389, grad_fn=<NllLossBackward>)\n","Train Loss 267: 0.3389 Acc: 0.8929 100\n","loss  268  -  tensor(0.3384, grad_fn=<NllLossBackward>)\n","Train Loss 268: 0.3384 Acc: 0.8929 100\n","loss  269  -  tensor(0.3379, grad_fn=<NllLossBackward>)\n","Train Loss 269: 0.3379 Acc: 0.8929 100\n","loss  270  -  tensor(0.3374, grad_fn=<NllLossBackward>)\n","Train Loss 270: 0.3374 Acc: 0.8929 100\n","loss  271  -  tensor(0.3369, grad_fn=<NllLossBackward>)\n","Train Loss 271: 0.3369 Acc: 0.8929 100\n","loss  272  -  tensor(0.3364, grad_fn=<NllLossBackward>)\n","Train Loss 272: 0.3364 Acc: 0.8929 100\n","loss  273  -  tensor(0.3360, grad_fn=<NllLossBackward>)\n","Train Loss 273: 0.3360 Acc: 0.8929 100\n","loss  274  -  tensor(0.3355, grad_fn=<NllLossBackward>)\n","Train Loss 274: 0.3355 Acc: 0.8929 100\n","loss  275  -  tensor(0.3350, grad_fn=<NllLossBackward>)\n","Train Loss 275: 0.3350 Acc: 0.8929 100\n","loss  276  -  tensor(0.3345, grad_fn=<NllLossBackward>)\n","Train Loss 276: 0.3345 Acc: 0.8929 100\n","loss  277  -  tensor(0.3341, grad_fn=<NllLossBackward>)\n","Train Loss 277: 0.3341 Acc: 0.8929 100\n","loss  278  -  tensor(0.3336, grad_fn=<NllLossBackward>)\n","Train Loss 278: 0.3336 Acc: 0.8929 100\n","loss  279  -  tensor(0.3331, grad_fn=<NllLossBackward>)\n","Train Loss 279: 0.3331 Acc: 0.8929 100\n","loss  280  -  tensor(0.3327, grad_fn=<NllLossBackward>)\n","Train Loss 280: 0.3327 Acc: 0.8929 100\n","loss  281  -  tensor(0.3322, grad_fn=<NllLossBackward>)\n","Train Loss 281: 0.3322 Acc: 0.8929 100\n","loss  282  -  tensor(0.3318, grad_fn=<NllLossBackward>)\n","Train Loss 282: 0.3318 Acc: 0.8929 100\n","loss  283  -  tensor(0.3313, grad_fn=<NllLossBackward>)\n","Train Loss 283: 0.3313 Acc: 0.8929 100\n","loss  284  -  tensor(0.3308, grad_fn=<NllLossBackward>)\n","Train Loss 284: 0.3308 Acc: 0.8929 100\n","loss  285  -  tensor(0.3304, grad_fn=<NllLossBackward>)\n","Train Loss 285: 0.3304 Acc: 0.8929 100\n","loss  286  -  tensor(0.3299, grad_fn=<NllLossBackward>)\n","Train Loss 286: 0.3299 Acc: 0.8929 100\n","loss  287  -  tensor(0.3295, grad_fn=<NllLossBackward>)\n","Train Loss 287: 0.3295 Acc: 0.8929 100\n","loss  288  -  tensor(0.3290, grad_fn=<NllLossBackward>)\n","Train Loss 288: 0.3290 Acc: 0.8929 100\n","loss  289  -  tensor(0.3286, grad_fn=<NllLossBackward>)\n","Train Loss 289: 0.3286 Acc: 0.8929 100\n","loss  290  -  tensor(0.3281, grad_fn=<NllLossBackward>)\n","Train Loss 290: 0.3281 Acc: 0.8929 100\n","loss  291  -  tensor(0.3277, grad_fn=<NllLossBackward>)\n","Train Loss 291: 0.3277 Acc: 0.8929 100\n","loss  292  -  tensor(0.3273, grad_fn=<NllLossBackward>)\n","Train Loss 292: 0.3273 Acc: 0.8929 100\n","loss  293  -  tensor(0.3268, grad_fn=<NllLossBackward>)\n","Train Loss 293: 0.3268 Acc: 0.8929 100\n","loss  294  -  tensor(0.3264, grad_fn=<NllLossBackward>)\n","Train Loss 294: 0.3264 Acc: 0.9018 101\n","loss  295  -  tensor(0.3260, grad_fn=<NllLossBackward>)\n","Train Loss 295: 0.3260 Acc: 0.9018 101\n","loss  296  -  tensor(0.3255, grad_fn=<NllLossBackward>)\n","Train Loss 296: 0.3255 Acc: 0.9018 101\n","loss  297  -  tensor(0.3251, grad_fn=<NllLossBackward>)\n","Train Loss 297: 0.3251 Acc: 0.9018 101\n","loss  298  -  tensor(0.3247, grad_fn=<NllLossBackward>)\n","Train Loss 298: 0.3247 Acc: 0.9018 101\n","loss  299  -  tensor(0.3242, grad_fn=<NllLossBackward>)\n","Train Loss 299: 0.3242 Acc: 0.9018 101\n"]}]},{"cell_type":"code","metadata":{"id":"XYiK6cjqWPrv","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1633997931299,"user_tz":-540,"elapsed":292,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"6e439eae-f69f-484f-9835-932394def2f8"},"source":["import matplotlib.pyplot as plt\n","\n","fig = plt.figure()\n","ax = fig.add_subplot()\n","ax.plot(list(range(len(acc))),acc )\n","ax.set_xlabel('#epoch')\n","ax.set_ylabel('accuracy')\n","fig.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"Ugf3YAFeu9Rz"},"source":["C++のコードは手作業で修正しています。"]},{"cell_type":"code","metadata":{"id":"ERcQTqId1ymt"},"source":["!g++ -std=c++14 ../src/cse1.cpp ../src/cse1_param.cpp ../src/cse1_train.cpp -I ../../../xtensor -lcblas -o ../bin/cse1_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Yvq5MIhdz-X","executionInfo":{"status":"ok","timestamp":1633999637148,"user_tz":-540,"elapsed":2987,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"e53034de-e507-41b6-f82b-f8de8552542c"},"source":["!../bin/cse1_train"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 0 - loss 1.15292 - accuracy 0.151786\n","epoch 1 - loss 1.12852 - accuracy 0.169643\n","epoch 2 - loss 1.10508 - accuracy 0.178571\n","epoch 3 - loss 1.08259 - accuracy 0.169643\n","epoch 4 - loss 1.06101 - accuracy 0.178571\n","epoch 5 - loss 1.04031 - accuracy 0.196429\n","epoch 6 - loss 1.02046 - accuracy 0.214286\n","epoch 7 - loss 1.00144 - accuracy 0.258929\n","epoch 8 - loss 0.983218 - accuracy 0.3125\n","epoch 9 - loss 0.965757 - accuracy 0.410714\n","epoch 10 - loss 0.949032 - accuracy 0.5\n","epoch 11 - loss 0.933012 - accuracy 0.535714\n","epoch 12 - loss 0.917665 - accuracy 0.544643\n","epoch 13 - loss 0.902962 - accuracy 0.580357\n","epoch 14 - loss 0.888876 - accuracy 0.616071\n","epoch 15 - loss 0.875378 - accuracy 0.669643\n","epoch 16 - loss 0.862441 - accuracy 0.678571\n","epoch 17 - loss 0.850039 - accuracy 0.678571\n","epoch 18 - loss 0.838145 - accuracy 0.678571\n","epoch 19 - loss 0.826735 - accuracy 0.678571\n","epoch 20 - loss 0.815785 - accuracy 0.678571\n","epoch 21 - loss 0.805272 - accuracy 0.678571\n","epoch 22 - loss 0.795173 - accuracy 0.6875\n","epoch 23 - loss 0.785468 - accuracy 0.6875\n","epoch 24 - loss 0.776137 - accuracy 0.6875\n","epoch 25 - loss 0.767161 - accuracy 0.6875\n","epoch 26 - loss 0.758522 - accuracy 0.6875\n","epoch 27 - loss 0.750202 - accuracy 0.696429\n","epoch 28 - loss 0.742185 - accuracy 0.696429\n","epoch 29 - loss 0.734454 - accuracy 0.705357\n","epoch 30 - loss 0.726997 - accuracy 0.705357\n","epoch 31 - loss 0.719799 - accuracy 0.705357\n","epoch 32 - loss 0.712847 - accuracy 0.705357\n","epoch 33 - loss 0.706129 - accuracy 0.714286\n","epoch 34 - loss 0.699634 - accuracy 0.723214\n","epoch 35 - loss 0.693349 - accuracy 0.723214\n","epoch 36 - loss 0.687266 - accuracy 0.723214\n","epoch 37 - loss 0.681374 - accuracy 0.723214\n","epoch 38 - loss 0.675663 - accuracy 0.723214\n","epoch 39 - loss 0.670126 - accuracy 0.723214\n","epoch 40 - loss 0.664753 - accuracy 0.732143\n","epoch 41 - loss 0.659537 - accuracy 0.732143\n","epoch 42 - loss 0.654471 - accuracy 0.732143\n","epoch 43 - loss 0.649547 - accuracy 0.741071\n","epoch 44 - loss 0.64476 - accuracy 0.741071\n","epoch 45 - loss 0.640102 - accuracy 0.75\n","epoch 46 - loss 0.635568 - accuracy 0.75\n","epoch 47 - loss 0.631153 - accuracy 0.75\n","epoch 48 - loss 0.626852 - accuracy 0.758929\n","epoch 49 - loss 0.622659 - accuracy 0.758929\n","epoch 50 - loss 0.61857 - accuracy 0.758929\n","epoch 51 - loss 0.61458 - accuracy 0.758929\n","epoch 52 - loss 0.610685 - accuracy 0.758929\n","epoch 53 - loss 0.606882 - accuracy 0.758929\n","epoch 54 - loss 0.603166 - accuracy 0.758929\n","epoch 55 - loss 0.599533 - accuracy 0.767857\n","epoch 56 - loss 0.595982 - accuracy 0.767857\n","epoch 57 - loss 0.592507 - accuracy 0.767857\n","epoch 58 - loss 0.589107 - accuracy 0.767857\n","epoch 59 - loss 0.585778 - accuracy 0.767857\n","epoch 60 - loss 0.582519 - accuracy 0.767857\n","epoch 61 - loss 0.579324 - accuracy 0.767857\n","epoch 62 - loss 0.576195 - accuracy 0.767857\n","epoch 63 - loss 0.573126 - accuracy 0.767857\n","epoch 64 - loss 0.570117 - accuracy 0.767857\n","epoch 65 - loss 0.567165 - accuracy 0.767857\n","epoch 66 - loss 0.564268 - accuracy 0.776786\n","epoch 67 - loss 0.561425 - accuracy 0.776786\n","epoch 68 - loss 0.558634 - accuracy 0.776786\n","epoch 69 - loss 0.555892 - accuracy 0.776786\n","epoch 70 - loss 0.553198 - accuracy 0.785714\n","epoch 71 - loss 0.550551 - accuracy 0.785714\n","epoch 72 - loss 0.547949 - accuracy 0.794643\n","epoch 73 - loss 0.545391 - accuracy 0.794643\n","epoch 74 - loss 0.542876 - accuracy 0.794643\n","epoch 75 - loss 0.540401 - accuracy 0.794643\n","epoch 76 - loss 0.537967 - accuracy 0.794643\n","epoch 77 - loss 0.535572 - accuracy 0.794643\n","epoch 78 - loss 0.533215 - accuracy 0.794643\n","epoch 79 - loss 0.530894 - accuracy 0.794643\n","epoch 80 - loss 0.528608 - accuracy 0.794643\n","epoch 81 - loss 0.526357 - accuracy 0.803571\n","epoch 82 - loss 0.524139 - accuracy 0.803571\n","epoch 83 - loss 0.521954 - accuracy 0.803571\n","epoch 84 - loss 0.519801 - accuracy 0.803571\n","epoch 85 - loss 0.517678 - accuracy 0.803571\n","epoch 86 - loss 0.515585 - accuracy 0.803571\n","epoch 87 - loss 0.513521 - accuracy 0.8125\n","epoch 88 - loss 0.511486 - accuracy 0.8125\n","epoch 89 - loss 0.509478 - accuracy 0.8125\n","epoch 90 - loss 0.507498 - accuracy 0.8125\n","epoch 91 - loss 0.505545 - accuracy 0.8125\n","epoch 92 - loss 0.503618 - accuracy 0.8125\n","epoch 93 - loss 0.501716 - accuracy 0.8125\n","epoch 94 - loss 0.499839 - accuracy 0.8125\n","epoch 95 - loss 0.497985 - accuracy 0.821429\n","epoch 96 - loss 0.496155 - accuracy 0.821429\n","epoch 97 - loss 0.494348 - accuracy 0.821429\n","epoch 98 - loss 0.492563 - accuracy 0.821429\n","epoch 99 - loss 0.4908 - accuracy 0.821429\n","epoch 100 - loss 0.489058 - accuracy 0.821429\n","epoch 101 - loss 0.487337 - accuracy 0.830357\n","epoch 102 - loss 0.485637 - accuracy 0.839286\n","epoch 103 - loss 0.483957 - accuracy 0.839286\n","epoch 104 - loss 0.482296 - accuracy 0.848214\n","epoch 105 - loss 0.480655 - accuracy 0.848214\n","epoch 106 - loss 0.479032 - accuracy 0.848214\n","epoch 107 - loss 0.477428 - accuracy 0.848214\n","epoch 108 - loss 0.475842 - accuracy 0.848214\n","epoch 109 - loss 0.474274 - accuracy 0.848214\n","epoch 110 - loss 0.472724 - accuracy 0.848214\n","epoch 111 - loss 0.47119 - accuracy 0.848214\n","epoch 112 - loss 0.469674 - accuracy 0.839286\n","epoch 113 - loss 0.468174 - accuracy 0.839286\n","epoch 114 - loss 0.46669 - accuracy 0.839286\n","epoch 115 - loss 0.465221 - accuracy 0.839286\n","epoch 116 - loss 0.463768 - accuracy 0.839286\n","epoch 117 - loss 0.462331 - accuracy 0.839286\n","epoch 118 - loss 0.460909 - accuracy 0.839286\n","epoch 119 - loss 0.459501 - accuracy 0.839286\n","epoch 120 - loss 0.458108 - accuracy 0.839286\n","epoch 121 - loss 0.45673 - accuracy 0.839286\n","epoch 122 - loss 0.455365 - accuracy 0.839286\n","epoch 123 - loss 0.454014 - accuracy 0.839286\n","epoch 124 - loss 0.452677 - accuracy 0.839286\n","epoch 125 - loss 0.451353 - accuracy 0.839286\n","epoch 126 - loss 0.450043 - accuracy 0.839286\n","epoch 127 - loss 0.448745 - accuracy 0.839286\n","epoch 128 - loss 0.44746 - accuracy 0.839286\n","epoch 129 - loss 0.446188 - accuracy 0.839286\n","epoch 130 - loss 0.444928 - accuracy 0.839286\n","epoch 131 - loss 0.44368 - accuracy 0.839286\n","epoch 132 - loss 0.442443 - accuracy 0.839286\n","epoch 133 - loss 0.441219 - accuracy 0.839286\n","epoch 134 - loss 0.440006 - accuracy 0.839286\n","epoch 135 - loss 0.438804 - accuracy 0.839286\n","epoch 136 - loss 0.437614 - accuracy 0.839286\n","epoch 137 - loss 0.436434 - accuracy 0.839286\n","epoch 138 - loss 0.435265 - accuracy 0.839286\n","epoch 139 - loss 0.434106 - accuracy 0.839286\n","epoch 140 - loss 0.432958 - accuracy 0.839286\n","epoch 141 - loss 0.43182 - accuracy 0.839286\n","epoch 142 - loss 0.430692 - accuracy 0.839286\n","epoch 143 - loss 0.429574 - accuracy 0.839286\n","epoch 144 - loss 0.428466 - accuracy 0.839286\n","epoch 145 - loss 0.427368 - accuracy 0.839286\n","epoch 146 - loss 0.426279 - accuracy 0.839286\n","epoch 147 - loss 0.425199 - accuracy 0.848214\n","epoch 148 - loss 0.424129 - accuracy 0.848214\n","epoch 149 - loss 0.423068 - accuracy 0.848214\n","epoch 150 - loss 0.422016 - accuracy 0.848214\n","epoch 151 - loss 0.420973 - accuracy 0.848214\n","epoch 152 - loss 0.419939 - accuracy 0.857143\n","epoch 153 - loss 0.418913 - accuracy 0.857143\n","epoch 154 - loss 0.417896 - accuracy 0.857143\n","epoch 155 - loss 0.416887 - accuracy 0.857143\n","epoch 156 - loss 0.415887 - accuracy 0.857143\n","epoch 157 - loss 0.414894 - accuracy 0.857143\n","epoch 158 - loss 0.41391 - accuracy 0.857143\n","epoch 159 - loss 0.412934 - accuracy 0.857143\n","epoch 160 - loss 0.411966 - accuracy 0.857143\n","epoch 161 - loss 0.411005 - accuracy 0.857143\n","epoch 162 - loss 0.410052 - accuracy 0.857143\n","epoch 163 - loss 0.409106 - accuracy 0.857143\n","epoch 164 - loss 0.408169 - accuracy 0.857143\n","epoch 165 - loss 0.407238 - accuracy 0.857143\n","epoch 166 - loss 0.406315 - accuracy 0.857143\n","epoch 167 - loss 0.405398 - accuracy 0.857143\n","epoch 168 - loss 0.404489 - accuracy 0.857143\n","epoch 169 - loss 0.403587 - accuracy 0.857143\n","epoch 170 - loss 0.402692 - accuracy 0.857143\n","epoch 171 - loss 0.401803 - accuracy 0.857143\n","epoch 172 - loss 0.400922 - accuracy 0.857143\n","epoch 173 - loss 0.400047 - accuracy 0.857143\n","epoch 174 - loss 0.399178 - accuracy 0.857143\n","epoch 175 - loss 0.398316 - accuracy 0.857143\n","epoch 176 - loss 0.397461 - accuracy 0.857143\n","epoch 177 - loss 0.396611 - accuracy 0.857143\n","epoch 178 - loss 0.395768 - accuracy 0.857143\n","epoch 179 - loss 0.394931 - accuracy 0.857143\n","epoch 180 - loss 0.394101 - accuracy 0.857143\n","epoch 181 - loss 0.393276 - accuracy 0.857143\n","epoch 182 - loss 0.392457 - accuracy 0.857143\n","epoch 183 - loss 0.391644 - accuracy 0.857143\n","epoch 184 - loss 0.390837 - accuracy 0.857143\n","epoch 185 - loss 0.390036 - accuracy 0.857143\n","epoch 186 - loss 0.38924 - accuracy 0.866071\n","epoch 187 - loss 0.38845 - accuracy 0.866071\n","epoch 188 - loss 0.387665 - accuracy 0.866071\n","epoch 189 - loss 0.386886 - accuracy 0.866071\n","epoch 190 - loss 0.386112 - accuracy 0.866071\n","epoch 191 - loss 0.385344 - accuracy 0.866071\n","epoch 192 - loss 0.384581 - accuracy 0.866071\n","epoch 193 - loss 0.383823 - accuracy 0.866071\n","epoch 194 - loss 0.38307 - accuracy 0.866071\n","epoch 195 - loss 0.382323 - accuracy 0.866071\n","epoch 196 - loss 0.38158 - accuracy 0.866071\n","epoch 197 - loss 0.380843 - accuracy 0.866071\n","epoch 198 - loss 0.38011 - accuracy 0.866071\n","epoch 199 - loss 0.379382 - accuracy 0.866071\n","epoch 200 - loss 0.378659 - accuracy 0.875\n","epoch 201 - loss 0.377941 - accuracy 0.875\n","epoch 202 - loss 0.377228 - accuracy 0.875\n","epoch 203 - loss 0.376519 - accuracy 0.875\n","epoch 204 - loss 0.375814 - accuracy 0.875\n","epoch 205 - loss 0.375115 - accuracy 0.875\n","epoch 206 - loss 0.37442 - accuracy 0.875\n","epoch 207 - loss 0.373729 - accuracy 0.875\n","epoch 208 - loss 0.373043 - accuracy 0.875\n","epoch 209 - loss 0.37236 - accuracy 0.875\n","epoch 210 - loss 0.371683 - accuracy 0.875\n","epoch 211 - loss 0.37101 - accuracy 0.875\n","epoch 212 - loss 0.37034 - accuracy 0.875\n","epoch 213 - loss 0.369675 - accuracy 0.875\n","epoch 214 - loss 0.369014 - accuracy 0.875\n","epoch 215 - loss 0.368358 - accuracy 0.875\n","epoch 216 - loss 0.367705 - accuracy 0.875\n","epoch 217 - loss 0.367056 - accuracy 0.875\n","epoch 218 - loss 0.366411 - accuracy 0.875\n","epoch 219 - loss 0.36577 - accuracy 0.875\n","epoch 220 - loss 0.365133 - accuracy 0.875\n","epoch 221 - loss 0.3645 - accuracy 0.875\n","epoch 222 - loss 0.36387 - accuracy 0.875\n","epoch 223 - loss 0.363244 - accuracy 0.875\n","epoch 224 - loss 0.362622 - accuracy 0.875\n","epoch 225 - loss 0.362004 - accuracy 0.875\n","epoch 226 - loss 0.361389 - accuracy 0.875\n","epoch 227 - loss 0.360778 - accuracy 0.875\n","epoch 228 - loss 0.360171 - accuracy 0.875\n","epoch 229 - loss 0.359567 - accuracy 0.883929\n","epoch 230 - loss 0.358967 - accuracy 0.883929\n","epoch 231 - loss 0.35837 - accuracy 0.883929\n","epoch 232 - loss 0.357776 - accuracy 0.883929\n","epoch 233 - loss 0.357186 - accuracy 0.883929\n","epoch 234 - loss 0.356599 - accuracy 0.883929\n","epoch 235 - loss 0.356016 - accuracy 0.883929\n","epoch 236 - loss 0.355435 - accuracy 0.883929\n","epoch 237 - loss 0.354857 - accuracy 0.883929\n","epoch 238 - loss 0.354283 - accuracy 0.883929\n","epoch 239 - loss 0.353712 - accuracy 0.883929\n","epoch 240 - loss 0.353144 - accuracy 0.883929\n","epoch 241 - loss 0.352579 - accuracy 0.883929\n","epoch 242 - loss 0.352018 - accuracy 0.883929\n","epoch 243 - loss 0.351459 - accuracy 0.883929\n","epoch 244 - loss 0.350903 - accuracy 0.883929\n","epoch 245 - loss 0.350351 - accuracy 0.883929\n","epoch 246 - loss 0.349801 - accuracy 0.883929\n","epoch 247 - loss 0.349254 - accuracy 0.883929\n","epoch 248 - loss 0.34871 - accuracy 0.883929\n","epoch 249 - loss 0.348169 - accuracy 0.883929\n","epoch 250 - loss 0.34763 - accuracy 0.883929\n","epoch 251 - loss 0.347095 - accuracy 0.883929\n","epoch 252 - loss 0.346562 - accuracy 0.883929\n","epoch 253 - loss 0.346032 - accuracy 0.892857\n","epoch 254 - loss 0.345504 - accuracy 0.892857\n","epoch 255 - loss 0.344979 - accuracy 0.892857\n","epoch 256 - loss 0.344457 - accuracy 0.892857\n","epoch 257 - loss 0.343938 - accuracy 0.892857\n","epoch 258 - loss 0.343421 - accuracy 0.892857\n","epoch 259 - loss 0.342907 - accuracy 0.892857\n","epoch 260 - loss 0.342396 - accuracy 0.892857\n","epoch 261 - loss 0.341886 - accuracy 0.892857\n","epoch 262 - loss 0.34138 - accuracy 0.892857\n","epoch 263 - loss 0.340876 - accuracy 0.892857\n","epoch 264 - loss 0.340374 - accuracy 0.892857\n","epoch 265 - loss 0.339875 - accuracy 0.892857\n","epoch 266 - loss 0.339378 - accuracy 0.892857\n","epoch 267 - loss 0.338884 - accuracy 0.892857\n","epoch 268 - loss 0.338392 - accuracy 0.892857\n","epoch 269 - loss 0.337903 - accuracy 0.892857\n","epoch 270 - loss 0.337417 - accuracy 0.892857\n","epoch 271 - loss 0.336932 - accuracy 0.892857\n","epoch 272 - loss 0.33645 - accuracy 0.892857\n","epoch 273 - loss 0.33597 - accuracy 0.892857\n","epoch 274 - loss 0.335492 - accuracy 0.892857\n","epoch 275 - loss 0.335017 - accuracy 0.892857\n","epoch 276 - loss 0.334544 - accuracy 0.892857\n","epoch 277 - loss 0.334073 - accuracy 0.892857\n","epoch 278 - loss 0.333604 - accuracy 0.892857\n","epoch 279 - loss 0.333137 - accuracy 0.892857\n","epoch 280 - loss 0.332673 - accuracy 0.892857\n","epoch 281 - loss 0.332211 - accuracy 0.892857\n","epoch 282 - loss 0.33175 - accuracy 0.892857\n","epoch 283 - loss 0.331292 - accuracy 0.892857\n","epoch 284 - loss 0.330836 - accuracy 0.892857\n","epoch 285 - loss 0.330382 - accuracy 0.892857\n","epoch 286 - loss 0.32993 - accuracy 0.892857\n","epoch 287 - loss 0.32948 - accuracy 0.892857\n","epoch 288 - loss 0.329032 - accuracy 0.892857\n","epoch 289 - loss 0.328586 - accuracy 0.892857\n","epoch 290 - loss 0.328143 - accuracy 0.892857\n","epoch 291 - loss 0.327702 - accuracy 0.892857\n","epoch 292 - loss 0.327262 - accuracy 0.892857\n","epoch 293 - loss 0.326825 - accuracy 0.892857\n","epoch 294 - loss 0.326389 - accuracy 0.901786\n","epoch 295 - loss 0.325955 - accuracy 0.901786\n","epoch 296 - loss 0.325523 - accuracy 0.901786\n","epoch 297 - loss 0.325093 - accuracy 0.901786\n","epoch 298 - loss 0.324665 - accuracy 0.901786\n","epoch 299 - loss 0.324238 - accuracy 0.901786\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ac-_IZA4d5dG","executionInfo":{"status":"ok","timestamp":1633999639852,"user_tz":-540,"elapsed":247,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"8c268300-de5c-430b-fd45-793b62afc41b"},"source":["f = open('./cse1.out', 'r')\n","\n","loss = []\n","acc=[]\n","\n","datalist = f.readlines()\n","for data in datalist:\n","  #print(data)\n","  ds = data.split(',')\n","  loss.append( float(ds[0]) )\n","  acc.append( float(ds[1]) )\n","\n","f.close()\n","\n","print(\"epoch =\", len(loss))\n","print( loss )"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch = 300\n","[1.152917, 1.128515, 1.105082, 1.082587, 1.061006, 1.040308, 1.020464, 1.001443, 0.983218, 0.965757, 0.949032, 0.933012, 0.917665, 0.902962, 0.888876, 0.875378, 0.862441, 0.850039, 0.838145, 0.826735, 0.815785, 0.805272, 0.795173, 0.785468, 0.776137, 0.767161, 0.758522, 0.750202, 0.742185, 0.734454, 0.726997, 0.719799, 0.712847, 0.706129, 0.699634, 0.693349, 0.687266, 0.681374, 0.675663, 0.670126, 0.664753, 0.659537, 0.654471, 0.649547, 0.64476, 0.640102, 0.635568, 0.631153, 0.626852, 0.622659, 0.61857, 0.61458, 0.610685, 0.606882, 0.603166, 0.599533, 0.595982, 0.592507, 0.589107, 0.585778, 0.582519, 0.579324, 0.576195, 0.573126, 0.570117, 0.567165, 0.564268, 0.561425, 0.558634, 0.555892, 0.553198, 0.550551, 0.547949, 0.545391, 0.542876, 0.540401, 0.537967, 0.535572, 0.533215, 0.530894, 0.528608, 0.526357, 0.524139, 0.521954, 0.519801, 0.517678, 0.515585, 0.513521, 0.511486, 0.509478, 0.507498, 0.505545, 0.503618, 0.501716, 0.499839, 0.497985, 0.496155, 0.494348, 0.492563, 0.4908, 0.489058, 0.487337, 0.485637, 0.483957, 0.482296, 0.480655, 0.479032, 0.477428, 0.475842, 0.474274, 0.472724, 0.47119, 0.469674, 0.468174, 0.46669, 0.465221, 0.463768, 0.462331, 0.460909, 0.459501, 0.458108, 0.45673, 0.455365, 0.454014, 0.452677, 0.451353, 0.450043, 0.448745, 0.44746, 0.446188, 0.444928, 0.44368, 0.442443, 0.441219, 0.440006, 0.438804, 0.437614, 0.436434, 0.435265, 0.434106, 0.432958, 0.43182, 0.430692, 0.429574, 0.428466, 0.427368, 0.426279, 0.425199, 0.424129, 0.423068, 0.422016, 0.420973, 0.419939, 0.418913, 0.417896, 0.416887, 0.415887, 0.414894, 0.41391, 0.412934, 0.411966, 0.411005, 0.410052, 0.409106, 0.408169, 0.407238, 0.406315, 0.405398, 0.404489, 0.403587, 0.402692, 0.401803, 0.400922, 0.400047, 0.399178, 0.398316, 0.397461, 0.396611, 0.395768, 0.394931, 0.394101, 0.393276, 0.392457, 0.391644, 0.390837, 0.390036, 0.38924, 0.38845, 0.387665, 0.386886, 0.386112, 0.385344, 0.384581, 0.383823, 0.38307, 0.382323, 0.38158, 0.380843, 0.38011, 0.379382, 0.378659, 0.377941, 0.377228, 0.376519, 0.375814, 0.375115, 0.37442, 0.373729, 0.373043, 0.37236, 0.371683, 0.37101, 0.37034, 0.369675, 0.369014, 0.368358, 0.367705, 0.367056, 0.366411, 0.36577, 0.365133, 0.3645, 0.36387, 0.363244, 0.362622, 0.362004, 0.361389, 0.360778, 0.360171, 0.359567, 0.358967, 0.35837, 0.357776, 0.357186, 0.356599, 0.356016, 0.355435, 0.354857, 0.354283, 0.353712, 0.353144, 0.352579, 0.352018, 0.351459, 0.350903, 0.350351, 0.349801, 0.349254, 0.34871, 0.348169, 0.34763, 0.347095, 0.346562, 0.346032, 0.345504, 0.344979, 0.344457, 0.343938, 0.343421, 0.342907, 0.342396, 0.341886, 0.34138, 0.340876, 0.340374, 0.339875, 0.339378, 0.338884, 0.338392, 0.337903, 0.337417, 0.336932, 0.33645, 0.33597, 0.335492, 0.335017, 0.334544, 0.334073, 0.333604, 0.333137, 0.332673, 0.332211, 0.33175, 0.331292, 0.330836, 0.330382, 0.32993, 0.32948, 0.329032, 0.328586, 0.328143, 0.327702, 0.327262, 0.326825, 0.326389, 0.325955, 0.325523, 0.325093, 0.324665, 0.324238]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"xMQUBvFqfMWg","executionInfo":{"status":"ok","timestamp":1633999642640,"user_tz":-540,"elapsed":568,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"740fc55c-9ff1-473a-da92-15b78fef395b"},"source":["# 交差エントロピー誤差\n","import matplotlib.pyplot as plt\n","\n","fig = plt.figure()\n","ax = fig.add_subplot()\n","ax.plot(list(range(len(loss))), loss)\n","ax.set_xlabel('epoch')\n","ax.set_ylabel('loss')\n","fig.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"iLif0DtvuYG5","executionInfo":{"status":"ok","timestamp":1633999644868,"user_tz":-540,"elapsed":271,"user":{"displayName":"Mari Hiroshi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18424111801791476769"}},"outputId":"b29b5d4d-e853-4e51-857a-48d177a6d930"},"source":["# 正解率\n","fig = plt.figure()\n","ax = fig.add_subplot()\n","ax.plot(list(range(len(acc))), acc)\n","ax.set_xlabel('epoch')\n","ax.set_ylabel('accuracy')\n","fig.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"OSkuEd7zfizf"},"source":[""],"execution_count":null,"outputs":[]}]}